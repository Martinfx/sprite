head     9.4;
branch   ;
access   ;
symbols  ds3100:9.4 sun3:9.4 sun4nw:9.4 symm:9.4 spur:9.4 Summer89:9.0 newlib:8.0 Summer88:6.0;
locks    ; strict;
comment  @ * @;


9.4
date     91.03.17.20.52.18;  author kupfer;  state Exp;
branches ;
next     9.3;

9.3
date     91.03.13.22.27.44;  author kupfer;  state Exp;
branches ;
next     9.2;

9.2
date     90.10.19.15.56.41;  author rab;  state Exp;
branches ;
next     9.1;

9.1
date     90.10.05.17.50.32;  author mendel;  state Exp;
branches ;
next     9.0;

9.0
date     89.09.12.15.20.07;  author douglis;  state Stable;
branches ;
next     8.4;

8.4
date     89.08.17.17.32.01;  author jhh;  state Exp;
branches ;
next     8.3;

8.3
date     89.02.19.22.15.39;  author jhh;  state Exp;
branches ;
next     8.2;

8.2
date     89.01.06.11.29.15;  author jhh;  state Exp;
branches ;
next     8.1;

8.1
date     88.11.22.19.36.45;  author jhh;  state Exp;
branches ;
next     8.0;

8.0
date     88.11.11.18.38.12;  author douglis;  state Stable;
branches ;
next     6.2;

6.2
date     88.08.25.22.40.09;  author douglis;  state Exp;
branches ;
next     6.1;

6.1
date     88.08.24.14.48.55;  author douglis;  state Exp;
branches ;
next     6.0;

6.0
date     88.08.11.12.27.14;  author brent;  state Stable;
branches ;
next     5.6;

5.6
date     88.07.18.22.40.07;  author nelson;  state Exp;
branches ;
next     5.5;

5.5
date     88.07.17.19.35.11;  author nelson;  state Exp;
branches ;
next     5.4;

5.4
date     88.05.05.18.00.35;  author nelson;  state Exp;
branches ;
next     5.3;

5.3
date     88.03.08.16.17.10;  author nelson;  state Exp;
branches ;
next     5.2;

5.2
date     87.12.16.13.04.06;  author nelson;  state Exp;
branches ;
next     5.1;

5.1
date     87.12.12.16.41.35;  author nelson;  state Exp;
branches ;
next     5.0;

5.0
date     87.08.11.10.51.07;  author sprite;  state Exp;
branches ;
next     ;


desc
@@


9.4
log
@Added overall comments about locking.  Call Vm_MakeAccessible before
touching user addresses (this gets rid of an ugly sequent ifdef and
needs to be done to run on top of Mach).  For locks, always use the
user address as the event number.
@
text
@/* 
 * syncUser.c --
 *
 *	These are system call routines of the Synchronization module that
 *	support monitors for user-level code.
 *
 *	A process is blocked by making it wait on an event.  An event is
 *	just an uninterpreted integer that gets 'signaled' by the routine
 *	Sync_UserSlowBroadcast.
 *	
 *	We pin a user's page into memory before touching it.  This
 *	simplifies locking and lets us hold the sched master lock
 *	while touching user memory.  
 *	
 *	The calls to Vm_MakeAccessible are assumed to succeed because
 *	the calls to Vm_PinUserMemory succeeded.  The calls to 
 *	Vm_MakeAccessible come before obtaining the sched master lock; 
 *	this shouldn't matter for native Sprite (Vm_MakeAccessible 
 *	shouldn't block), but it could make a difference for the
 *	Sprite server (on Mach).  The locks set by Vm_MakeAccessible
 *	should not be held while waiting for an event.
 *
 * Copyright 1986, 1991 Regents of the University of California
 * All rights reserved.
 */

#ifndef lint
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 9.3 91/03/13 22:27:44 kupfer Exp Locker: kupfer $ SPRITE (Berkeley)";
#endif /* not lint */

#include <sprite.h>
#include <mach.h>
#include <sync.h>
#include <syncInt.h>
#include <sched.h>
#include <sys.h>
#include <vm.h>


/*
 * ----------------------------------------------------------------------------
 *
 * Sync_SlowLockStub --
 *
 *	Stub for the Sync_SlowLock system call.
 *	Acquire a lock while holding the synchronization master lock.
 *
 *      Inside the critical section the inUse bit is checked.  If we have
 *      to wait the process is put to sleep waiting on an event associated
 *      with the lock.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *      The lock is acquired when this procedure returns.  The process may
 *      have been put to sleep while waiting for the lock to become
 *      available.
 *
 * ----------------------------------------------------------------------------
 */

ReturnStatus
Sync_SlowLockStub(lockPtr)
    Sync_UserLock *lockPtr;
{
    ReturnStatus	status = SUCCESS;
    Proc_ControlBlock	*procPtr;
    Sync_UserLock	*userLockPtr; /* lock's addr in kernel addr space */
    int			numBytes; /* number of bytes accessible */
    Boolean		waiting = TRUE;

    procPtr = Proc_GetCurrentProc();

    status = Vm_PinUserMem(VM_READWRITE_ACCESS, sizeof(*lockPtr),
			(Address)lockPtr);
    if (status != SUCCESS) {
	return(status);
    }

    while (waiting) {
	Vm_MakeAccessible(VM_READWRITE_ACCESS, sizeof(*lockPtr),
			  (Address)lockPtr, &numBytes,
			  (Address *)&userLockPtr);
	MASTER_LOCK(sched_MutexPtr);
	waiting = (Mach_TestAndSet(&(userLockPtr->inUse)) != 0);
	if (waiting) {
	    userLockPtr->waiting = TRUE;
	}
	Vm_MakeUnaccessible((Address)userLockPtr, numBytes);
	if (!waiting) {
	    break;
	}

	(void) SyncEventWaitInt((unsigned int)lockPtr, TRUE);
#ifdef spur
	Mach_InstCountEnd(1);
#endif
	MASTER_UNLOCK(sched_MutexPtr);
	MASTER_LOCK(sched_MutexPtr);
	if (Sig_Pending(procPtr)) {
	    status = GEN_ABORTED_BY_SIGNAL;
	    break;
	}
	MASTER_UNLOCK(sched_MutexPtr);
    }
#ifdef spur
    if (Mach_InstCountIsOn(1)) {
	panic("About to unlock sched_Mutex with inst count on.\n");
    }
#endif

    /* 
     * We always exit the loop holding the sched master lock.
     */
    MASTER_UNLOCK(sched_MutexPtr);

    (void)Vm_UnpinUserMem(sizeof(*lockPtr), (Address)lockPtr);
    return(status);
}


/*
 * ----------------------------------------------------------------------------
 *
 * Sync_SlowWaitStub --
 *
 *	Stub for the Sync_SlowWait system call.
 *      Wait on an event.  The lock is released and the process is blocked
 *      on the event.  A future call to Sync_UserSlowBroadcast will signal the
 *      event and make this process runnable again.
 *
 *      This can only be called while a lock is held.  This forces our
 *      client to safely check global state while in a monitor.  The 
 *      caller of this system call (i.e., the Sync_SlowWait library
 *      routine) must reacquire the lock before returning.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *      Put the process to sleep and release the monitor lock.  Other
 *      processes waiting on the monitor lock become runnable.
 *	
 *
 * ----------------------------------------------------------------------------
 */
ReturnStatus
Sync_SlowWaitStub(event, lockPtr, wakeIfSignal)
    unsigned 	int 	event;
    Sync_UserLock	*lockPtr;
    Boolean		wakeIfSignal;
{
    ReturnStatus	status;
    Sync_UserLock 	*userLockPtr; /* lock's addr in kernel addr space */
    int			numBytes; /* number of bytes accessible */

    status = Vm_PinUserMem(VM_READWRITE_ACCESS, sizeof(*lockPtr),
			(Address)lockPtr);
    if (status != SUCCESS) {
	return(status);
    }

    Vm_MakeAccessible(VM_READWRITE_ACCESS, sizeof(*lockPtr),
		      (Address)lockPtr, &numBytes,
		      (Address *)&userLockPtr);
    MASTER_LOCK(sched_MutexPtr);
    /*
     * release the monitor lock and wait on the condition
     */
    userLockPtr->inUse = 0;
    userLockPtr->waiting = FALSE;
    Vm_MakeUnaccessible((Address)userLockPtr, numBytes);
    SyncEventWakeupInt((unsigned int)lockPtr);

    if (SyncEventWaitInt(event, wakeIfSignal)) {
	status = GEN_ABORTED_BY_SIGNAL;
    } else {
	status = SUCCESS;
    }
#ifdef spur
    Mach_InstCountEnd(1);
#endif
    MASTER_UNLOCK(sched_MutexPtr);

    (void)Vm_UnpinUserMem(sizeof(*lockPtr), (Address)lockPtr);
    return(status);
}


/*
 * ----------------------------------------------------------------------------
 *
 * Sync_SlowBroadcastStub --
 *
 *	Stub for the Sync_SlowBroadcast system call.
 *      Mark all processes waiting on an event as runable.  The flag that
 *      indicates there are waiters is cleared here inside the protected
 *      critical section.  This has "broadcast" semantics because everyone
 *      waiting is made runable.  We don't yet have a mechanism to wake up
 *      just one waiting process.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Make processes waiting on the event runnable.
 *
 * ----------------------------------------------------------------------------
 */

ReturnStatus
Sync_SlowBroadcastStub(event, waitFlagPtr)
    unsigned int event;
    int *waitFlagPtr;
{
    ReturnStatus	status;
    int		*userWaitFlagPtr; /* waitFlagPtr in kernel addr space */
    int		numBytes;	/* number of bytes accessible */

    status = Vm_PinUserMem(VM_READWRITE_ACCESS, sizeof(*waitFlagPtr), 
			(Address)waitFlagPtr);
    if (status != SUCCESS) {
	return(status);
    }
    Vm_MakeAccessible(VM_READWRITE_ACCESS, sizeof(*waitFlagPtr),
		      (Address)waitFlagPtr, &numBytes,
		      (Address *)&userWaitFlagPtr);

    MASTER_LOCK(sched_MutexPtr);

    *userWaitFlagPtr = FALSE;
    Vm_MakeUnaccessible((Address)userWaitFlagPtr, numBytes);
    SyncEventWakeupInt(event);

#ifdef spur
    if (Mach_InstCountIsOn(1)) {
	panic("About to unlock sched_Mutex with inst count on.\n");
    }
#endif
    MASTER_UNLOCK(sched_MutexPtr);

    (void)Vm_UnpinUserMem(sizeof(*waitFlagPtr), (Address)waitFlagPtr);

    return(SUCCESS);
}
@


9.3
log
@Add comment about how locking for Sync_SlowWaitStub works.
@
text
@d10 12
d23 1
a23 1
 * Copyright 1986 Regents of the University of California
d28 1
a28 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 9.2 90/10/19 15:56:41 rab Exp Locker: kupfer $ SPRITE (Berkeley)";
d69 3
a71 6
#ifdef	sequent
    Sync_UserLock *userLockPtr =
		(Sync_UserLock*) ((Address) lockPtr + mach_KernelVirtAddrUser);
#else	/* !sequent */
    Sync_UserLock *userLockPtr = lockPtr;
#endif	/* sequent */
d80 16
a95 4
    MASTER_LOCK(sched_MutexPtr);
    while (Mach_TestAndSet(&(userLockPtr->inUse)) != 0) {
	userLockPtr->waiting = TRUE;
	(void) SyncEventWaitInt((unsigned int)userLockPtr, TRUE);
d105 1
d112 4
d155 2
a156 6
#ifdef	sequent
    Sync_UserLock *userLockPtr =
		(Sync_UserLock*) ((Address) lockPtr + mach_KernelVirtAddrUser);
#else	/* !sequent */
    Sync_UserLock *userLockPtr = lockPtr;
#endif	/* sequent */
d163 4
d173 1
d218 2
a219 5
#ifdef	sequent
    int *userWaitFlagPtr =(int*)((Address)waitFlagPtr+mach_KernelVirtAddrUser);
#else	/* !sequent */
    int *userWaitFlagPtr = waitFlagPtr;
#endif	/* sequent */
d226 3
d233 1
@


9.2
log
@Changes for symmetry.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 9.1 90/10/05 17:50:32 mendel Exp Locker: rab $ SPRITE (Berkeley)";
d108 3
a110 1
 *      client to safely check global state while in a monitor.
@


9.1
log
@Did function prototypeing and fixed include files.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 9.0 89/09/12 15:20:07 douglis Stable Locker: mendel $ SPRITE (Berkeley)";
d57 6
d72 3
a74 3
    while (Mach_TestAndSet(&(lockPtr->inUse)) != 0) {
	lockPtr->waiting = TRUE;
	(void) SyncEventWaitInt((unsigned int)lockPtr, TRUE);
d127 6
d143 2
a144 2
    lockPtr->inUse = 0;
    lockPtr->waiting = FALSE;
d189 5
d203 1
a203 1
    *waitFlagPtr = FALSE;
@


9.0
log
@Changing version numbers.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 8.4 89/08/17 17:32:01 jhh Exp Locker: douglis $ SPRITE (Berkeley)";
d19 7
a25 7
#include "sprite.h"
#include "mach.h"
#include "sync.h"
#include "syncInt.h"
#include "sched.h"
#include "sys.h"
#include "vm.h"
@


8.4
log
@Added instruction counting for spur
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 8.3 89/02/19 22:15:39 jhh Exp $ SPRITE (Berkeley)";
@


8.3
log
@Changes due to lock registration
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 8.2 89/01/06 11:29:15 jhh Exp $ SPRITE (Berkeley)";
d69 3
d79 5
d140 3
d189 5
@


8.2
log
@New Sync_Lock definition
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 8.1 88/11/22 19:36:45 jhh Exp Locker: jhh $ SPRITE (Berkeley)";
d53 1
a53 1
    Sync_Lock_User *lockPtr;
d109 1
a109 1
    Sync_Lock_User	*lockPtr;
@


8.1
log
@new semaphore definition
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 6.2 88/08/25 22:40:09 douglis Exp $ SPRITE (Berkeley)";
d53 1
a53 1
    Sync_Lock *lockPtr;
d109 1
a109 1
    Sync_Lock 		*lockPtr;
@


8.0
log
@Changing version numbers.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: /sprite/src/kernel/sync/RCS/syncUser.c,v 6.2 88/08/25 22:40:09 douglis Exp Locker: douglis $ SPRITE (Berkeley)";
d65 1
a65 1
    MASTER_LOCK(sched_Mutex);
d69 2
a70 2
	MASTER_UNLOCK(sched_Mutex);
	MASTER_LOCK(sched_Mutex);
d76 1
a76 1
    MASTER_UNLOCK(sched_Mutex);
d119 1
a119 1
    MASTER_LOCK(sched_Mutex);
d132 1
a132 1
    MASTER_UNLOCK(sched_Mutex);
d173 1
a173 1
    MASTER_LOCK(sched_Mutex);
d178 1
a178 1
    MASTER_UNLOCK(sched_Mutex);
@


6.2
log
@
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 6.1 88/08/24 14:48:55 douglis Exp $ SPRITE (Berkeley)";
@


6.1
log
@fixed bug due to assigning an uninitialized ptr.
@
text
@d16 2
a17 2
static char rcsid[] = "$Header: syncUser.c,v 6.0 88/08/11 12:27:14 brent Stable $ SPRITE (Berkeley)";
#endif not lint
@


6.0
log
@Changing version numbers.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 5.6 88/07/18 22:40:07 nelson Exp $ SPRITE (Berkeley)";
a164 2
    int			*newWaitFlagPtr;
    int			len;
d175 1
a175 1
    *newWaitFlagPtr = FALSE;
@


5.6
log
@Vm_UserMap => Vm_PinUserMem and Vm_UserUnmap => Vm_UnpinUserMem
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 5.5 88/07/17 19:35:11 nelson Exp $ SPRITE (Berkeley)";
@


5.5
log
@Changed to use the new vm mapping calls instead of set jump which never
was right anyway.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 5.4 88/05/05 18:00:35 nelson Exp $ SPRITE (Berkeley)";
d60 1
a60 1
    status = Vm_UserMap(VM_READWRITE_ACCESS, sizeof(*lockPtr),
d78 1
a78 1
    (void)Vm_UserUnmap(sizeof(*lockPtr), (Address)lockPtr);
d114 1
a114 1
    status = Vm_UserMap(VM_READWRITE_ACCESS, sizeof(*lockPtr),
d134 1
a134 1
    (void)Vm_UserUnmap(sizeof(*lockPtr), (Address)lockPtr);
d169 1
a169 1
    status = Vm_UserMap(VM_READWRITE_ACCESS, sizeof(*waitFlagPtr), 
d182 1
a182 1
    (void)Vm_UserUnmap(sizeof(*waitFlagPtr), (Address)waitFlagPtr);
@


5.4
log
@Handles move of functionality from sys to mach.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 5.3 88/03/08 16:17:10 nelson Exp $ SPRITE (Berkeley)";
a54 1
    Mach_SetJumpState	setJumpState;
d60 10
a69 7
    /*
     * We have to use set-jump instead of MakeAccessible because we would have 
     * to sleep for an indeterminate amount of time with something made
     * accessible which is not a good idea.  Note that we assume that a user
     * pointer is directly accessible from the kernel.
     */
    if (Mach_SetJump(&setJumpState) == SUCCESS) {
d71 3
a73 10

	while (Mach_TestAndSet(&(lockPtr->inUse)) != 0) {
	    lockPtr->waiting = TRUE;
	    (void) SyncEventWaitInt((unsigned int)lockPtr, TRUE);
	    MASTER_UNLOCK(sched_Mutex);
	    MASTER_LOCK(sched_Mutex);
	    if (Sig_Pending(procPtr)) {
		status = GEN_ABORTED_BY_SIGNAL;
		break;
	    }
a74 2
    } else {
	status = SYS_ARG_NOACCESS;
d77 2
a78 1
    Mach_UnsetJump();
a105 1

a112 1
    Mach_SetJumpState	setJumpState;
d114 6
d121 1
a121 4
     * We have to use set-jump instead of MakeAccessible because we would have 
     * to sleep for an indeterminate amount of time with something made
     * accessible which is not a good idea.  Note that we assume that a user
     * pointer is directly accessible from the kernel.
d123 3
a125 8
    if (Mach_SetJump(&setJumpState) == SUCCESS) {
	MASTER_LOCK(sched_Mutex);
	/*
	 * release the monitor lock and wait on the condition
	 */
	lockPtr->inUse = 0;
	lockPtr->waiting = FALSE;
	SyncEventWakeupInt((unsigned int)lockPtr);
d127 2
a128 5
	if (SyncEventWaitInt(event, wakeIfSignal)) {
	    status = GEN_ABORTED_BY_SIGNAL;
	} else {
	    status = SUCCESS;
	}
d130 1
a130 1
	status = SYS_ARG_NOACCESS;
d133 2
a134 1
    Mach_UnsetJump();
d165 3
a167 2
    int *newWaitFlagPtr;
    int len;
d169 4
a172 5
    Vm_MakeAccessible(VM_READWRITE_ACCESS, 
			sizeof(int), (Address) waitFlagPtr, 
			&len, (Address *) &newWaitFlagPtr);
    if (len != sizeof(int)) {
	return(SYS_ARG_NOACCESS);
d182 2
a183 1
    Vm_MakeUnaccessible((Address) newWaitFlagPtr, len);
@


5.3
log
@Removed un-needed calls to VmMach_SetupContext
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 5.2 87/12/16 13:04:06 nelson Exp $ SPRITE (Berkeley)";
d20 1
d55 1
a55 1
    Sys_SetJumpState	setJumpState;
d59 1
a59 1
    procPtr = Proc_GetCurrentProc(Sys_GetProcessorNumber());
d67 1
a67 1
    if (Sys_SetJump(&setJumpState) == SUCCESS) {
d84 1
a84 1
    Sys_UnsetJump();
d120 1
a120 1
    Sys_SetJumpState	setJumpState;
d128 1
a128 1
    if (Sys_SetJump(&setJumpState) == SUCCESS) {
d146 1
a146 1
    Sys_UnsetJump();
@


5.2
log
@Fixed bug in user level synchronization.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 5.1 87/12/12 16:41:35 nelson Exp $ SPRITE (Berkeley)";
a72 1
	    VmMach_SetupContext(procPtr);
a144 1
    VmMach_SetupContext(Proc_GetCurrentProc(Sys_GetProcessorNumber()));
@


5.1
log
@Moved call to set up the context out of the master lock so that it could
be monitored.
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 5.0 87/08/11 10:51:07 sprite Exp $ SPRITE (Berkeley)";
d72 3
a83 1
    VmMach_SetupContext(procPtr);
@


5.0
log
@First Sprite native copy
@
text
@d16 1
a16 1
static char rcsid[] = "$Header: syncUser.c,v 4.2 87/04/08 11:30:05 nelson Exp $ SPRITE (Berkeley)";
d81 1
d144 1
@
