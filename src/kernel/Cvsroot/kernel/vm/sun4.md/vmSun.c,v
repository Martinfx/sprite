head     9.44;
branch   ;
access   ;
symbols  ds3100:9.44 sun3:9.44 sun4nw:9.40 symm:9.40 spur:9.40 Summer89:9.0;
locks    ; strict;
comment  @ * @;


9.44
date     92.10.26.12.02.12;  author elm;  state Exp;
branches ;
next     9.43;

9.43
date     92.08.10.23.04.00;  author mgbaker;  state Exp;
branches ;
next     9.42;

9.42
date     92.07.30.17.23.10;  author mgbaker;  state Exp;
branches ;
next     9.41;

9.41
date     92.07.28.14.59.04;  author mgbaker;  state Exp;
branches ;
next     9.40;

9.40
date     91.10.18.01.10.45;  author dlong;  state Exp;
branches ;
next     9.39;

9.39
date     91.10.08.13.59.58;  author shirriff;  state Exp;
branches ;
next     9.38;

9.38
date     91.09.24.23.18.01;  author shirriff;  state Exp;
branches ;
next     9.37;

9.37
date     91.09.10.18.30.34;  author rab;  state Exp;
branches ;
next     9.36;

9.36
date     91.08.20.12.22.58;  author mgbaker;  state Exp;
branches ;
next     9.35;

9.35
date     91.08.19.18.36.08;  author shirriff;  state Exp;
branches ;
next     9.34;

9.34
date     91.08.12.22.19.25;  author shirriff;  state Exp;
branches ;
next     9.33;

9.33
date     91.08.09.15.03.08;  author shirriff;  state Exp;
branches ;
next     9.32;

9.32
date     91.07.26.17.12.30;  author shirriff;  state Exp;
branches ;
next     9.31;

9.31
date     91.06.29.19.03.54;  author mgbaker;  state Exp;
branches ;
next     9.30;

9.30
date     91.03.29.18.46.09;  author mgbaker;  state Exp;
branches ;
next     9.29;

9.29
date     90.12.07.13.46.37;  author mgbaker;  state Exp;
branches ;
next     9.28;

9.28
date     90.12.06.17.59.47;  author mgbaker;  state Exp;
branches ;
next     9.27;

9.27
date     90.10.19.15.50.03;  author jhh;  state Exp;
branches ;
next     9.26;

9.26
date     90.09.20.16.30.24;  author mgbaker;  state Exp;
branches ;
next     9.25;

9.25
date     90.09.20.16.27.47;  author mgbaker;  state Exp;
branches ;
next     9.24;

9.24
date     90.09.12.13.31.50;  author mendel;  state Exp;
branches ;
next     9.23;

9.23
date     90.09.11.10.47.15;  author shirriff;  state Exp;
branches ;
next     9.22;

9.22
date     90.08.14.18.53.46;  author mgbaker;  state Exp;
branches ;
next     9.21;

9.21
date     90.07.16.11.10.07;  author mendel;  state Exp;
branches 9.21.1.1;
next     9.20;

9.20
date     90.06.21.14.00.39;  author shirriff;  state Exp;
branches ;
next     9.19;

9.19
date     90.03.14.09.16.49;  author mendel;  state Exp;
branches ;
next     9.18;

9.18
date     90.01.29.19.34.43;  author mgbaker;  state Exp;
branches ;
next     9.17;

9.17
date     89.12.12.18.06.16;  author mgbaker;  state Exp;
branches ;
next     9.16;

9.16
date     89.11.30.11.54.23;  author mgbaker;  state Exp;
branches ;
next     9.15;

9.15
date     89.11.28.12.07.13;  author brent;  state Exp;
branches ;
next     9.14;

9.14
date     89.11.27.14.54.20;  author mgbaker;  state Exp;
branches ;
next     9.13;

9.13
date     89.11.21.19.37.28;  author mgbaker;  state Exp;
branches ;
next     9.12;

9.12
date     89.10.30.20.32.20;  author mgbaker;  state Exp;
branches ;
next     9.11;

9.11
date     89.10.30.18.03.58;  author shirriff;  state Exp;
branches ;
next     9.10;

9.10
date     89.10.22.23.43.26;  author shirriff;  state Exp;
branches ;
next     9.9;

9.9
date     89.10.22.23.02.34;  author mgbaker;  state Exp;
branches ;
next     9.8;

9.8
date     89.10.19.15.50.21;  author shirriff;  state Exp;
branches ;
next     9.7;

9.7
date     89.10.18.10.44.01;  author mgbaker;  state Exp;
branches ;
next     9.6;

9.6
date     89.10.11.17.36.01;  author mgbaker;  state Exp;
branches ;
next     9.5;

9.5
date     89.10.10.14.30.34;  author mgbaker;  state Exp;
branches ;
next     9.4;

9.4
date     89.09.18.21.01.38;  author mgbaker;  state Exp;
branches ;
next     9.3;

9.3
date     89.09.14.19.02.53;  author shirriff;  state Exp;
branches ;
next     9.2;

9.2
date     89.09.14.15.19.03;  author mgbaker;  state Exp;
branches ;
next     9.1;

9.1
date     89.09.13.16.58.01;  author mgbaker;  state Exp;
branches ;
next     9.0;

9.0
date     89.09.12.15.24.20;  author douglis;  state Stable;
branches ;
next     1.24;

1.24
date     89.09.12.14.17.28;  author mgbaker;  state Exp;
branches ;
next     1.23;

1.23
date     89.09.08.16.22.47;  author mgbaker;  state Exp;
branches ;
next     1.22;

1.22
date     89.08.30.12.25.46;  author mgbaker;  state Exp;
branches ;
next     1.21;

1.21
date     89.08.25.13.42.07;  author mendel;  state Exp;
branches ;
next     1.20;

1.20
date     89.08.25.12.27.01;  author mgbaker;  state Exp;
branches ;
next     1.19;

1.19
date     89.08.10.00.12.21;  author mgbaker;  state Exp;
branches ;
next     1.18;

1.18
date     89.08.09.15.48.25;  author mgbaker;  state Exp;
branches ;
next     1.17;

1.17
date     89.08.09.12.39.05;  author mendel;  state Exp;
branches ;
next     1.16;

1.16
date     89.07.31.12.58.37;  author mgbaker;  state Exp;
branches ;
next     1.15;

1.15
date     89.07.18.13.12.36;  author mgbaker;  state Exp;
branches ;
next     1.14;

1.14
date     89.06.16.08.55.14;  author mendel;  state Exp;
branches ;
next     1.13;

1.13
date     89.06.02.10.23.03;  author mendel;  state Exp;
branches ;
next     1.12;

1.12
date     89.05.24.15.51.47;  author mgbaker;  state Exp;
branches ;
next     1.11;

1.11
date     89.05.04.23.29.40;  author mgbaker;  state Exp;
branches ;
next     1.10;

1.10
date     89.05.02.23.26.11;  author mgbaker;  state Exp;
branches ;
next     1.9;

1.9
date     89.04.30.18.18.41;  author mgbaker;  state Exp;
branches ;
next     1.8;

1.8
date     89.04.29.19.52.03;  author mgbaker;  state Exp;
branches ;
next     1.7;

1.7
date     89.04.25.15.23.33;  author mgbaker;  state Exp;
branches ;
next     1.6;

1.6
date     89.04.23.22.07.50;  author mgbaker;  state Exp;
branches ;
next     1.5;

1.5
date     89.04.21.23.33.16;  author mgbaker;  state Exp;
branches ;
next     1.4;

1.4
date     89.03.06.12.01.22;  author mgbaker;  state Exp;
branches ;
next     1.3;

1.3
date     89.03.03.15.40.55;  author mgbaker;  state Exp;
branches ;
next     1.2;

1.2
date     89.02.24.15.03.04;  author mgbaker;  state Exp;
branches ;
next     1.1;

1.1
date     89.02.23.12.58.43;  author mgbaker;  state Exp;
branches ;
next     ;

9.21.1.1
date     90.08.07.18.23.19;  author mgbaker;  state Exp;
branches ;
next     ;


desc
@It gets through vm initialization now.
@


9.44
log
@Added VmMachIsXbusMem.
@
text
@/*
 * vmSun.c -
 *
 *     	This file contains all hardware-dependent vm C routines for Sun2's, 3's 
 *	and 4's.  I will not attempt to explain the Sun mapping hardware in 
 *	here.  See the sun architecture manuals for details on
 *	the mapping hardware.
 *
 * Copyright 1990 Regents of the University of California
 * Permission to use, copy, modify, and distribute this
 * software and its documentation for any purpose and without
 * fee is hereby granted, provided that the above copyright
 * notice appear in all copies.  The University of California
 * makes no representations about the suitability of this
 * software for any purpose.  It is provided "as is" without
 * express or implied warranty.
 */

#ifndef lint
static char rcsid[] = "$Header: /sprite/src/kernel/Cvsroot/kernel/vm/sun4.md/vmSun.c,v 9.43 92/08/10 23:04:00 mgbaker Exp $ SPRITE (Berkeley)";
#endif not lint

#include <sprite.h>
#include <vmSunConst.h>
#include <machMon.h>
#include <vm.h>
#include <vmInt.h>
#include <vmMach.h>
#include <vmMachInt.h>
#include <list.h>
#include <mach.h>
#include <proc.h>
#include <sched.h>
#include <stdlib.h>
#include <sync.h>
#include <sys.h>
#include <dbg.h>
#include <net.h>
#include <stdio.h>
#include <bstring.h>
#include <recov.h>

#if (MACH_MAX_NUM_PROCESSORS == 1) /* uniprocessor implementation */
#undef MASTER_LOCK
#undef MASTER_UNLOCK
#define MASTER_LOCK(x) DISABLE_INTR()
#define MASTER_UNLOCK(x) ENABLE_INTR()
#else

/*
 * The master lock to synchronize access to pmegs and context.
 */
static Sync_Semaphore vmMachMutex;
static Sync_Semaphore *vmMachMutexPtr = &vmMachMutex;
#endif

#ifndef sun4c
/*
 * Macros to translate from a virtual page to a physical page and back.
 * For the sun4c, these are no longer macros, since they are much more
 * complicated due to physical memory no longer being contiguous.  For
 * speed perhaps someday they should be converted to complicated macros
 * instead of functions.
 */
#define VirtToPhysPage(pfNum) ((pfNum) << VMMACH_CLUSTER_SHIFT)
#define PhysToVirtPage(pfNum) ((pfNum) >> VMMACH_CLUSTER_SHIFT)
#endif

extern int debugVmStubs; /* Unix compat debug flag. */

/*
 * Convert from page to hardware segment, with correction for
 * any difference between virtAddrPtr offset and segment offset.
 * (This difference will only happen for shared segments.)
*/
#define PageToOffSeg(page,virtAddrPtr) (PageToSeg((page)- \
	segOffset(virtAddrPtr)+(virtAddrPtr->segPtr->offset)))

extern	Address	vmStackEndAddr;

static int GetNumPages _ARGS_((void));
static int GetNumValidPages _ARGS_((Address virtAddr));
static void FlushValidPages _ARGS_ ((Address virtAddr));
#ifdef sun4c
static int VirtToPhysPage _ARGS_((int pfNum));
static int PhysToVirtPage _ARGS_((int pfNum));
#endif /* sun4c */
static void VmMachSetSegMapInContext _ARGS_((unsigned int context,
	Address addr, unsigned int pmeg));
static void MMUInit _ARGS_((int firstFreeSegment));
ENTRY static void CopySegData _ARGS_((register Vm_Segment *segPtr,
	register VmMach_SegData *oldSegDataPtr,
	register VmMach_SegData *newSegDataPtr));
static void SegDelete _ARGS_((Vm_Segment *segPtr));
INTERNAL static int PMEGGet _ARGS_((Vm_Segment  *softSegPtr, int hardSegNum,
	Boolean flags));
INTERNAL static void PMEGFree _ARGS_((int pmegNum));
ENTRY static Boolean PMEGLock _ARGS_ ((register VmMach_SegData *machPtr,
	int segNum));
INTERNAL static void SetupContext _ARGS_((register Proc_ControlBlock
	*procPtr));
static void InitNetMem _ARGS_((void));
ENTRY static void WriteHardMapSeg _ARGS_((VmMach_ProcData *machPtr));
static void PageInvalidate _ARGS_((register Vm_VirtAddr *virtAddrPtr,
	unsigned int virtPage, Boolean segDeletion));
INTERNAL static void DevBufferInit _ARGS_((void));
static void VmMachTrap _ARGS_((void));
#ifndef sun4c
INTERNAL static void Dev32BitDMABufferInit _ARGS_((void));
#endif

/*----------------------------------------------------------------------
 * 
 * 			Hardware data structures
 *
 * Terminology: 
 *	1) Physical page frame: A frame that contains one hardware page.
 *	2) Virtual page frame:  A frame that contains VMMACH_CLUSTER_SIZE 
 *				hardware pages.
 *	3) Software segment: The segment structure used by the hardware
 *			     independent VM module.
 *	4) Hardware segment: A piece of a hardware context.
 *
 * A hardware context corresponds to a process's address space.  A context
 * is made up of many equal sized hardware segments.  The 
 * kernel is mapped into each hardware context so that the kernel has easy
 * access to user data.  One context (context 0) is reserved for use by
 * kernel processes.  The hardware contexts are defined by the array
 * contextArray which contains an entry for each context.  Each entry 
 * contains a pointer back to the process that is executing in the context 
 * and an array which is an exact duplicate of the hardware segment map for
 * the context.  The contexts are managed by keeping all contexts except for
 * the system context in a list that is kept in LRU order.  Whenever a context 
 * is needed the first context off of the list is used and the context is
 * stolen from the current process that owns it if necessary.
 *
 * PMEGs are allocated to software segments in order to allow pages to be mapped
 * into the segment's virtual address space. There are only a few hundred
 * PMEGs, which have to be shared by all segments.  PMEGs that have
 * been allocated to user segments can be stolen at any time.  PMEGs that have
 * been allocated to the system segment cannot be taken away unless the system
 * segment voluntarily gives it up.  In order to manage the PMEGs there are
 * two data structures.  One is an array of PMEG info structures that contains
 * one entry for each PMEG.  The other is an array stored with each software
 * segment struct that contains the PMEGs that have been allocated to the
 * software segment.  Each entry in the array of PMEG info structures
 * contains enough information to remove the PMEG from its software segment.
 * One of the fields in the PMEG info struct is the count of pages that have
 * been validated in the PMEG.  This is used to determine when a PMEG is no
 * longer being actively used.  
 *
 * There are two lists that are used to manage PMEGs.  The pmegFreeList 
 * contains all PMEGs that are either not being used or contain no valid 
 * page map entries; unused ones are inserted at the front of the list 
 * and empty ones at the rear.  The pmegInuseList contains all PMEGs
 * that are being actively used to map user segments and is managed as a FIFO.
 * PMEGs that are being used to map tbe kernel's VAS do not appear on the 
 * pmegInuseList. When a pmeg is needed to map a virtual address, first the
 * free list is checked.  If it is not empty then the first PMEG is pulled 
 * off of the list.  If it is empty then the first PMEG is pulled off of the
 * inUse list.  If the PMEG  that is selected is being used (either actively 
 * or inactively) then it is freed from the software segment that is using it.
 * Once the PMEG is freed up then if it is being allocated for a user segment
 * it is put onto the end of the pmegInuseList.
 *
 * Page frames are allocated to software segments even when there is
 * no PMEG to map it in.  Thus when a PMEG that was mapping a page needs to
 * be removed from the software segment that owns the page, the reference
 * and modify bits stored in the PMEG for the page must be saved.  The
 * array refModMap is used for this.  It contains one entry for each
 * virtual page frame.  Its value for a page frame or'd with the bits stored
 * in the PMEG (if any) comprise the referenced and modified bits for a 
 * virtual page frame.
 * 
 * IMPORTANT SYNCHRONIZATION NOTE:
 *
 * The internal data structures in this file have to be protected by a
 * master lock if this code is to be run on a multi-processor.  Since a
 * process cannot start executing unless VmMach_SetupContext can be
 * executed first, VmMach_SetupContext cannot context switch inside itself;
 * otherwise a deadlock will occur.  However, VmMach_SetupContext mucks with
 * contexts and PMEGS and therefore would have to be synchronized
 * on a multi-processor.  A monitor lock cannot be used because it may force
 * VmMach_SetupContext to be context switched.
 *
 * The routines in this file also muck with other per segment data structures.
 * Access to these data structures is synchronized by our caller (the
 * machine independent module).
 *
 *----------------------------------------------------------------------
 */

/*
 * Machine dependent flags for the flags field in the Vm_VirtAddr struct.
 * We are only allowed to use the second byte of the flags.
 *
 *	USING_MAPPED_SEG		The parsed virtual address falls into
 *					the mapping segment.
 */
#define	USING_MAPPED_SEG	0x100

/*
 * Macros to get to and from hardware segments and pages.
 */
#define PageToSeg(page) ((page) >> (VMMACH_SEG_SHIFT - VMMACH_PAGE_SHIFT))
#define SegToPage(seg) ((seg) << (VMMACH_SEG_SHIFT - VMMACH_PAGE_SHIFT))

#if (VMMACH_CLUSTER_SIZE == 1) 
/*
 * Macro to set all page map entries for the given virtual address.
 */
#define	SET_ALL_PAGE_MAP(virtAddr, pte) { \
	VmMachSetPageMap((Address)(virtAddr), (pte)); \
}
/*
 * Macro to flush all pages for the given virtual address.
 */
#define	FLUSH_ALL_PAGE(virtAddr) { \
	VmMachFlushPage((Address)(virtAddr)); \
}
#else
#define	SET_ALL_PAGE_MAP(virtAddr, pte) { \
    int	__i; \
    for (__i = 0; __i < VMMACH_CLUSTER_SIZE; __i++) { \
	VmMachSetPageMap((virtAddr) + __i * VMMACH_PAGE_SIZE_INT, (pte) + __i); \
    } \
}
#define	FLUSH_ALL_PAGE(virtAddr) { \
    int	__i; \
    for (__i = 0; __i < VMMACH_CLUSTER_SIZE; __i++) { \
	VmMachFlushPage((virtAddr) + __i * VMMACH_PAGE_SIZE_INT); \
    } \
}
#endif

/*
 * PMEG table entry structure.
 */
typedef struct {
    List_Links			links;		/* Links so that the pmeg */
              					/* can be in a list */
    struct VmMach_PMEGseg	segInfo;	/* Info on software segment. */
    int				pageCount;	/* Count of resident pages in
						 * this pmeg. */
    int				lockCount;	/* The number of times that
						 * this PMEG has been locked.*/
    int				flags;		/* Flags defined below. */
} PMEG;

/*
 * Flags to indicate the state of a pmeg.
 *
 *    PMEG_DONT_ALLOC	This pmeg should not be reallocated.  This is 
 *			when a pmeg cannot be reclaimed until it is
 *			voluntarily freed.
 *    PMEG_NEVER_FREE	Don't ever free this pmeg no matter what anybody says.
 */
#define	PMEG_DONT_ALLOC		0x1
#define	PMEG_NEVER_FREE		0x2

/*
 * Pmeg information.  pmegArray contains one entry for each pmeg.  pmegFreeList
 * is a list of all pmegs that aren't being actively used.  pmegInuseList
 * is a list of all pmegs that are being actively used.
 */

static	unsigned int	vmNumPmegs;
	unsigned int	vmPmegMask;
static	PMEG   		*pmegArray;
static	List_Links   	pmegFreeListHeader;
static	List_Links   	*pmegFreeList = &pmegFreeListHeader;
static	List_Links   	pmegInuseListHeader;
static	List_Links   	*pmegInuseList = &pmegInuseListHeader;

#ifdef sun4c
unsigned int	vmCacheLineSize;
unsigned int	vmCacheSize;
unsigned int	vmCacheShift;
#endif

/*
 * The context table structure.
 */
typedef struct VmMach_Context {
    List_Links			links;	 /* Links so that the contexts can be
					     in a list. */
    struct Proc_ControlBlock	*procPtr;	/* A pointer to the process
						 * table entry for the process
						 * that is running in this
						 * context. */
    VMMACH_SEG_NUM		map[VMMACH_NUM_SEGS_PER_CONTEXT];
					/* A reflection of the hardware context
					 * map. */
    unsigned int		context;/* Which context this is. */
    int				flags;	/* Defined below. */
} VmMach_Context;

/*
 * Context flags:
 *
 *     	CONTEXT_IN_USE	This context is used by a process.
 */
#define	CONTEXT_IN_USE	0x1

/*
 * Context information.  contextArray contains one entry for each context. 
 * contextList is a list of contexts in LRU order.
 */
#ifdef sun4c
static	unsigned int	vmNumContexts;
static	VmMach_Context	*contextArray;
#else
static	VmMach_Context	contextArray[VMMACH_NUM_CONTEXTS];
#endif
static	List_Links   	contextListHeader;
static	List_Links   	*contextList = &contextListHeader;

/*
 * Map containing one entry for each virtual page.
 */
static	VmMachPTE		*refModMap;

/*
 * Macro to get a pointer into a software segment's hardware segment table.
 */
#ifdef CLEAN
#define GetHardSegPtr(machPtr, segNum) \
    ((machPtr)->segTablePtr + (segNum) - (machPtr)->offset)
#else
#define GetHardSegPtr(machPtr, segNum) \
    ( ((unsigned)((segNum) - (machPtr)->offset) > (machPtr)->numSegs) ? \
    (panic("Invalid segNum\n"),(machPtr)->segTablePtr) : \
    ((machPtr)->segTablePtr + (segNum) - (machPtr)->offset) )
#endif

/*
 * Macro to check to see if address is in a hardware segment (PMEG) that
 * is used by file cache virtual addresses.
 * These kernel segment pmegs may be stolen.
 */

#define	_ROUND2SEG(x) ((unsigned int)(x)  & ~(VMMACH_SEG_SIZE-1))
#define	IN_FILE_CACHE_SEG(addr) 					     \
          ( ((unsigned int)(addr) >=					     \
		_ROUND2SEG(vmBlockCacheBaseAddr + (VMMACH_SEG_SIZE-1))) &&   \
	    ((unsigned int)(addr) < _ROUND2SEG(vmBlockCacheEndAddr)) )

/*
 * TRUE if stealing of file cache pmegs are permitted. Initialized to FALSE
 * for backward compat with old file cache code.
 */
Boolean vmMachCanStealFileCachePmegs = FALSE;

/*
 * The maximum amount of kernel code + data available.  This is set to however
 * big you want it to be to make sure that the kernel heap has enough memory
 * for all the file handles, etc.
 */
#ifdef sun2
int	vmMachKernMemSize = 2048 * 1024;
#endif
#ifdef sun3
int     vmMachKernMemSize = 8192 * 1024;
#endif
#ifdef sun4
#ifdef sun4c
int	vmMachKernMemSize = 32 * 1024 * 1024;
#else
int	vmMachKernMemSize = 40 * 1024 * 1024;
#endif
#endif

/*
 * The segment that is used to map a segment into a process's virtual address
 * space for cross-address-space copies.
 */
#define	MAP_SEG_NUM (((unsigned int) VMMACH_MAP_SEG_ADDR) >> VMMACH_SEG_SHIFT)

static	VmMach_SegData	*sysMachPtr;
Address			vmMachPTESegAddr;
Address			vmMachPMEGSegAddr;

#ifdef sun4c
/*
 * Structure for mapping virtual page frame numbers to physical page frame
 * numbers and back for each memory board.
 */
typedef struct	{
    unsigned int	endVirPfNum;	/* Ending virtual page frame number
					 * on board. */
    unsigned int	startVirPfNum;	/* Starting virtual page frame number
					 * on board. */
    unsigned int	physStartAddr;	/* Physical address of page frame. */
    unsigned int	physEndAddr;	/* End physical address of page frame.*/
} Memory_Board;

/*
 * Pointer to board after last configured Memory board structure.
 */
static	Memory_Board	*lastMemBoard;

/*
 * Memory_Board structures for each board in the system.  This array is sorted
 * by endVirPfNum.
 */
static	Memory_Board	Mboards[6];
#endif /* sun4c */

/*
 * vmMachHasVACache - TRUE if the machine has a virtual address cache.
 * vmMachHasHwFlush - TRUE if the machine has the hardware assist cache flush
 *		      option.
 */
Boolean	vmMachHasVACache	= TRUE;
Boolean	vmMachHasHwFlush	= FALSE;

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_BootInit --
 *
 *      Do hardware dependent boot time initialization.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Hardware page map for the kernel is initialized.  Also the various size
 * 	fields are filled in.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_BootInit(pageSizePtr, pageShiftPtr, pageTableIncPtr, kernMemSizePtr,
		numKernPagesPtr, maxSegsPtr, maxProcessesPtr)
    int	*pageSizePtr;
    int	*pageShiftPtr;
    int	*pageTableIncPtr;
    int	*kernMemSizePtr;
    int	*numKernPagesPtr;
    int	*maxSegsPtr;
    int *maxProcessesPtr;
{
    register Address	virtAddr;
    register int	i;
    int			kernPages;
    int			numPages;
#ifdef sun4c
    Mach_MemList	*memPtr;
    int			nextVframeNum, numFrames;
#endif

#if (MACH_MAX_NUM_PROCESSORS != 1) /* multiprocessor implementation */
    Sync_SemInitDynamic(&vmMachMutex, "Vm:vmMachMutex");
#endif

#ifdef sun4c
    /*
     * Initialize the physical to virtual page mappings, since memory isn't
     * contiguous.
     */
    lastMemBoard = Mboards;
    nextVframeNum = 0;
    if (romVectorPtr->v_romvec_version >= 2) {
	Mach_MemList mlist[128];
	int	 numMemList, i;

	numMemList = Mach_MonSearchProm("memory", "available", (char *) mlist,
					sizeof(mlist)) / sizeof(Mach_MemList);
	/*
	 * XXX - fix this
	 * Currently the VM module assumes that the kernel
	 * is in phsyically contiguous memory. Kernel page 0 is in 
	 * phsyical page zero (or at least startVirPfNum == 0). The
	 * Prom on the 4/75 returns memlist is reverse order with
	 * phsical zero last.
	 */
	for (i = numMemList - 1; i >= 0; --i) {
	    memPtr = mlist + i;
	    if (memPtr->size != 0) {
		numFrames = memPtr->size / VMMACH_PAGE_SIZE;
		lastMemBoard->startVirPfNum = nextVframeNum;
		lastMemBoard->endVirPfNum = nextVframeNum + numFrames;
		nextVframeNum += numFrames;
		lastMemBoard->physStartAddr =
		    memPtr->address >> VMMACH_PAGE_SHIFT_INT;
		lastMemBoard->physEndAddr = lastMemBoard->physStartAddr +
		    numFrames * VMMACH_CLUSTER_SIZE;
		lastMemBoard++;
	    }
	}
    } else {
	for (memPtr = *(romVectorPtr->availMemory);
		memPtr != (Mach_MemList *) 0;
		memPtr = memPtr->next) {
	    if (memPtr->size != 0) {
		numFrames = memPtr->size / VMMACH_PAGE_SIZE;
		lastMemBoard->startVirPfNum = nextVframeNum;
		lastMemBoard->endVirPfNum = nextVframeNum + numFrames;
		nextVframeNum += numFrames;
		lastMemBoard->physStartAddr =
		    memPtr->address >> VMMACH_PAGE_SHIFT_INT;
		lastMemBoard->physEndAddr = lastMemBoard->physStartAddr +
		    numFrames * VMMACH_CLUSTER_SIZE;
		lastMemBoard++;
	    }
	}
    }
    if (lastMemBoard == Mboards) {
	panic("No memory boards in system configuration.");
    }

    if (Mach_MonSearchProm("*", "vac-linesize", (char *)&vmCacheLineSize,
	    sizeof vmCacheLineSize) == -1) {
	vmCacheLineSize = VMMACH_CACHE_LINE_SIZE_60;
    }
    vmCacheSize = VMMACH_CACHE_SIZE;	/* for assembly code */
    {
	unsigned i = vmCacheLineSize;
	vmCacheShift = 0;
	while (i >>= 1) {
	    ++vmCacheShift;
	}
    }
    if (Mach_MonSearchProm("*", "vac_hwflush", (char *)&vmMachHasHwFlush,
	    sizeof vmMachHasHwFlush) == -1) {
	vmMachHasHwFlush = FALSE;
    }
    if (Mach_MonSearchProm("*", "mmu-npmg", (char *)&vmNumPmegs,
	    sizeof vmNumPmegs) == -1) {
	vmNumPmegs = VMMACH_NUM_PMEGS_60;
    }
#else /* sun4c */
    switch (Mach_GetMachineType()) {
	case SYS_SUN_4_110:
	    vmNumPmegs = VMMACH_NUM_PMEGS_110;
	    vmMachHasVACache = FALSE;
	    break;
	case SYS_SUN_4_260:
	    vmNumPmegs = VMMACH_NUM_PMEGS_260;
	    break;
	case SYS_SUN_4_330:
	    vmNumPmegs = VMMACH_NUM_PMEGS_330;
	    break;
	case SYS_SUN_4_470:
	    vmNumPmegs = VMMACH_NUM_PMEGS_470;
	    break;
	default:
	    panic("What sort of machine type is %x?\n", Mach_GetMachineType());
    }
#endif /* sun4c */
    vmPmegMask = vmNumPmegs - 1;
    
    kernPages = vmMachKernMemSize / VMMACH_PAGE_SIZE_INT;
    /*
     * Map all of the kernel memory that we might need one for one.  We know
     * that the monitor maps the first part of memory one for one but for some
     * reason it doesn't map enough.  We assume that the pmegs have been
     * mapped correctly.
     */
    for (i = 0, virtAddr = (Address)mach_KernStart; 
	i < kernPages;
	i++, virtAddr += VMMACH_PAGE_SIZE_INT) {
	if (VmMachGetSegMap(virtAddr) != VMMACH_INV_PMEG) {
	    VmMachPTE pte = VmMachGetPageMap(virtAddr);
	    if (pte & VMMACH_RESIDENT_BIT) { 
		pte &= VMMACH_PAGE_FRAME_FIELD;
#ifdef sun4
		pte |= VMMACH_DONT_CACHE_BIT;
#endif
		VmMachSetPageMap(virtAddr, 
		    (VmMachPTE)(VMMACH_KRW_PROT | VMMACH_RESIDENT_BIT | pte));
	    } else {
		printf("VmMach_BootInit: Last page ends at %x.\n", virtAddr);
		break;
	    }
	} else {
	    printf("VmMach_BootInit: Last segment ends at %x.\n", virtAddr);
	    break;
	}
    }

    /*
     * Do boot time allocation.
     */
    pmegArray = (PMEG *)Vm_BootAlloc(sizeof(PMEG) * vmNumPmegs);
#ifdef sun4c
    if (Mach_MonSearchProm("*", "mmu-nctx", (char *)&vmNumContexts,
	    sizeof vmNumContexts) == -1) {
	vmNumContexts = VMMACH_NUM_CONTEXTS_60;
    }
    contextArray = (VmMach_Context *)Vm_BootAlloc(sizeof(VmMach_Context) *
	    vmNumContexts);
#endif
    sysMachPtr = (VmMach_SegData *)Vm_BootAlloc(sizeof(VmMach_SegData) + 
	    (sizeof (VMMACH_SEG_NUM) * VMMACH_NUM_SEGS_PER_CONTEXT));
    numPages = GetNumPages();
    refModMap = (VmMachPTE *)Vm_BootAlloc(sizeof(VmMachPTE) * numPages);

    /*
     * Return lots of sizes to the machine independent module who called us.
     */
    *pageSizePtr = VMMACH_PAGE_SIZE;
    *pageShiftPtr = VMMACH_PAGE_SHIFT;
    *pageTableIncPtr = VMMACH_PAGE_TABLE_INCREMENT;
    *kernMemSizePtr = vmMachKernMemSize;
    *maxProcessesPtr = VMMACH_MAX_KERN_STACKS;
    *numKernPagesPtr = numPages;
    /* 
     * We don't care how many software segments there are so return -1 as
     * the max.
     */
    *maxSegsPtr = -1;
}


/*
 * ----------------------------------------------------------------------------
 *
 * GetNumPages --
 *
 *     Determine how many pages of physical memory there are.
 *     For the sun4c, this determines how many physical pages of memory
 *     are available after the prom has grabbed some.
 *
 * Results:
 *     The number of physical pages available.
 *
 * Side effects:
 *     None.
 *
 * ----------------------------------------------------------------------------
 */
static int
GetNumPages()
{
#ifdef sun4c
    int	memory = 0;
    Mach_MemList	*memPtr;

    if (romVectorPtr->v_romvec_version >= 2) {	
	Mach_MemList mlist[128];
	int	 numMemList, i;

	numMemList = Mach_MonSearchProm("memory", "available", (char *) mlist,
					sizeof(mlist)) / sizeof(Mach_MemList);
	for (i = 0; i < numMemList; i++) {
	    memPtr = mlist + i;
	    memory += memPtr->size / VMMACH_PAGE_SIZE;
	}
    } else {
	for (memPtr = *(romVectorPtr->availMemory);
		memPtr != (Mach_MemList *) 0;
		memPtr = memPtr->next) {
	    memory += memPtr->size / VMMACH_PAGE_SIZE;
	}
    }
    return memory;
#else
    return (*romVectorPtr->memoryAvail / VMMACH_PAGE_SIZE);
#endif
}

#ifdef sun4c

/*
 * ----------------------------------------------------------------------------
 *
 * VirtToPhysPage --
 *
 *     Translate from a virtual page to a physical page.
 *     This was a macro on the other suns, but for the sun4c, physical
 *     memory isn't contiguous.
 *
 * Results:
 *     An address.
 *
 * Side effects:
 *     None.
 *
 * ----------------------------------------------------------------------------
 */
static int
VirtToPhysPage(pfNum)
    int		pfNum;
{
    register	Memory_Board	*mb;

    for (mb = Mboards; mb < lastMemBoard; mb++) {
	if (pfNum < mb->endVirPfNum && pfNum >= mb->startVirPfNum) {
	    return mb->physStartAddr +
		(pfNum - mb->startVirPfNum) * VMMACH_CLUSTER_SIZE;
	}
    }
    panic("VirtToPhysPage: virtual page %x not found.\n", pfNum);
    return NIL;
}


/*
 * ----------------------------------------------------------------------------
 *
 * PhysToVirtPage --
 *
 *     Translate from a physical page to a virtual page.
 *     This was a macro on the other suns, but for the sun4c, physical
 *     memory isn't contiguous.
 *
 * Results:
 *     An address.
 *
 * Side effects:
 *     None.
 *
 * ----------------------------------------------------------------------------
 */
static int
PhysToVirtPage(pfNum)
    int		pfNum;
{
    register	Memory_Board	*mb;

    for (mb = Mboards; mb < lastMemBoard; mb++) {
	if (pfNum >= mb->physStartAddr && pfNum < mb->physEndAddr) {
	    return mb->startVirPfNum +
		(pfNum - mb->physStartAddr) / VMMACH_CLUSTER_SIZE;
	}
    }
    panic("PhysToVirtPage: physical page %x not found.\n", pfNum);
    return NIL;
}
#endif /* sun4c */


/*
 * ----------------------------------------------------------------------------
 *
 * VmMachSetSegMapInContext --
 *
 *	Set the segment map in a context that may not yet be mapped without
 *	causing a fault.  So far, this is only useful on the sun4c.
 *
 * Results:
 *     None.
 *
 * Side effects:
 *     The segment map in another context is modified..
 *
 * ----------------------------------------------------------------------------
 */
static void
VmMachSetSegMapInContext(context, addr, pmeg)
    unsigned	int	context;
    Address		addr;
    unsigned	int	pmeg;
{
#ifdef sun4
    /* This matters for a 4/110 */
    if (!VMMACH_ADDR_CHECK(addr)) {
	addr += VMMACH_TOP_OF_HOLE - VMMACH_BOTTOM_OF_HOLE + 1;
    }
#endif
    romVectorPtr->SetSegInContext(context, addr, pmeg);
    return;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_AllocKernSpace --
 *
 *     Allocate memory for machine dependent stuff in the kernels VAS.
 *
 * Results:
 *     None.  Well, it returns something, Mike...  It seems to return the
 *     address of the next free area.
 *
 * Side effects:
 *     None.
 *
 * ----------------------------------------------------------------------------
 */
Address
VmMach_AllocKernSpace(baseAddr)
    Address	baseAddr;
{
    /*
     * If the base address is at the beginning of a segment, we just want
     * to allocate this segment for vmMachPTESegAddr.  If base addr is partway
     * into a segment, we want a whole segment, so move to the next segment
     * to allocate that one.  (baseAddr + VMMACH_SEG_SIZE - 1) moves us to
     * next segment unless we were at the very beginning of one.  Then divide
     * by VMMACH_SEG_SIZE to get segment number.  Then multiply by
     * VMMACH_SEG_SIZE to get address of the begginning of the segment.
     */
    baseAddr = (Address) ((((unsigned int)baseAddr + VMMACH_SEG_SIZE - 1) / 
					VMMACH_SEG_SIZE) * VMMACH_SEG_SIZE);
    vmMachPTESegAddr = baseAddr;	/* first seg for Pte mapping */
    vmMachPMEGSegAddr = baseAddr + VMMACH_SEG_SIZE;	/* next for pmegs */
    return(baseAddr + 2 * VMMACH_SEG_SIZE);	/* end of allocated area */
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_Init --
 *
 *     Initialize all virtual memory data structures.
 *
 * Results:
 *     None.
 *
 * Side effects:
 *     All virtual memory linked lists and arrays are initialized.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_Init(firstFreePage)
    int	firstFreePage;	/* Virtual page that is the first free for the 
			 * kernel. */
{
    register	int 		i;
    int 			firstFreeSegment;
    Address			virtAddr;
    Address			lastCodeAddr;
    extern	int		etext;
    VMMACH_SEG_NUM		pmeg;

    /*
     * Initialize the kernel's hardware segment table.
     */
    vm_SysSegPtr->machPtr = sysMachPtr;
    sysMachPtr->numSegs = VMMACH_NUM_SEGS_PER_CONTEXT;
    sysMachPtr->offset = PageToSeg(vm_SysSegPtr->offset);
    sysMachPtr->segTablePtr =
	    (VMMACH_SEG_NUM *) ((Address)sysMachPtr + sizeof(VmMach_SegData));
    for (i = 0; i < VMMACH_NUM_SEGS_PER_CONTEXT; i++) {
	sysMachPtr->segTablePtr[i] = VMMACH_INV_PMEG;
    }

    /*
     * Determine which hardware segment is the first that is not in use.
     */
    firstFreeSegment = ((firstFreePage - 1) << VMMACH_PAGE_SHIFT) / 
					VMMACH_SEG_SIZE + 1;
    firstFreeSegment += (unsigned int)mach_KernStart >> VMMACH_SEG_SHIFT;

    /* 
     * Initialize the PMEG and context tables and lists.
     */
    MMUInit(firstFreeSegment);

    /*
     * Initialize the page map.
     */
    bzero((Address)refModMap, sizeof(VmMachPTE) * GetNumPages());

    /*
     * The code segment is read only and all other in use kernel memory
     * is read/write.  Since the loader may put the data in the same page
     * as the last code page, the last code page is also read/write.
     */
    lastCodeAddr = (Address) ((unsigned)&etext - VMMACH_PAGE_SIZE);
    for (i = 0, virtAddr = (Address)mach_KernStart;
	    i < firstFreePage;
	    virtAddr += VMMACH_PAGE_SIZE, i++) {
	register VmMachPTE pte;
	register int physPage;

	physPage = VirtToPhysPage(i);
	{
	    int j;

	    for (j = 0; j < VMMACH_CLUSTER_SIZE; ++j) {
		pte = VmMachGetPageMap(virtAddr + j * VMMACH_PAGE_SIZE_INT)
		    & VMMACH_PAGE_FRAME_FIELD;
		if (pte != physPage + j) {
		    panic("VmMach_Init: weird mapping %x != %x\n",
			pte, physPage + j);
		}
	    }
	}

	if (recov_Transparent && virtAddr >= (Address) mach_RestartTablePtr &&
		virtAddr <
		((Address) mach_RestartTablePtr + Mach_GetRestartTableSize())) {
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | physPage;
	} else if (virtAddr >= (Address)MACH_CODE_START &&
	    virtAddr <= lastCodeAddr) {
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KR_PROT | physPage;
	} else {
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | physPage;
#ifdef sun4
	    if (virtAddr >= vmStackEndAddr) {
		pte |= VMMACH_DONT_CACHE_BIT;
	    }
#endif /* sun4 */
	}
	SET_ALL_PAGE_MAP(virtAddr, pte);
    }

    /*
     * Protect the bottom of the kernel stack.
     */
    SET_ALL_PAGE_MAP((Address)mach_StackBottom, (VmMachPTE)0);

    /*
     * Invalid until the end of the last segment
     */
    for (;virtAddr < (Address) (firstFreeSegment << VMMACH_SEG_SHIFT);
	 virtAddr += VMMACH_PAGE_SIZE) {
	SET_ALL_PAGE_MAP(virtAddr, (VmMachPTE)0);
    }

    /* 
     * Zero out the invalid pmeg.
     */
    /* Need I flush something? */
    VmMachPMEGZero(VMMACH_INV_PMEG);

    /*
     * Finally copy the kernel's context to each of the other contexts.
     */
    for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	int		segNum;

	if (i == VMMACH_KERN_CONTEXT) {
	    continue;
	}
#ifdef NOTDEF
	VmMachSetUserContext(i);
	for (segNum = ((unsigned int) mach_KernStart) / VMMACH_SEG_SIZE,
		 segTablePtr = vm_SysSegPtr->machPtr->segTablePtr;
		 segNum < VMMACH_NUM_SEGS_PER_CONTEXT;
		 segNum++, segTablePtr++) {

	    virtAddr = (Address) (segNum * VMMACH_SEG_SIZE);
	    /*
	     * No need to flush stuff since the other contexts haven't
	     * been used yet.
	     */
	    VmMachSetSegMap(virtAddr, (int)*segTablePtr);
	}
#else
	/*
	 * For the sun4c, there is currently a problem just copying things
	 * out of the segTable, so do it from the real hardware.  I need to
	 * figure out what's wrong.
	 */
	for (segNum = ((unsigned int) mach_KernStart) / VMMACH_SEG_SIZE;
		 segNum < VMMACH_NUM_SEGS_PER_CONTEXT; segNum++) {

	    virtAddr = (Address) (segNum * VMMACH_SEG_SIZE);
#ifdef KERNEL_NOT_ABOVE_HOLE
	    if (!VMMACH_ADDR_CHECK(virtAddr)) {
		continue;
	    }
#endif
	    pmeg = VmMachGetSegMap(virtAddr);
	    VmMachSetSegMapInContext((unsigned char) i, virtAddr, pmeg);
	}
#endif
    }
#ifdef NOTDEF
    VmMachSetUserContext(VMMACH_KERN_CONTEXT);
#endif
#ifndef sun4
    if (Mach_GetMachineType() == SYS_SUN_3_50) {
	unsigned int vidPage;

#define	VIDEO_START	0x100000	/* From Sun3 architecture manual */
#define	VIDEO_SIZE	0x20000
	vidPage = VIDEO_START / VMMACH_PAGE_SIZE;
	if (firstFreePage > vidPage) {
	    panic("VmMach_Init: We overran video memory.\n");
	}
	/*
	 * On 3/50's the display is kept in main memory beginning at 1 
	 * Mbyte and going for 128 kbytes.  Reserve this memory so VM
	 * doesn't try to use it.
	 */
	for (;vidPage < (VIDEO_START + VIDEO_SIZE) / VMMACH_PAGE_SIZE;
	     vidPage++) {
	    Vm_ReservePage(vidPage);
	}
    }
#endif /* sun4 */
    /*
     * Turn on caching.
     */
    if (vmMachHasVACache) {
	VmMachClearCacheTags();
    }
#ifndef sun4c
    VmMachInitAddrErrorControlReg();
#endif
    VmMachInitSystemEnableReg();
}


/*
 *----------------------------------------------------------------------
 *
 * MMUInit --
 *
 *	Initialize the context table and lists and the Pmeg table and 
 *	lists.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Context table and Pmeg table are initialized.  Also context list
 *	and pmeg list are initialized.
 *
 *----------------------------------------------------------------------
 */
static void
MMUInit(firstFreeSegment)
    int		firstFreeSegment;
{
    register	int		i;
    register	PMEG		*pmegPtr;
    register	VMMACH_SEG_NUM	*segTablePtr;
    VMMACH_SEG_NUM		pageCluster;

    /*
     * Initialize the context table.
     */
    contextArray[VMMACH_KERN_CONTEXT].flags = CONTEXT_IN_USE;
    List_Init(contextList);
    for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	if (i != VMMACH_KERN_CONTEXT) {
	    contextArray[i].flags = 0;
	    List_Insert((List_Links *) &contextArray[i], 
			LIST_ATREAR(contextList));
	}
	contextArray[i].context = i;
    }

    /*
     * Initialize the page cluster list.
     */
    List_Init(pmegFreeList);
    List_Init(pmegInuseList);

    /*
     * Initialize the pmeg structure.
     */
    bzero((Address)pmegArray, VMMACH_NUM_PMEGS * sizeof(PMEG));
    for (i = 0, pmegPtr = pmegArray; i < VMMACH_NUM_PMEGS; i++, pmegPtr++) {
	pmegPtr->segInfo.segPtr = (Vm_Segment *) NIL;
	pmegPtr->flags = PMEG_DONT_ALLOC;
	pmegPtr->segInfo.nextLink = (struct VmMach_PMEGseg *)NIL;
    }

#ifdef sun2
    /*
     * Segment 0 is left alone because it is required for the monitor.
     */
    pmegArray[0].segPtr = (Vm_Segment *)NIL;
    pmegArray[0].hardSegNum = 0;
    i = 1;
#else
    i = 0;
#endif /* sun2 */

    /*
     * Invalidate all hardware segments from first segment up to the beginning
     * of the kernel.
     */
    for (; i < ((((unsigned int) mach_KernStart) & VMMACH_ADDR_MASK) >>
	    VMMACH_SEG_SHIFT); i++) {
	int	j;
	Address	addr;

	addr = (Address) (i << VMMACH_SEG_SHIFT);
	/*
	 * Copy the invalidation to all the other contexts, so that
	 * the user contexts won't have double-mapped pmegs at the low-address
	 * segments.
	 */
	for (j = 0; j < VMMACH_NUM_CONTEXTS; j++) {
	    /* Yes, do this here since user stuff would be double mapped. */
	    VmMachSetSegMapInContext((unsigned char) j, addr,
		    (unsigned char) VMMACH_INV_PMEG);
	}
    }
    i = ((unsigned int) mach_KernStart >> VMMACH_SEG_SHIFT);

    /*
     * Reserve all pmegs that have kernel code or heap.
     */
    for (segTablePtr = vm_SysSegPtr->machPtr->segTablePtr;
         i < firstFreeSegment;
	 i++, segTablePtr++) {
	pageCluster = VmMachGetSegMap((Address) (i << VMMACH_SEG_SHIFT));
	pmegArray[pageCluster].pageCount = VMMACH_NUM_PAGES_PER_SEG;
	pmegArray[pageCluster].segInfo.segPtr = vm_SysSegPtr;
	pmegArray[pageCluster].segInfo.hardSegNum = i;
	*segTablePtr = pageCluster;
    }

    /*
     * Invalidate all hardware segments that aren't in code or heap and are 
     * before the specially mapped page clusters.
     */
    for (; i < VMMACH_FIRST_SPECIAL_SEG; i++, segTablePtr++) {
	Address	addr;
	/* Yes, do this here, since user stuff would be double-mapped. */
	addr = (Address) (i << VMMACH_SEG_SHIFT);
	VmMachSetSegMap(addr, VMMACH_INV_PMEG);
    }

    /*
     * Mark the invalid pmeg so that it never gets used.
     */
    pmegArray[VMMACH_INV_PMEG].segInfo.segPtr = vm_SysSegPtr;
    pmegArray[VMMACH_INV_PMEG].flags = PMEG_NEVER_FREE;

    /*
     * Now reserve the rest of the page clusters that have been set up by
     * the monitor.  Don't reserve any PMEGs that don't have any valid 
     * mappings in them.
     */
    for (; i < VMMACH_NUM_SEGS_PER_CONTEXT; i++, segTablePtr++) {
	Address		virtAddr;
	int		j;
	VmMachPTE	pte;
	Boolean		inusePMEG;

	virtAddr = (Address) (i << VMMACH_SEG_SHIFT);
        if ((virtAddr >= (Address)VMMACH_DMA_START_ADDR) &&
	    (virtAddr < (Address)(VMMACH_DMA_START_ADDR+VMMACH_DMA_SIZE))) {
	    /*
	     * Blow away anything in DMA space. 
	     */
	    pageCluster = VMMACH_INV_PMEG;
	    VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);
	} else {
	    pageCluster = VmMachGetSegMap(virtAddr);
	    if (pageCluster != VMMACH_INV_PMEG) {
		inusePMEG = FALSE;
		for (j = 0; 
		     j < VMMACH_NUM_PAGES_PER_SEG_INT; 
		     j++, virtAddr += VMMACH_PAGE_SIZE_INT) {
		    pte = VmMachGetPageMap(virtAddr);
		    if ((pte & VMMACH_RESIDENT_BIT) &&
			(pte & (VMMACH_TYPE_FIELD|VMMACH_PAGE_FRAME_FIELD)) != 0) {
			/*
			 * A PMEG contains a valid mapping if the resident
			 * bit is set and the page frame and type field
			 * are non-zero.  On Sun 2/50's the PROM sets
			 * the resident bit but leaves the page frame equal
			 * to zero.
			 */
			if (!inusePMEG) {
			    pmegArray[pageCluster].segInfo.segPtr =
				    vm_SysSegPtr;
			    pmegArray[pageCluster].segInfo.hardSegNum = i;
			    pmegArray[pageCluster].flags = PMEG_NEVER_FREE;
			    inusePMEG = TRUE;
			}
		    } else {
			VmMachSetPageMap(virtAddr, (VmMachPTE)0);
		    }
		}
		virtAddr -= VMMACH_SEG_SIZE;
		if (!inusePMEG ||
		    (virtAddr >= (Address)VMMACH_DMA_START_ADDR &&
		     virtAddr < (Address)(VMMACH_DMA_START_ADDR+VMMACH_DMA_SIZE))) {
		    VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);
		    pageCluster = VMMACH_INV_PMEG;
		}
	    }
	}
	*segTablePtr = pageCluster;
    }

#if defined (sun3)
    /*
     * We can't use the hardware segment that corresponds to the
     * last segment of physical memory for some reason.  Zero it out
     * and can't reboot w/o powering the machine off.
     */
    dontUse = (*romVectorPtr->memoryAvail - 1) / VMMACH_SEG_SIZE;
#endif

    /*
     * Now finally, all page clusters that have a NIL segment pointer are
     * put onto the page cluster fifo.  On a Sun-3 one hardware segment is 
     * off limits for some reason.  Zero it out and can't reboot w/o 
     * powering the machine off.
     */
    for (i = 0, pmegPtr = pmegArray; i < VMMACH_NUM_PMEGS; i++, pmegPtr++) {

	if (pmegPtr->segInfo.segPtr == (Vm_Segment *) NIL 
#if defined (sun3)
	    && i != dontUse
#endif
	) {
	    List_Insert((List_Links *) pmegPtr, LIST_ATREAR(pmegFreeList));
	    pmegPtr->flags = 0;
	    VmMachPMEGZero(i);
	}
    }
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SegInit --
 *
 *      Initialize hardware dependent data for a segment.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Machine dependent data struct and is allocated and initialized.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_SegInit(segPtr)
    Vm_Segment	*segPtr;
{
    register	VmMach_SegData	*segDataPtr;
    int				segTableSize;
    int		i;

    if (segPtr->type == VM_CODE) {
	segTableSize =
	    (segPtr->ptSize + segPtr->offset + VMMACH_NUM_PAGES_PER_SEG - 1) / 
						    VMMACH_NUM_PAGES_PER_SEG;
    } else {
	segTableSize = segPtr->ptSize / VMMACH_NUM_PAGES_PER_SEG;
    }
    segDataPtr = (VmMach_SegData *)malloc(sizeof(VmMach_SegData) +
	(segTableSize * sizeof (VMMACH_SEG_NUM)));

    segDataPtr->numSegs = segTableSize;
    segDataPtr->offset = PageToSeg(segPtr->offset);
    segDataPtr->segTablePtr =
	    (VMMACH_SEG_NUM *) ((Address)segDataPtr + sizeof(VmMach_SegData));
    segDataPtr->pmegInfo.inuse = 0;
    for (i = 0; i < segTableSize; i++) {
	segDataPtr->segTablePtr[i] = VMMACH_INV_PMEG;
    }
    segPtr->machPtr = segDataPtr;
    /*
     * Set the minimum and maximum virtual addresses for this segment to
     * be as small and as big as possible respectively because things will
     * be prevented from growing automatically as soon as segments run into
     * each other.
     */
    segPtr->minAddr = (Address)0;
    segPtr->maxAddr = (Address)0xffffffff;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_SegExpand --
 *
 *	Allocate more space for the machine dependent structure.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Memory allocated for a new hardware segment table.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMach_SegExpand(segPtr, firstPage, lastPage)
    register	Vm_Segment	*segPtr;	/* Segment to expand. */
    int				firstPage;	/* First page to add. */
    int				lastPage;	/* Last page to add. */
{
    int				newSegTableSize;
    register	VmMach_SegData	*oldSegDataPtr;
    register	VmMach_SegData	*newSegDataPtr;

    newSegTableSize = (segPtr->ptSize + VMMACH_NUM_PAGES_PER_SEG-1 +
	    (segPtr->offset%VMMACH_NUM_PAGES_PER_SEG)) /
	    VMMACH_NUM_PAGES_PER_SEG;
    oldSegDataPtr = segPtr->machPtr;
    if (newSegTableSize <= oldSegDataPtr->numSegs) {
	return;
    }
    newSegDataPtr = 
	(VmMach_SegData *)malloc(sizeof(VmMach_SegData) + (newSegTableSize *
		sizeof (VMMACH_SEG_NUM)));
    newSegDataPtr->numSegs = newSegTableSize;
    newSegDataPtr->offset = PageToSeg(segPtr->offset);
    newSegDataPtr->segTablePtr = (VMMACH_SEG_NUM *) ((Address)newSegDataPtr +
		    sizeof(VmMach_SegData));
    CopySegData(segPtr, oldSegDataPtr, newSegDataPtr);
    free((Address)oldSegDataPtr);
}


/*
 *----------------------------------------------------------------------
 *
 * CopySegData --
 *
 *	Copy over the old hardware segment data into the new expanded
 *	structure.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The hardware segment table is copied.
 *
 *----------------------------------------------------------------------
 */
ENTRY static void
CopySegData(segPtr, oldSegDataPtr, newSegDataPtr)
    register	Vm_Segment	*segPtr;	/* The segment to add the
						   virtual pages to. */
    register	VmMach_SegData	*oldSegDataPtr;
    register	VmMach_SegData	*newSegDataPtr;
{
    int		i, j;

    MASTER_LOCK(vmMachMutexPtr);

    if (segPtr->type == VM_HEAP) {
	/*
	 * Copy over the hardware segment table into the lower part
	 * and set the rest to invalid.
	 */
	bcopy((Address)oldSegDataPtr->segTablePtr,
		(Address)newSegDataPtr->segTablePtr,
		oldSegDataPtr->numSegs * sizeof (VMMACH_SEG_NUM));
	j = newSegDataPtr->numSegs - oldSegDataPtr->numSegs;

	for (i = 0; i < j; i++) {
	    newSegDataPtr->segTablePtr[oldSegDataPtr->numSegs + i]
		    = VMMACH_INV_PMEG;
	}
    } else {
	/*
	 * Copy the current segment table into the high part of the
	 * new segment table and set the lower part to invalid.
	 */
	bcopy((Address)oldSegDataPtr->segTablePtr,
	    (Address)(newSegDataPtr->segTablePtr + 
	    newSegDataPtr->numSegs - oldSegDataPtr->numSegs),
	    oldSegDataPtr->numSegs * sizeof (VMMACH_SEG_NUM));
	j = newSegDataPtr->numSegs - oldSegDataPtr->numSegs;

	for (i = 0; i < j; i++) {
	    newSegDataPtr->segTablePtr[i] = VMMACH_INV_PMEG;
	}
    }
    segPtr->machPtr = newSegDataPtr;

    MASTER_UNLOCK(vmMachMutexPtr);
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SegDelete --
 *
 *      Free hardware dependent resources for this software segment.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Machine dependent struct freed and the pointer in the segment
 *	is set to NIL.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_SegDelete(segPtr)
    register	Vm_Segment	*segPtr;    /* Pointer to segment to free. */
{
    SegDelete(segPtr);
    free((Address)segPtr->machPtr);
    segPtr->machPtr = (VmMach_SegData *)NIL;
    if (segPtr->type==VM_SHARED && debugVmStubs) {
	printf("Done with seg %d\n", segPtr->segNum);
    }
}


/*
 * ----------------------------------------------------------------------------
 *
 * SegDelete --
 *
 *      Free up any pmegs used by this segment.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      All pmegs used by this segment are freed.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY static void
SegDelete(segPtr)
    Vm_Segment	*segPtr;    /* Pointer to segment to free. */
{
    register	int 		i;
    register	VMMACH_SEG_NUM	*pmegPtr;
    register	VmMach_SegData	*machPtr;

    MASTER_LOCK(vmMachMutexPtr);

    machPtr = segPtr->machPtr;
    for (i = 0, pmegPtr = (VMMACH_SEG_NUM *) machPtr->segTablePtr;
	     i < machPtr->numSegs; i++, pmegPtr++) {
	if (*pmegPtr != VMMACH_INV_PMEG) {
	    /* Flushing is done in PMEGFree */
	    PMEGFree((int) *pmegPtr);
	}
    }

    MASTER_UNLOCK(vmMachMutexPtr);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_GetContext --
 *
 *	Return the context for a process, given its pcb.
 *
 * Results:
 *	Context number for process. -1 if the process doesn't
 *	have a context allocated.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
int
VmMach_GetContext(procPtr)
    Proc_ControlBlock	*procPtr;
{
    VmMach_Context	*contextPtr;
    contextPtr = procPtr->vmPtr->machPtr->contextPtr;
    return ((contextPtr == (VmMach_Context *)NIL) ? -1 : contextPtr->context);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_ProcInit --
 *
 *	Initalize the machine dependent part of the VM proc info.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Machine dependent proc info is initialized.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_ProcInit(vmPtr)
    register	Vm_ProcInfo	*vmPtr;
{
    if (vmPtr->machPtr == (VmMach_ProcData *)NIL) {
	vmPtr->machPtr = (VmMach_ProcData *)malloc(sizeof(VmMach_ProcData));
    }
    vmPtr->machPtr->contextPtr = (VmMach_Context *)NIL;
    vmPtr->machPtr->mapSegPtr = (struct Vm_Segment *)NIL;
    vmPtr->machPtr->sharedData.allocVector = (int *)NIL;
}


/*
 * ----------------------------------------------------------------------------
 *
 * PMEGGet --
 *
 *      Return the next pmeg from the list of available pmegs.  If the 
 *      lock flag is set then the pmeg is removed from the pmeg list.
 *      Otherwise it is moved to the back.
 *
 * Results:
 *      The pmeg number that is allocated.
 *
 * Side effects:
 *      A pmeg is either removed from the pmeg list or moved to the back.
 *
 * ----------------------------------------------------------------------------
 */
INTERNAL static int
PMEGGet(softSegPtr, hardSegNum, flags)
    Vm_Segment 	*softSegPtr;	/* Which software segment this is. */
    int		hardSegNum;	/* Which hardware segment in the software 
				   segment that this is */
    Boolean	flags;		/* Flags that indicate the state of the pmeg. */
{
    register PMEG		*pmegPtr;
    register Vm_Segment		*segPtr;
    register VmMachPTE		*ptePtr;
    register VmMach_Context	*contextPtr;
    register int		i;
    register VmMachPTE		hardPTE;
    VmMachPTE			pteArray[VMMACH_NUM_PAGES_PER_SEG_INT];
    int	     			oldContext;
    int				pmegNum;
    Address			virtAddr;
    Boolean			found = FALSE;
    int				numValidPages;
    struct VmMach_PMEGseg	*curSeg, *nextSeg;

    if (List_IsEmpty(pmegFreeList)) {
	
	LIST_FORALL(pmegInuseList, (List_Links *)pmegPtr) {
	    if (pmegPtr->lockCount == 0) {
		found = TRUE;
		break;
	    }
	}
	if (!found) {
	    panic("Pmeg lists empty\n");
	    return(VMMACH_INV_PMEG);
	}
    } else {
	pmegPtr = (PMEG *)List_First(pmegFreeList);
    }
    pmegNum = pmegPtr - pmegArray;

    oldContext = VmMachGetContextReg();
    if (pmegPtr->segInfo.segPtr != (Vm_Segment *) NIL) {
	/*
	 * Need to steal the pmeg from its current owner.
	 */
	for (curSeg = &pmegPtr->segInfo; curSeg != (struct VmMach_PMEGseg *)NIL;) {
	    vmStat.machDepStat.stealPmeg++;
	    segPtr = curSeg->segPtr;
	    *GetHardSegPtr(segPtr->machPtr, curSeg->hardSegNum) =
		    VMMACH_INV_PMEG;
	    virtAddr = (Address) (curSeg->hardSegNum << VMMACH_SEG_SHIFT);
	    /*
	     * Delete the pmeg from all appropriate contexts.
	     */
	    if (segPtr->type == VM_SYSTEM) {
		/*
		 * For cache accesses of data with the supervisor tag set,
		 * the flush only needs to be done in one context.
		 */
		numValidPages = GetNumValidPages(virtAddr);
		if (numValidPages >
			(VMMACH_CACHE_SIZE / VMMACH_PAGE_SIZE_INT)) {
		    if (vmMachHasVACache) {
			VmMachFlushSegment(virtAddr);
		    }
		} else {
		    /* flush the pages */
		    FlushValidPages(virtAddr);
		}

		for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
		    VmMachSetContextReg(i);
		    VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);
		}
	    } else {
		for (i = 1, contextPtr = &contextArray[1];
		     i < VMMACH_NUM_CONTEXTS; 
		     i++, contextPtr++) {
		    if (contextPtr->flags & CONTEXT_IN_USE) {
			if (contextPtr->map[curSeg->hardSegNum] ==
				pmegNum) {
			    VmMachSetContextReg(i);
			    contextPtr->map[curSeg->hardSegNum] =
				    VMMACH_INV_PMEG;
			    numValidPages = GetNumValidPages(virtAddr);
			    if (numValidPages >
				    (VMMACH_CACHE_SIZE / VMMACH_PAGE_SIZE_INT)) {
				if (vmMachHasVACache) {
				    VmMachFlushSegment(virtAddr);
				}
			    } else {
				/* flush the pages */
				FlushValidPages(virtAddr);
			    }
			    VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);
			}
			if (contextPtr->map[MAP_SEG_NUM] == pmegNum) {
			    VmMachSetContextReg(i);
			    contextPtr->map[MAP_SEG_NUM] = VMMACH_INV_PMEG;
			    if (vmMachHasVACache) {
				VmMachFlushSegment((Address)VMMACH_MAP_SEG_ADDR);
			    }
			    VmMachSetSegMap((Address)VMMACH_MAP_SEG_ADDR,
					    VMMACH_INV_PMEG);
			}
		    }
		}
	    }
	    nextSeg = curSeg->nextLink;
	    if (curSeg != &pmegPtr->segInfo) {
		curSeg->inuse = 0;
	    }
	    curSeg = nextSeg;
	}
	pmegPtr->segInfo.nextLink = (struct VmMach_PMEGseg *)NIL;
	VmMachSetContextReg(oldContext);
	/*
	 * Read out all reference and modify bits from the pmeg.
	 */
	if (pmegPtr->pageCount > 0) {
	    ptePtr = pteArray;
	    VmMachReadAndZeroPMEG(pmegNum, ptePtr);
	    for (i = 0;
		 i < VMMACH_NUM_PAGES_PER_SEG_INT;
		 i++, ptePtr++) {
		hardPTE = *ptePtr;
		if ((hardPTE & VMMACH_RESIDENT_BIT) &&
		    (hardPTE & (VMMACH_REFERENCED_BIT | VMMACH_MODIFIED_BIT))) {
		    refModMap[PhysToVirtPage(hardPTE & VMMACH_PAGE_FRAME_FIELD)]
		     |= hardPTE & (VMMACH_REFERENCED_BIT | VMMACH_MODIFIED_BIT);
		}
	    }
	}
    }

    /* Initialize the pmeg and delete it from the fifo.  If we aren't 
     * supposed to lock this pmeg, then put it at the rear of the list.
     */
    pmegPtr->segInfo.segPtr = softSegPtr;
    pmegPtr->segInfo.hardSegNum = hardSegNum;
    pmegPtr->pageCount = 0;
    List_Remove((List_Links *) pmegPtr);
    if (!(flags & PMEG_DONT_ALLOC)) {
	List_Insert((List_Links *) pmegPtr, LIST_ATREAR(pmegInuseList));
    }
    pmegPtr->flags = flags;

    return(pmegNum);
}


/*
 *----------------------------------------------------------------------
 *
 * GetNumValidPages --
 *
 *	Return the number of valid pages in a segment.
 *
 * Results:
 *	The number of valid pages.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
static int
GetNumValidPages(virtAddr)
    Address	virtAddr;
{
    int		i;
    int		numValid = 0;
    unsigned	int	pte;

    for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG_INT; i++) {
	pte = VmMachGetPageMap(virtAddr + (i * VMMACH_PAGE_SIZE_INT));
	if (pte & VMMACH_RESIDENT_BIT) {
	    numValid++;
	}
    }

    return numValid;
}


/*
 *----------------------------------------------------------------------
 *
 * FlushValidPages --
 *
 *	Flush the valid pages in a segment.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The valid pages in a segment are flushed.
 *
 *----------------------------------------------------------------------
 */
static void
FlushValidPages(virtAddr)
    Address	virtAddr;
{
    int		i;
    unsigned	int	pte;

    if (vmMachHasVACache) {
	for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG_INT; i++) {
	    pte = VmMachGetPageMap(virtAddr + (i * VMMACH_PAGE_SIZE_INT));
	    if (pte & VMMACH_RESIDENT_BIT) {
		VmMachFlushPage(virtAddr + (i * VMMACH_PAGE_SIZE_INT));
	    }
	}
    }
}


/*
 * ----------------------------------------------------------------------------
 *
 * PMEGFree --
 *
 *      Return the given pmeg to the pmeg list.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The pmeg is returned to the pmeg list.
 *
 * ----------------------------------------------------------------------------
 */
INTERNAL static void
PMEGFree(pmegNum)
    int 	pmegNum;	/* Which pmeg to free */
{
    register	PMEG	*pmegPtr;
    struct VmMach_PMEGseg	*segPtr, *nextPtr;

    pmegPtr = &pmegArray[pmegNum];
    /*
     * If this pmeg can never be freed then don't free it.  This case can
     * occur when a device is mapped into a user's address space.
     */
    if (pmegPtr->flags & PMEG_NEVER_FREE) {
	return;
    }

    if (pmegPtr->pageCount > 0) {

	/*
	 * Deal with pages that are still cached in this pmeg.
	 */
	VmMachPMEGZero(pmegNum);
    }
    if (pmegPtr->segInfo.segPtr != (Vm_Segment *)NIL) {
	for (segPtr = &pmegPtr->segInfo; segPtr != (struct VmMach_PMEGseg *)NIL;) {
	    if (segPtr->segPtr->machPtr == (VmMach_SegData *)NIL) {
		printf("PMEGFree(%d): seg %d has no machPtr!\n", pmegNum,
			segPtr->segPtr->segNum);
	    } else {
		*GetHardSegPtr(segPtr->segPtr->machPtr, segPtr->hardSegNum) =
			VMMACH_INV_PMEG;
	    }
	    nextPtr = segPtr->nextLink;
	    if (segPtr != &pmegPtr->segInfo) {
		segPtr->inuse = 0;
	    }
	    segPtr = nextPtr;
	}
    }
    pmegPtr->segInfo.nextLink = (struct VmMach_PMEGseg *) NIL;
    pmegPtr->segInfo.segPtr = (Vm_Segment *) NIL;

    /*
     * I really don't understand the code here.  The original was the second
     * line.  The first line was to try to fix an error that shows up in
     * UnmapIntelPage(), but I've tried now to fix that error there, since the
     * second line breaks things elsewhere.
     */
    if (pmegPtr->pageCount == 0 || !(pmegPtr->flags & PMEG_DONT_ALLOC)) {
	List_Remove((List_Links *) pmegPtr);
    }
    pmegPtr->flags = 0;
    pmegPtr->lockCount = 0;
    /*
     * Put this pmeg at the front of the pmeg free list.
     */
    List_Insert((List_Links *) pmegPtr, LIST_ATFRONT(pmegFreeList));
}


/*
 * ----------------------------------------------------------------------------
 *
 * PMEGLock --
 *
 *      Increment the lock count on a pmeg.
 *
 * Results:
 *      TRUE if there was a valid PMEG behind the given hardware segment.
 *
 * Side effects:
 *      The lock count is incremented if there is a valid pmeg at the given
 *	hardware segment.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY static Boolean
PMEGLock(machPtr, segNum)
    register VmMach_SegData	*machPtr;
    int				segNum;
{
    unsigned int pmegNum;

    MASTER_LOCK(vmMachMutexPtr);

    pmegNum = *GetHardSegPtr(machPtr, segNum);
    if (pmegNum != VMMACH_INV_PMEG) {
	pmegArray[pmegNum].lockCount++;
	MASTER_UNLOCK(vmMachMutexPtr);
	return(TRUE);
    } else {
	MASTER_UNLOCK(vmMachMutexPtr);
	return(FALSE);
    }
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SetupContext --
 *
 *      Return the value of the context register for the given process.
 *	It is assumed that this routine is called on a uni-processor right
 *	before the process starts executing.
 *	
 * Results:
 *      None.
 *
 * Side effects:
 *      The context list is modified.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY ClientData
VmMach_SetupContext(procPtr)
    register	Proc_ControlBlock	*procPtr;
{
    register	VmMach_Context	*contextPtr;

    MASTER_LOCK(vmMachMutexPtr);

    while (TRUE) {
	contextPtr = procPtr->vmPtr->machPtr->contextPtr;
	if (contextPtr != (VmMach_Context *)NIL) {
	    if (contextPtr != &contextArray[VMMACH_KERN_CONTEXT]) {
		List_Move((List_Links *)contextPtr, LIST_ATREAR(contextList));
	    }
	    MASTER_UNLOCK(vmMachMutexPtr);
	    return((ClientData)contextPtr->context);
	}
        SetupContext(procPtr);
    }
}


/*
 * ----------------------------------------------------------------------------
 *
 * SetupContext --
 *
 *      Initialize the context for the given process.  If the process does
 *	not have a context associated with it then one is allocated.
 *
 *	Note that this routine runs unsynchronized even though it is using
 *	internal structures.  See the note above while this is OK.  I
 * 	eliminated the monitor lock because it is unnecessary anyway and
 *	it slows down context-switching.
 *	
 * Results:
 *      None.
 *
 * Side effects:
 *      The context field in the process table entry and the context list are
 * 	both modified if a new context is allocated.
 *
 * ----------------------------------------------------------------------------
 */
INTERNAL static void
SetupContext(procPtr)
    register	Proc_ControlBlock	*procPtr;
{
    register	VmMach_Context	*contextPtr;
    register	VmMach_SegData	*segDataPtr;
    register	Vm_ProcInfo	*vmPtr;
    int		stolenContext	= FALSE;

    vmPtr = procPtr->vmPtr;
    contextPtr = vmPtr->machPtr->contextPtr;

    if (procPtr->genFlags & (PROC_KERNEL | PROC_NO_VM)) {
	/*
	 * This is a kernel process or a process that is exiting.
	 * Set the context to kernel and return.
	 */
	VmMachSetContextReg(VMMACH_KERN_CONTEXT);
	vmPtr->machPtr->contextPtr = &contextArray[VMMACH_KERN_CONTEXT];
	return;
    }

    if (contextPtr == (VmMach_Context *)NIL) {
	/*
	 * In this case there is no context setup for this process.  Therefore
	 * we have to find a context, initialize the context table entry and 
	 * initialize the context stuff in the proc table.
	 */
	if (List_IsEmpty((List_Links *) contextList)) {
	    panic("SetupContext: Context list empty\n");
	}
	/* 
	 * Take the first context off of the context list.
	 */
	contextPtr = (VmMach_Context *) List_First(contextList);
	if (contextPtr->flags & CONTEXT_IN_USE) {
	    contextPtr->procPtr->vmPtr->machPtr->contextPtr =
							(VmMach_Context *)NIL;
	    vmStat.machDepStat.stealContext++;
	    stolenContext = TRUE;
	}
	/*
	 * Initialize the context table entry.
	 */
	contextPtr->flags = CONTEXT_IN_USE;
	contextPtr->procPtr = procPtr;
	vmPtr->machPtr->contextPtr = contextPtr;
	VmMachSetContextReg((int)contextPtr->context);
	if (stolenContext && vmMachHasVACache) {
	    VmMach_FlushCurrentContext();
	}
	/*
	 * Set the context map.
	 */
	{
	    int			i;
	    unsigned int	j;

	    /*
	     * Since user addresses are never higher than the bottom of the
	     * hole in the address space, this will save something like 30ms
	     * by ending at VMMACH_BOTTOM_OF_HOLE rather than mach_KernStart.
	     */
	    j = ((unsigned int)VMMACH_BOTTOM_OF_HOLE) >> VMMACH_SEG_SHIFT;
	    for (i = 0; i < j; i++) {
		contextPtr->map[i] = VMMACH_INV_PMEG;
	    }
	}
	segDataPtr = vmPtr->segPtrArray[VM_CODE]->machPtr;
	bcopy((Address)segDataPtr->segTablePtr, 
	    (Address) (contextPtr->map + segDataPtr->offset),
	    segDataPtr->numSegs * sizeof (VMMACH_SEG_NUM));

	segDataPtr = vmPtr->segPtrArray[VM_HEAP]->machPtr;
	bcopy((Address)segDataPtr->segTablePtr, 
		(Address) (contextPtr->map + segDataPtr->offset),
		segDataPtr->numSegs * sizeof (VMMACH_SEG_NUM));

	segDataPtr = vmPtr->segPtrArray[VM_STACK]->machPtr;
	bcopy((Address)segDataPtr->segTablePtr, 
		(Address) (contextPtr->map + segDataPtr->offset),
		segDataPtr->numSegs * sizeof (VMMACH_SEG_NUM));
	if (vmPtr->sharedSegs != (List_Links *)NIL) {
	    Vm_SegProcList *segList;
	    LIST_FORALL(vmPtr->sharedSegs,(List_Links *)segList) {
		segDataPtr = segList->segTabPtr->segPtr->machPtr;
		bcopy((Address)segDataPtr->segTablePtr, 
			(Address) (contextPtr->map+PageToSeg(segList->offset)),
			segDataPtr->numSegs);
	    }
	}
	if (vmPtr->machPtr->mapSegPtr != (struct Vm_Segment *)NIL) {
	    contextPtr->map[MAP_SEG_NUM] = vmPtr->machPtr->mapHardSeg;
	} else {
	    contextPtr->map[MAP_SEG_NUM] = VMMACH_INV_PMEG;
	}
	/*
	 * Push map out to hardware.
	 */
	VmMachCopyUserSegMap(contextPtr->map);
    } else {
	VmMachSetContextReg((int)contextPtr->context);
    }
    List_Move((List_Links *)contextPtr, LIST_ATREAR(contextList));
}


/*
 * ----------------------------------------------------------------------------
 *
 * Vm_FreeContext --
 *
 *      Free the given context.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The context table and context lists are modified.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_FreeContext(procPtr)
    register	Proc_ControlBlock	*procPtr;
{
    register	VmMach_Context	*contextPtr;
    register	VmMach_ProcData	*machPtr;

    MASTER_LOCK(vmMachMutexPtr);

    machPtr = procPtr->vmPtr->machPtr;
    contextPtr = machPtr->contextPtr;
    if (contextPtr == (VmMach_Context *)NIL ||
        contextPtr->context == VMMACH_KERN_CONTEXT) {
	MASTER_UNLOCK(vmMachMutexPtr);
	return;
    }

    List_Move((List_Links *)contextPtr, LIST_ATFRONT(contextList));
    contextPtr->flags = 0;
    machPtr->contextPtr = (VmMach_Context *)NIL;

    MASTER_UNLOCK(vmMachMutexPtr);
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_ReinitContext --
 *
 *      Free the current context and set up another one.  This is called
 *	by routines such as Proc_Exec that add things to the context and
 *	then have to abort or start a process running with a new image.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The context table and context lists are modified.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_ReinitContext(procPtr)
    register	Proc_ControlBlock	*procPtr;
{
    VmMach_FreeContext(procPtr);
    MASTER_LOCK(vmMachMutexPtr);
    procPtr->vmPtr->machPtr->contextPtr = (VmMach_Context *)NIL;
    SetupContext(procPtr);
    MASTER_UNLOCK(vmMachMutexPtr);
}

#if defined(sun2)

static int	 allocatedPMEG;
static VmMachPTE intelSavedPTE;		/* The page table entry that is stored
					 * at the address that the intel page
					 * has to overwrite. */
static unsigned int intelPage;		/* The page frame that was allocated.*/
#endif


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_MapIntelPage --
 *
 *      Allocate and validate a page for the Intel Ethernet chip.  This routine
 *	is required in order to initialize the chip.  The chip expects 
 *	certain stuff to be at a specific virtual address when it is 
 *	initialized.  This routine sets things up so that the expected
 *	virtual address is accessible.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The old pte stored at the virtual address and the page frame that is
 *	allocated are stored in static globals.
 *
 * ----------------------------------------------------------------------------
 */
#ifndef sun4c
/*ARGSUSED*/
void
VmMach_MapIntelPage(virtAddr) 
    Address	virtAddr; /* Virtual address where a page has to be validated
			     at. */
{
#if defined(sun2)
    VmMachPTE		pte;
    int			pmeg;

    /*
     * See if there is a PMEG already.  If not allocate one.
     */
    pmeg = VmMachGetSegMap(virtAddr);
    if (pmeg == VMMACH_INV_PMEG) {
	MASTER_LOCK(vmMachMutexPtr);
	/* No flush, since PMEGGet takes care of that. */
	allocatedPMEG = PMEGGet(vm_SysSegPtr, 
				(unsigned)virtAddr >> VMMACH_SEG_SHIFT,
				PMEG_DONT_ALLOC);
	MASTER_UNLOCK(vmMachMutexPtr);
	VmMachSetSegMap(virtAddr, allocatedPMEG);
    } else {
	allocatedPMEG = VMMACH_INV_PMEG;
	intelSavedPTE = VmMachGetPageMap(virtAddr);
    }

    /*
     * Set up the page table entry.
     */
    intelPage = Vm_KernPageAllocate();
    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | VirtToPhysPage(intelPage);
    /* No flush since this should never be cached. */
    SET_ALL_PAGE_MAP(virtAddr, pte);
#endif /* sun2  */
#ifdef sun4
    VmMachPTE		pte;
    int			pmeg;
    int			oldContext;
    int			i;

    /*
     * See if there is a PMEG already.  If not allocate one.
     */
    pmeg = VmMachGetSegMap(virtAddr);
    if (pmeg == VMMACH_INV_PMEG) {
	MASTER_LOCK(vmMachMutexPtr);
	/* No flush, since PMEGGet takes care of that. */
	pmeg = PMEGGet(vm_SysSegPtr, 
				(int)((unsigned)virtAddr) >> VMMACH_SEG_SHIFT,
				PMEG_DONT_ALLOC);
	MASTER_UNLOCK(vmMachMutexPtr);
	VmMachSetSegMap(virtAddr, pmeg);
    } 
    oldContext = VmMachGetContextReg();
    for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	VmMachSetContextReg(i);
	VmMachSetSegMap(virtAddr, pmeg);
    }
    VmMachSetContextReg(oldContext);
    /*
     * Set up the page table entry.
     */
    pte = VmMachGetPageMap(virtAddr);
    if (pte == 0) {
	pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | VMMACH_DONT_CACHE_BIT |
	      VirtToPhysPage(Vm_KernPageAllocate());
	SET_ALL_PAGE_MAP(virtAddr, pte);
    } 
#endif
}
#endif


/*
 * ----------------------------------------------------------------------------
 *
 * Vm_UnmapIntelPage --
 *
 *      Deallocate and invalidate a page for the intel chip.  This is a special
 *	case routine that is only for the intel ethernet chip.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The hardware segment table associated with the segment
 *      is modified to invalidate the page.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
#ifndef sun4c
void
VmMach_UnmapIntelPage(virtAddr) 
    Address	virtAddr;
{
#if defined(sun2)
    PMEG		*pmegPtr;
    Boolean	found = FALSE;

    if (allocatedPMEG != VMMACH_INV_PMEG) {
	/*
	 * Free up the PMEG.
	 */
	/* No flush since this should never be cached. */
	VmMachSetPageMap(virtAddr, (VmMachPTE)0);
	VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);

	MASTER_LOCK(vmMachMutexPtr);

	/*
	 * This is a little gross, but for some reason this pmeg has a 0
	 * page count.  Since it has the PMEG_DONT_ALLOC flag set, it wasn't
	 * removed from the pmegInuseList in PMEGGet().  So the remove done
	 * in PMEGFree would remove it twice, unless we check.
	 */
	LIST_FORALL(pmegInuseList, (List_Links *)pmegPtr) {
	    if (pmegPtr == &pmegArray[allocatedPMEG]) {
		found = TRUE;
	    }
	}
	if (!found) {
	    pmegPtr = &pmegArray[allocatedPMEG];
	    List_Insert((List_Links *) pmegPtr, LIST_ATREAR(pmegInuseList));
	}
	PMEGFree(allocatedPMEG);

	MASTER_UNLOCK(vmMachMutexPtr);
    } else {
	/*
	 * Restore the saved pte and free the allocated page.
	 */
	/* No flush since this should never be cached. */
	VmMachSetPageMap(virtAddr, intelSavedPTE);
    }
    Vm_KernPageFree(intelPage);
#endif
}
#endif /* sun4c */


static Address		netMemAddr;
static unsigned int	netLastPage;


/*
 * ----------------------------------------------------------------------------
 *
 * InitNetMem --
 *
 *      Initialize the memory mappings for the network.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      PMEGS are allocated and initialized.
 *
 * ----------------------------------------------------------------------------
 */
static void
InitNetMem()
{
    VMMACH_SEG_NUM		pmeg;
    register VMMACH_SEG_NUM	*segTablePtr;
    int				i;
    int				j;
    int				lastSegNum;
    int				segNum;
    Address			virtAddr;

    /*
     * Allocate pmegs  for net mapping.
     */
    segNum = ((unsigned)VMMACH_NET_MAP_START) >> VMMACH_SEG_SHIFT;
    lastSegNum = ((unsigned)(VMMACH_NET_MAP_START+VMMACH_NET_MAP_SIZE-1)) /
			  VMMACH_SEG_SIZE;

    for (i = 0, virtAddr = (Address)VMMACH_NET_MAP_START,
	    segTablePtr = GetHardSegPtr(vm_SysSegPtr->machPtr, segNum);
	 segNum <= lastSegNum;
         i++, virtAddr += VMMACH_SEG_SIZE, segNum++) {
	pmeg = VmMachGetSegMap(virtAddr);
	if (pmeg == VMMACH_INV_PMEG) {
	    *(segTablePtr + i) = PMEGGet(vm_SysSegPtr, segNum, PMEG_DONT_ALLOC);
	    VmMachSetSegMap(virtAddr, (int)*(segTablePtr + i));
	} else {
	    *(segTablePtr + i) = pmeg;
	}
	/*
	 * Propagate the new pmeg mapping to all contexts.
	 */
	for (j = 0; j < VMMACH_NUM_CONTEXTS; j++) {
	    if (j == VMMACH_KERN_CONTEXT) {
		continue;
	    }
	    VmMachSetContextReg(j);
	    VmMachSetSegMap(virtAddr, (int)*(segTablePtr + i));
	}
	VmMachSetContextReg(VMMACH_KERN_CONTEXT);
    }
    /*
     * Repeat for the network memory range. 
     */
    segNum = ((unsigned)VMMACH_NET_MEM_START) >> VMMACH_SEG_SHIFT;
    lastSegNum = ((unsigned)(VMMACH_NET_MEM_START+VMMACH_NET_MEM_SIZE-1)) /
			 VMMACH_SEG_SIZE;

    for (i = 0, virtAddr = (Address)VMMACH_NET_MEM_START,
	    segTablePtr = GetHardSegPtr(vm_SysSegPtr->machPtr, segNum);
	 segNum <= lastSegNum;
         i++, virtAddr += VMMACH_SEG_SIZE, segNum++) {
	pmeg = VmMachGetSegMap(virtAddr);
	if (pmeg == VMMACH_INV_PMEG) {
	    *(segTablePtr + i) = PMEGGet(vm_SysSegPtr, segNum, PMEG_DONT_ALLOC);
	    VmMachSetSegMap(virtAddr, (int)*(segTablePtr + i));
	} else {
	    *(segTablePtr + i) = pmeg;
	}
	/*
	 * Propagate the new pmeg mapping to all contexts.
	 */
	for (j = 0; j < VMMACH_NUM_CONTEXTS; j++) {
	    if (j == VMMACH_KERN_CONTEXT) {
		continue;
	    }
	    VmMachSetContextReg(j);
	    VmMachSetSegMap(virtAddr, (int)*(segTablePtr + i));
	}
	VmMachSetContextReg(VMMACH_KERN_CONTEXT);
    }

    netMemAddr = (Address)VMMACH_NET_MEM_START;
    netLastPage = (((unsigned)VMMACH_NET_MEM_START) >> VMMACH_PAGE_SHIFT) - 1;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_NetMemAlloc --
 *
 *      Allocate physical memory for a network driver.
 *
 * Results:
 *      The address where the memory is allocated at.
 *
 * Side effects:
 *      Memory allocated.
 *
 * ----------------------------------------------------------------------------
 */
Address
VmMach_NetMemAlloc(numBytes)
    int	numBytes;	/* Number of bytes of memory to allocated. */
{
    VmMachPTE	pte;
    Address	retAddr;
    Address	maxAddr;
    Address	virtAddr;
    static Boolean initialized = FALSE;

    if (!initialized) {
	InitNetMem();
	initialized = TRUE;
    }

    retAddr = netMemAddr;
    netMemAddr += (numBytes + 7) & ~7;	/* is this necessary for sun4? */
    /*
     * Panic if we are out of memory.  
     */
    if (netMemAddr > (Address) (VMMACH_NET_MEM_START + VMMACH_NET_MEM_SIZE)) {
	panic("VmMach_NetMemAlloc: Out of network memory\n");
    }

    maxAddr = (Address) ((netLastPage + 1) * VMMACH_PAGE_SIZE - 1);

    /*
     * Add new pages to the virtual address space until we have added enough
     * to handle this memory request.
     */
    while (netMemAddr - 1 > maxAddr) {
	maxAddr += VMMACH_PAGE_SIZE;
	netLastPage++;
	virtAddr = (Address) (netLastPage << VMMACH_PAGE_SHIFT);
	pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT |
#ifdef sun4c
	      /*
	       * For some reason on the sparcStation, we can't allow the
	       * network pages to be cached. This is really a problem and it
	       * totally breaks the driver.
	       */
	      VMMACH_DONT_CACHE_BIT | 
#endif
	      VirtToPhysPage(Vm_KernPageAllocate());
	SET_ALL_PAGE_MAP(virtAddr, pte);
    }

    bzero(retAddr, numBytes);
    return(retAddr);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_NetMapPacket --
 *
 *	Map the packet pointed to by the scatter-gather array.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The outScatGathArray is filled in with pointers to where the
 *	packet was mapped in.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_NetMapPacket(inScatGathPtr, scatGathLength, outScatGathPtr)
    register Net_ScatterGather	*inScatGathPtr;
    register int		scatGathLength;
    register Net_ScatterGather	*outScatGathPtr;
{
    register Address	mapAddr;
    register Address	endAddr;
#ifndef sun4c
    int			segNum;
    int			pageNum = 0;
#endif

#ifdef sun4c
    /*
     * The network driver on the sparcstation never accesses the data
     * through the cache, so there should be no need to flush it.
     */
    for (mapAddr = (Address)VMMACH_NET_MAP_START;
	    scatGathLength > 0;
	    scatGathLength--, inScatGathPtr++, outScatGathPtr++) {
        outScatGathPtr->length = inScatGathPtr->length;
        if (inScatGathPtr->length == 0) {
            continue;
        }
        /*
         * Map the piece of the packet in.  Note that we know that a packet
         * piece is no longer than 1536 bytes so we know that we will need
         * at most two page table entries to map a piece in.
         */
        VmMachSetPageMap(mapAddr, VmMachGetPageMap(inScatGathPtr->bufAddr));
        outScatGathPtr->bufAddr = mapAddr +
	    ((unsigned)inScatGathPtr->bufAddr & VMMACH_OFFSET_MASK_INT);
        mapAddr += VMMACH_PAGE_SIZE_INT;
        endAddr = inScatGathPtr->bufAddr + inScatGathPtr->length - 1;
        if (((unsigned)inScatGathPtr->bufAddr & ~VMMACH_OFFSET_MASK_INT) !=
            ((unsigned)endAddr & ~VMMACH_OFFSET_MASK_INT)) {
            VmMachSetPageMap(mapAddr, VmMachGetPageMap(endAddr));
            mapAddr += VMMACH_PAGE_SIZE_INT;
        }
    }
#else
    for (segNum = 0 ; scatGathLength > 0;
	    scatGathLength--, inScatGathPtr++, outScatGathPtr++) {
	outScatGathPtr->length = inScatGathPtr->length;
	if (inScatGathPtr->length == 0) {
	    continue;
	}
	/*
	 * For the first (VMMACH_NUM_NET_SEGS) - 1 elements in the
	 * scatter gather array, map each element into a segment, aligned
	 * with the mapping pages so that cache flushes will be avoided.
	 */
	if (segNum < VMMACH_NUM_NET_SEGS - 1) {
	    /* do silly mapping */
	    mapAddr = (Address)VMMACH_NET_MAP_START +
		    (segNum * VMMACH_SEG_SIZE);
	    /* align to same cache boundary */
	    mapAddr += ((unsigned int)inScatGathPtr->bufAddr &
		    (VMMACH_CACHE_SIZE - 1));
	    /* set addr to beginning of page */
	    mapAddr = (Address)((unsigned int) mapAddr & ~VMMACH_OFFSET_MASK);
	    if (vmMachHasVACache) {
		VmMachFlushPage(mapAddr);
	    }
	    VmMachSetPageMap(mapAddr, VmMachGetPageMap(inScatGathPtr->bufAddr));
	    outScatGathPtr->bufAddr = (Address) ((unsigned)mapAddr +
		    ((unsigned)inScatGathPtr->bufAddr & VMMACH_OFFSET_MASK));
	    mapAddr += VMMACH_PAGE_SIZE_INT;
	    endAddr = (Address)inScatGathPtr->bufAddr +
		    inScatGathPtr->length - 1;
	    if (((unsigned)inScatGathPtr->bufAddr & ~VMMACH_OFFSET_MASK_INT) !=
		((unsigned)endAddr & ~VMMACH_OFFSET_MASK_INT)) {
		if (vmMachHasVACache) {
		    VmMachFlushPage(mapAddr);
		}
		VmMachSetPageMap(mapAddr, VmMachGetPageMap(endAddr));
	    }
	    segNum++;
	} else {
	    /*
	     * For elements beyond the last one, map them all into the
	     * last mapping segment.  Cache flushing will be necessary for
	     * these.
	     */
	    mapAddr = (Address)VMMACH_NET_MAP_START +
		    (segNum * VMMACH_SEG_SIZE) + (pageNum * VMMACH_PAGE_SIZE);
	    if (vmMachHasVACache) {
		VmMachFlushPage(inScatGathPtr->bufAddr);
		VmMachFlushPage(mapAddr);
	    }
	    VmMachSetPageMap(mapAddr, VmMachGetPageMap(inScatGathPtr->bufAddr));
	    outScatGathPtr->bufAddr = (Address) ((unsigned)mapAddr +
		    ((unsigned)inScatGathPtr->bufAddr & VMMACH_OFFSET_MASK));
	    mapAddr += VMMACH_PAGE_SIZE_INT;
	    pageNum++;
	    endAddr = (Address)inScatGathPtr->bufAddr +
		    inScatGathPtr->length - 1;
	    if (((unsigned)inScatGathPtr->bufAddr & ~VMMACH_OFFSET_MASK_INT) !=
		((unsigned)endAddr & ~VMMACH_OFFSET_MASK_INT)) {
		if (vmMachHasVACache) {
		    VmMachFlushPage(endAddr);
		    VmMachFlushPage(mapAddr);
		}
		VmMachSetPageMap(mapAddr, VmMachGetPageMap(endAddr));
		pageNum++;
	    }
	    printf("MapPacket: segNum is %d, pageNum is %d\n", segNum, pageNum);
	}
    }
#endif /* sun4c */
}



/*
 *----------------------------------------------------------------------
 *
 * VmMach_VirtAddrParse --
 *
 *	See if the given address falls into the special mapping segment.
 *	If so parse it for our caller.
 *
 * Results:
 *	TRUE if the address fell into the special mapping segment, FALSE
 *	otherwise.
 *
 * Side effects:
 *	*transVirtAddrPtr may be filled in.
 *
 *----------------------------------------------------------------------
 */
Boolean
VmMach_VirtAddrParse(procPtr, virtAddr, transVirtAddrPtr)
    Proc_ControlBlock		*procPtr;
    Address			virtAddr;
    register	Vm_VirtAddr	*transVirtAddrPtr;
{
    Address	origVirtAddr;
    Boolean	retVal;

#ifdef sun4
    if (!VMMACH_ADDR_CHECK(virtAddr)) {
	panic("VmMach_VirtAddrParse: virt addr 0x%x falls in illegal range!\n",
		virtAddr);
    }
#endif sun4
    if (virtAddr >= (Address)VMMACH_MAP_SEG_ADDR && 
        virtAddr < (Address)mach_KernStart) {
	/*
	 * The address falls into the special mapping segment.  Translate
	 * the address back to the segment that it falls into.
	 */
	transVirtAddrPtr->segPtr = procPtr->vmPtr->machPtr->mapSegPtr;
	origVirtAddr = 
	    (Address)(procPtr->vmPtr->machPtr->mapHardSeg << VMMACH_SEG_SHIFT);
	transVirtAddrPtr->sharedPtr = procPtr->vmPtr->machPtr->sharedPtr;
 	if (transVirtAddrPtr->segPtr->type == VM_SHARED) {
 	    origVirtAddr -= ( transVirtAddrPtr->segPtr->offset
 		    >>(VMMACH_SEG_SHIFT-VMMACH_PAGE_SHIFT))
 		    << VMMACH_SEG_SHIFT;
	    origVirtAddr += segOffset(transVirtAddrPtr)<<VMMACH_PAGE_SHIFT;
 	}
	origVirtAddr += (unsigned int)virtAddr & (VMMACH_SEG_SIZE - 1);
	transVirtAddrPtr->page = (unsigned) (origVirtAddr) >> VMMACH_PAGE_SHIFT;
	transVirtAddrPtr->offset = (unsigned)virtAddr & VMMACH_OFFSET_MASK;
	transVirtAddrPtr->flags = USING_MAPPED_SEG;
	retVal = TRUE;
    } else {
	retVal = FALSE;
    }
    return(retVal);
}



/*
 *----------------------------------------------------------------------
 *
 * VmMach_CopyInProc --
 *
 *	Copy from another process's address space into the current address
 *	space.   This is done by mapping the other processes segment into
 *	the current VAS and then doing the copy.  It assumed that this 
 *	routine is called with the source process locked such that its
 *	VM will not go away while we are doing this copy.
 *
 * Results:
 *	SUCCESS if the copy succeeded, SYS_ARG_NOACCESS if fromAddr is invalid.
 *
 * Side effects:
 *	What toAddr points to is modified.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
ReturnStatus
VmMach_CopyInProc(numBytes, fromProcPtr, fromAddr, virtAddrPtr,
	      toAddr, toKernel)
    int 	numBytes;		/* The maximum number of bytes to 
					   copy in. */
    Proc_ControlBlock	*fromProcPtr;	/* Which process to copy from.*/
    Address		fromAddr;	/* The address to copy from */
    Vm_VirtAddr		*virtAddrPtr;
    Address		toAddr;		/* The address to copy to */
    Boolean		toKernel;	/* This copy is happening to the
					 * kernel's address space. */
{
    ReturnStatus		status = SUCCESS;
    register VmMach_ProcData	*machPtr;
    Proc_ControlBlock		*toProcPtr;
    int				segOffset;
    int				bytesToCopy;
    int				oldContext;

    toProcPtr = Proc_GetCurrentProc();
    machPtr = toProcPtr->vmPtr->machPtr;
    machPtr->mapSegPtr = virtAddrPtr->segPtr;
    machPtr->mapHardSeg = (unsigned int) (fromAddr) >> VMMACH_SEG_SHIFT;
    machPtr->sharedPtr = virtAddrPtr->sharedPtr;
    if (virtAddrPtr->sharedPtr != (Vm_SegProcList*)NIL) {
	/*
	 * Mangle the segment offset so that it matches the offset
	 * of the mapped segment.
	 */
	if (debugVmStubs) {
	    printf("Copying in shared segment\n");
	}
	machPtr->mapHardSeg -= (virtAddrPtr->sharedPtr->offset<<
		VMMACH_PAGE_SHIFT_INT)>>VMMACH_SEG_SHIFT;
	machPtr->mapHardSeg += machPtr->mapSegPtr->machPtr->offset;
    }

#ifdef sun4
    /*
     * Since this is a cross-address-space copy, we must make sure everything
     * has been flushed to the stack from our windows so that we don't
     * miss stuff on the stack not yet flushed. 
     */
    Mach_FlushWindowsToStack();
#endif

    /*
     * Do a hardware segment's worth at a time until done.
     */
    while (numBytes > 0 && status == SUCCESS) {
	segOffset = (unsigned int)fromAddr & (VMMACH_SEG_SIZE - 1);
	bytesToCopy = VMMACH_SEG_SIZE - segOffset;
	if (bytesToCopy > numBytes) {
	    bytesToCopy = numBytes;
	}
	/*
	 * First try quick and dirty copy.  If it fails, do regular copy.
	 */
	if (fromProcPtr->vmPtr->machPtr->contextPtr != (VmMach_Context *)NIL) {
	    unsigned int	fromContext;
	    unsigned int	toContext;

	    toContext = VmMachGetContextReg();
	    fromContext = fromProcPtr->vmPtr->machPtr->contextPtr->context;

	    status = VmMachQuickNDirtyCopy(bytesToCopy, fromAddr, toAddr,
		fromContext, toContext);
	    VmMachSetContextReg((int)toContext);

	    if (status == SUCCESS) {
		numBytes -= bytesToCopy;
		fromAddr += bytesToCopy;
		toAddr += bytesToCopy;
		continue;
	    }
	}
	/*
	 * Flush segment in context of fromProcPtr.  If the context is NIL, then
	 * we can't and don't have to flush it, since it will be flushed
	 * before being reused.
	 */
	if (vmMachHasVACache &&
	    fromProcPtr->vmPtr->machPtr->contextPtr != (VmMach_Context *)NIL) {

	    oldContext = VmMachGetContextReg();
	    VmMachSetContextReg(
		    (int)fromProcPtr->vmPtr->machPtr->contextPtr->context);
	    VmMachFlushSegment(fromAddr);
	    VmMachSetContextReg(oldContext);
	}
	/*
	 * Push out the hardware segment.
	 */
	WriteHardMapSeg(machPtr);
	/*
	 * Do the copy.
	 */
	toProcPtr->vmPtr->vmFlags |= VM_COPY_IN_PROGRESS;
	status = VmMachDoCopy(bytesToCopy,
			      (Address)(VMMACH_MAP_SEG_ADDR + segOffset),
			      toAddr);
	toProcPtr->vmPtr->vmFlags &= ~VM_COPY_IN_PROGRESS;
	if (status == SUCCESS) {
	    numBytes -= bytesToCopy;
	    fromAddr += bytesToCopy;
	    toAddr += bytesToCopy;
	} else {
	    status = SYS_ARG_NOACCESS;
	}
	/*
	 * Zap the hardware segment.
	 */
	if (vmMachHasVACache) {
	    VmMachFlushSegment((Address) VMMACH_MAP_SEG_ADDR);
	}
	VmMachSetSegMap((Address)VMMACH_MAP_SEG_ADDR, VMMACH_INV_PMEG); 
	machPtr->mapHardSeg++;
    }
    machPtr->mapSegPtr = (struct Vm_Segment *)NIL;
    return(status);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_CopyOutProc --
 *
 *	Copy from the current VAS to another processes VAS.  This is done by 
 *	mapping the other processes segment into the current VAS and then 
 *	doing the copy.  It assumed that this routine is called with the dest
 *	process locked such that its VM will not go away while we are doing
 *	the copy.
 *
 * Results:
 *	SUCCESS if the copy succeeded, SYS_ARG_NOACCESS if fromAddr is invalid.
 *
 * Side effects:
 *	What toAddr points to is modified.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
ReturnStatus
VmMach_CopyOutProc(numBytes, fromAddr, fromKernel, toProcPtr, toAddr,
		   virtAddrPtr)
    int 		numBytes;	/* The maximum number of bytes to 
					   copy in. */
    Address		fromAddr;	/* The address to copy from */
    Boolean		fromKernel;	/* This copy is happening to the
					 * kernel's address space. */
    Proc_ControlBlock	*toProcPtr;	/* Which process to copy from.*/
    Address		toAddr;		/* The address to copy to */
    Vm_VirtAddr		*virtAddrPtr;
{
    ReturnStatus		status = SUCCESS;
    register VmMach_ProcData	*machPtr;
    Proc_ControlBlock		*fromProcPtr;
    int				segOffset;
    int				bytesToCopy;
    int				oldContext;


    fromProcPtr = Proc_GetCurrentProc();
    machPtr = fromProcPtr->vmPtr->machPtr;
    machPtr->mapSegPtr = virtAddrPtr->segPtr;
    machPtr->mapHardSeg = (unsigned int) (toAddr) >> VMMACH_SEG_SHIFT;
    machPtr->sharedPtr = virtAddrPtr->sharedPtr;
    if (virtAddrPtr->sharedPtr != (Vm_SegProcList*)NIL) {
	/*
	 * Mangle the segment offset so that it matches the offset
	 * of the mapped segment.
	 */
	if (debugVmStubs) {
	    printf("Copying out shared segment\n");
	}
	machPtr->mapHardSeg -= (virtAddrPtr->sharedPtr->offset<<
		VMMACH_PAGE_SHIFT_INT)>>VMMACH_SEG_SHIFT;
	machPtr->mapHardSeg += machPtr->mapSegPtr->machPtr->offset;
    }

#ifdef sun4
    /*
     * Since this is a cross-address-space copy, we must make sure everything
     * has been flushed to the stack from our windows so that we don't
     * get stuff from windows overwriting stuff we copy to the stack later
     * when they're flushed.
     */
    Mach_FlushWindowsToStack();
#endif

    /*
     * Do a hardware segment's worth at a time until done.
     */
    while (numBytes > 0 && status == SUCCESS) {
	segOffset = (unsigned int)toAddr & (VMMACH_SEG_SIZE - 1);
	bytesToCopy = VMMACH_SEG_SIZE - segOffset;
	if (bytesToCopy > numBytes) {
	    bytesToCopy = numBytes;
	}
	/*
	 * First try quick and dirty copy.  If it fails, do regular copy.
	 */
	if (toProcPtr->vmPtr->machPtr->contextPtr != (VmMach_Context *)NIL) {
	    unsigned int	fromContext;
	    unsigned int	toContext;

	    fromContext = VmMachGetContextReg();
	    toContext = toProcPtr->vmPtr->machPtr->contextPtr->context;

	    status = VmMachQuickNDirtyCopy(bytesToCopy, fromAddr, toAddr,
		    fromContext, toContext);
	    VmMachSetContextReg((int)fromContext);

	    if (status == SUCCESS) {
		numBytes -= bytesToCopy;
		fromAddr += bytesToCopy;
		toAddr += bytesToCopy;
		continue;
	    }
	}
	/*
	 * Flush segment in context of toProcPtr.  If the context is NIL, then
	 * we can't and don't have to flush it, since it will be flushed before
	 * being re-used.
	 */
	if (vmMachHasVACache &&
	    toProcPtr->vmPtr->machPtr->contextPtr != (VmMach_Context *)NIL) {

	    oldContext = VmMachGetContextReg();
	    VmMachSetContextReg(
		    (int)toProcPtr->vmPtr->machPtr->contextPtr->context);
	    VmMachFlushSegment(toAddr);
	    VmMachSetContextReg(oldContext);
	}
	/*
	 * Push out the hardware segment.
	 */
	WriteHardMapSeg(machPtr);
	/*
	 * Do the copy.
	 */
	fromProcPtr->vmPtr->vmFlags |= VM_COPY_IN_PROGRESS;
	status = VmMachDoCopy(bytesToCopy, fromAddr,
			  (Address) (VMMACH_MAP_SEG_ADDR + segOffset));
	fromProcPtr->vmPtr->vmFlags &= ~VM_COPY_IN_PROGRESS;
	if (status == SUCCESS) {
	    numBytes -= bytesToCopy;
	    fromAddr += bytesToCopy;
	    toAddr += bytesToCopy;
	} else {
	    status = SYS_ARG_NOACCESS;
	}
	/*
	 * Zap the hardware segment.
	 */

	/* Flush this in current context */
	if (vmMachHasVACache) {
	    VmMachFlushSegment((Address) VMMACH_MAP_SEG_ADDR);
	}
	VmMachSetSegMap((Address)VMMACH_MAP_SEG_ADDR, VMMACH_INV_PMEG); 

	machPtr->mapHardSeg++;
    }
    machPtr->mapSegPtr = (struct Vm_Segment *)NIL;
    return(status);
}


/*
 *----------------------------------------------------------------------
 *
 * WriteHardMapSeg --
 *
 *	Push the hardware segment map entry out to the hardware for the
 *	given mapped segment.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Hardware segment modified.
 *
 *----------------------------------------------------------------------
 */
ENTRY static void
WriteHardMapSeg(machPtr)
    VmMach_ProcData	*machPtr;
{
    MASTER_LOCK(vmMachMutexPtr);

    if (machPtr->contextPtr != (VmMach_Context *) NIL) {
	machPtr->contextPtr->map[MAP_SEG_NUM] = 
	    (int)*GetHardSegPtr(machPtr->mapSegPtr->machPtr,
	    machPtr->mapHardSeg);
    }
    VmMachSetSegMap((Address)VMMACH_MAP_SEG_ADDR, 
	 (int)*GetHardSegPtr(machPtr->mapSegPtr->machPtr, machPtr->mapHardSeg));

    MASTER_UNLOCK(vmMachMutexPtr);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_SetSegProt --
 *
 *	Change the protection in the page table for the given range of bytes
 *	for the given segment.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Page table may be modified for the segment.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_SetSegProt(segPtr, firstPage, lastPage, makeWriteable)
    register Vm_Segment		*segPtr;    /* Segment to change protection
					       for. */
    register int		firstPage;  /* First page to set protection
					     * for. */
    int				lastPage;   /* First page to set protection
					     * for. */
    Boolean			makeWriteable;/* TRUE => make the pages 
					       *	 writable.
					       * FALSE => make readable only.*/
{
    register	VmMachPTE	pte;
    register	Address		virtAddr;
    register	VMMACH_SEG_NUM	*pmegNumPtr;
    register	PMEG		*pmegPtr;
    register	Boolean		skipSeg = FALSE;
    Boolean			nextSeg = TRUE;
    Address			tVirtAddr;
    Address			pageVirtAddr;
    int				i;
    int				oldContext;

    MASTER_LOCK(vmMachMutexPtr);

    pmegNumPtr = (VMMACH_SEG_NUM *)
	    GetHardSegPtr(segPtr->machPtr, PageToSeg(firstPage)) - 1;
    virtAddr = (Address)(firstPage << VMMACH_PAGE_SHIFT);
    while (firstPage <= lastPage) {
	if (nextSeg) {
	    pmegNumPtr++;
	    if (*pmegNumPtr != VMMACH_INV_PMEG) {
		pmegPtr = &pmegArray[*pmegNumPtr];
		if (pmegPtr->pageCount != 0) {
		    if (vmMachHasVACache) {
			/* Flush this segment in all contexts */
			oldContext =  VmMachGetContextReg();
			for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
			    VmMachSetContextReg(i);
			    VmMachFlushSegment(virtAddr);
			}
			VmMachSetContextReg(oldContext);
		    }
		    VmMachSetSegMap(vmMachPTESegAddr, (int)*pmegNumPtr);
		    skipSeg = FALSE;
		} else {
		    skipSeg = TRUE;
		}
	    } else {
		skipSeg = TRUE;
	    }
	    nextSeg = FALSE;
	}
	if (!skipSeg) {
	    /*
	     * Change the hardware page table.
	     */
	    tVirtAddr =
		((unsigned int)virtAddr & VMMACH_PAGE_MASK) + vmMachPTESegAddr;
	    for (i = 0; i < VMMACH_CLUSTER_SIZE; i++) {
		pageVirtAddr = tVirtAddr + i * VMMACH_PAGE_SIZE_INT;
		pte = VmMachGetPageMap(pageVirtAddr);
		if (pte & VMMACH_RESIDENT_BIT) {
		    pte &= ~VMMACH_PROTECTION_FIELD;
		    pte |= makeWriteable ? VMMACH_URW_PROT : VMMACH_UR_PROT;
#ifdef sun4
		    if (virtAddr >= vmStackEndAddr) {
			pte |= VMMACH_DONT_CACHE_BIT;
		    } else {
			pte &= ~VMMACH_DONT_CACHE_BIT;
		    }
#endif /* sun4 */
		    VmMachSetPageMap(pageVirtAddr, pte);
		}
	    }
	    virtAddr += VMMACH_PAGE_SIZE;
	    firstPage++;
	    if (((unsigned int)virtAddr & VMMACH_PAGE_MASK) == 0) {
		nextSeg = TRUE;
	    }
	} else {
	    int	segNum;

	    segNum = PageToSeg(firstPage) + 1;
	    firstPage = SegToPage(segNum);
	    virtAddr = (Address)(firstPage << VMMACH_PAGE_SHIFT);
	    nextSeg = TRUE;
	}
    }

    MASTER_UNLOCK(vmMachMutexPtr);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_SetPageProt --
 *
 *	Set the protection in hardware and software for the given virtual
 *	page.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Page table may be modified for the segment.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_SetPageProt(virtAddrPtr, softPTE)
    register	Vm_VirtAddr	*virtAddrPtr;	/* The virtual page to set the
						 * protection for.*/
    Vm_PTE			softPTE;	/* Software pte. */
{
    register	VmMachPTE 	hardPTE;
    register	VmMach_SegData	*machPtr;
    Address   			virtAddr;
    int				pmegNum;
    int				i;
#ifdef sun4
    Address			testVirtAddr;
#endif /* sun4 */
    int				j;
    int				oldContext;

    MASTER_LOCK(vmMachMutexPtr);

    machPtr = virtAddrPtr->segPtr->machPtr;
    pmegNum = *GetHardSegPtr(machPtr, PageToOffSeg(virtAddrPtr->page,
	    virtAddrPtr));
    if (pmegNum != VMMACH_INV_PMEG) {
	virtAddr = ((virtAddrPtr->page << VMMACH_PAGE_SHIFT) & 
			VMMACH_PAGE_MASK) + vmMachPTESegAddr;	
#ifdef sun4
	testVirtAddr = (Address) (virtAddrPtr->page << VMMACH_PAGE_SHIFT);
#endif sun4
	for (i = 0; 
	     i < VMMACH_CLUSTER_SIZE; 
	     i++, virtAddr += VMMACH_PAGE_SIZE_INT) {
	    hardPTE = VmMachReadPTE(pmegNum, virtAddr);
	    hardPTE &= ~VMMACH_PROTECTION_FIELD;
	    hardPTE |= (softPTE & (VM_COW_BIT | VM_READ_ONLY_PROT)) ? 
					VMMACH_UR_PROT : VMMACH_URW_PROT;
#ifdef sun4
	    if (testVirtAddr >= vmStackEndAddr) {
		hardPTE |= VMMACH_DONT_CACHE_BIT;
	    } else {
		hardPTE &= ~VMMACH_DONT_CACHE_BIT;
	    }
#endif /* sun4 */
	    if (vmMachHasVACache) {
		/* Flush this page in all contexts */
		oldContext =  VmMachGetContextReg();
		for (j = 0; j < VMMACH_NUM_CONTEXTS; j++) {
		    VmMachSetContextReg(j);
		    FLUSH_ALL_PAGE(testVirtAddr);
		}
		VmMachSetContextReg(oldContext);
	    }
	    VmMachWritePTE(pmegNum, virtAddr, hardPTE);
	}
    }

    MASTER_UNLOCK(vmMachMutexPtr);
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_AllocCheck --
 *
 *      Determine if this page can be reallocated.  A page can be reallocated
 *	if it has not been referenced or modified.
 *  
 * Results:
 *      None.
 *
 * Side effects:
 *      The given page will be invalidated in the hardware if it has not
 *	been referenced and *refPtr and *modPtr will have the hardware 
 *	reference and modify bits or'd in.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_AllocCheck(virtAddrPtr, virtFrameNum, refPtr, modPtr)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned	int		virtFrameNum;
    register	Boolean		*refPtr;
    register	Boolean		*modPtr;
{
    register VmMach_SegData	*machPtr;
    register VmMachPTE 		hardPTE;  
    int				pmegNum; 
    Address			virtAddr;
    int				i;
    int				origMod;

    MASTER_LOCK(vmMachMutexPtr);

    origMod = *modPtr;

    *refPtr |= refModMap[virtFrameNum] & VMMACH_REFERENCED_BIT;
    *modPtr = refModMap[virtFrameNum] & VMMACH_MODIFIED_BIT;
    if (!*refPtr || !*modPtr) {
	machPtr = virtAddrPtr->segPtr->machPtr;
	pmegNum = *GetHardSegPtr(machPtr, PageToOffSeg(virtAddrPtr->page,
		virtAddrPtr));
	if (pmegNum != VMMACH_INV_PMEG) {
	    hardPTE = 0;
	    virtAddr = 
		((virtAddrPtr->page << VMMACH_PAGE_SHIFT) & VMMACH_PAGE_MASK) + 
		    vmMachPTESegAddr;
	    for (i = 0; i < VMMACH_CLUSTER_SIZE; i++ ) {
		hardPTE |= VmMachReadPTE(pmegNum, 
					virtAddr + i * VMMACH_PAGE_SIZE_INT);
	    }
	    *refPtr |= hardPTE & VMMACH_REFERENCED_BIT;
	    *modPtr |= hardPTE & VMMACH_MODIFIED_BIT;
	}
    }
    if (!*refPtr) {
	/*
	 * Invalidate the page so that it will force a fault if it is
	 * referenced.  Since our caller has blocked all faults on this
	 * page, by invalidating it we can guarantee that the reference and
	 * modify information that we are returning will be valid until
	 * our caller reenables faults on this page.
	 */
	PageInvalidate(virtAddrPtr, virtFrameNum, FALSE);

	if (origMod && !*modPtr) {
	    /*
	     * This page had the modify bit set in software but not in
	     * hardware.
	     */
	    vmStat.notHardModPages++;
	}
    }
    *modPtr |= origMod;

    MASTER_UNLOCK(vmMachMutexPtr);

}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_GetRefModBits --
 *
 *      Pull the reference and modified bits out of hardware.
 *  
 * Results:
 *      None.
 *
 * Side effects:
 *      
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_GetRefModBits(virtAddrPtr, virtFrameNum, refPtr, modPtr)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned	int		virtFrameNum;
    register	Boolean		*refPtr;
    register	Boolean		*modPtr;
{
    register VmMach_SegData	*machPtr;
    register VmMachPTE 		hardPTE;  
    int				pmegNum; 
    Address			virtAddr;
    int				i;

    MASTER_LOCK(vmMachMutexPtr);

    *refPtr = refModMap[virtFrameNum] & VMMACH_REFERENCED_BIT;
    *modPtr = refModMap[virtFrameNum] & VMMACH_MODIFIED_BIT;
    if (!*refPtr || !*modPtr) {
	machPtr = virtAddrPtr->segPtr->machPtr;
	pmegNum = *GetHardSegPtr(machPtr, PageToOffSeg(virtAddrPtr->page,
		virtAddrPtr));
	if (pmegNum != VMMACH_INV_PMEG) {
	    hardPTE = 0;
	    virtAddr = 
		((virtAddrPtr->page << VMMACH_PAGE_SHIFT) & VMMACH_PAGE_MASK) + 
		    vmMachPTESegAddr;
	    for (i = 0; i < VMMACH_CLUSTER_SIZE; i++ ) {
		hardPTE |= VmMachReadPTE(pmegNum, 
					virtAddr + i * VMMACH_PAGE_SIZE_INT);
	    }
	    if (!*refPtr) {
		*refPtr = hardPTE & VMMACH_REFERENCED_BIT;
	    }
	    if (!*modPtr) {
		*modPtr = hardPTE & VMMACH_MODIFIED_BIT;
	    }
	}
    }

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_ClearRefBit --
 *
 *      Clear the reference bit at the given virtual address.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Hardware reference bit cleared.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_ClearRefBit(virtAddrPtr, virtFrameNum)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned 	int		virtFrameNum;
{
    register	VmMach_SegData	*machPtr;
    int				pmegNum;
    Address			virtAddr;
    int				i;
    VmMachPTE			pte;

    MASTER_LOCK(vmMachMutexPtr);

    refModMap[virtFrameNum] &= ~VMMACH_REFERENCED_BIT;
    machPtr = virtAddrPtr->segPtr->machPtr;
    pmegNum = *GetHardSegPtr(machPtr, PageToOffSeg(virtAddrPtr->page,
	    virtAddrPtr));
    if (pmegNum != VMMACH_INV_PMEG) {
	virtAddr = ((virtAddrPtr->page << VMMACH_PAGE_SHIFT) & 
			VMMACH_PAGE_MASK) + vmMachPTESegAddr;
	for (i = 0; 
	     i < VMMACH_CLUSTER_SIZE;
	     i++, virtAddr += VMMACH_PAGE_SIZE_INT) {
	    pte = VmMachReadPTE(pmegNum, virtAddr);
	    pte &= ~VMMACH_REFERENCED_BIT;
	    VmMachWritePTE(pmegNum, virtAddr, pte);
	}
    }

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_ClearModBit --
 *
 *      Clear the modified bit at the given virtual address.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Hardware modified bit cleared.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_ClearModBit(virtAddrPtr, virtFrameNum)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned	int		virtFrameNum;
{
    register	VmMach_SegData	*machPtr;
    int				pmegNum;
    Address			virtAddr;
    int				i;
    Vm_PTE			pte;

    MASTER_LOCK(vmMachMutexPtr);

    refModMap[virtFrameNum] &= ~VMMACH_MODIFIED_BIT;
    machPtr = virtAddrPtr->segPtr->machPtr;
    pmegNum = *GetHardSegPtr(machPtr, PageToOffSeg(virtAddrPtr->page,
	    virtAddrPtr));
    if (pmegNum != VMMACH_INV_PMEG) {
	virtAddr = ((virtAddrPtr->page << VMMACH_PAGE_SHIFT) & 
			VMMACH_PAGE_MASK) + vmMachPTESegAddr;
	for (i = 0; 
	     i < VMMACH_CLUSTER_SIZE; 
	     i++, virtAddr += VMMACH_PAGE_SIZE_INT) {
	    pte = VmMachReadPTE(pmegNum, virtAddr);
	    pte &= ~VMMACH_MODIFIED_BIT;
	    VmMachWritePTE(pmegNum, virtAddr, pte);
	}
    }

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_PageValidate --
 *
 *      Validate a page for the given virtual address.  It is assumed that when
 *      this routine is called that the user context register contains the
 *	context in which the page will be validated.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The page table and hardware segment tables associated with the segment
 *      are modified to validate the page.
 *
 * ----------------------------------------------------------------------------
 */

ENTRY void
VmMach_PageValidate(virtAddrPtr, pte) 
    register	Vm_VirtAddr	*virtAddrPtr;
    Vm_PTE			pte;
{
    register  Vm_Segment	*segPtr;
    register  VMMACH_SEG_NUM	*segTablePtr;
    register  PMEG		*pmegPtr;
    register  int		hardSeg;
    register  VmMachPTE		hardPTE;
    register  VmMachPTE		tHardPTE;
    struct	VmMach_PMEGseg	*pmegSegPtr;
    Vm_PTE    *ptePtr;
    Address	addr;
    Boolean	reLoadPMEG;  	/* TRUE if we had to reload this PMEG. */
    int		i;
    int		tmpSegNum;

    MASTER_LOCK(vmMachMutexPtr);

    segPtr = virtAddrPtr->segPtr;
    addr = (Address) (virtAddrPtr->page << VMMACH_PAGE_SHIFT);
#ifdef sun4
	if (!VMMACH_ADDR_CHECK(addr)) {
	panic("VmMach_PageValidate: virt addr 0x%x falls into illegal range!\n",
		addr);
    }
#endif sun4

    /*
     * Find out the hardware segment that has to be mapped.
     * If this is a mapping segment, this gives us the seg num
     * for the segment in the other process and the segPtr is the mapSegPtr
     * which is set to the segPtr of the other process.
     */

    hardSeg = PageToOffSeg(virtAddrPtr->page, virtAddrPtr);

    segTablePtr = (VMMACH_SEG_NUM *) GetHardSegPtr(segPtr->machPtr, hardSeg);
    pmegPtr = &pmegArray[*segTablePtr]; /* Software seg's pmeg. */
    tmpSegNum = VmMachGetSegMap(addr);  /* Hardware seg's pmeg. */
    if (tmpSegNum != VMMACH_INV_PMEG && tmpSegNum != (int)*segTablePtr) {
	if (!(Proc_GetCurrentProc()->vmPtr->vmFlags & VM_COPY_IN_PROGRESS)) {
	    if (*segTablePtr != VMMACH_INV_PMEG) {
		if (debugVmStubs) {
		    printf("VmMach_PageValidate: multiple pmegs used!\n");
		    printf(" seg = %d, pmeg %d,%d, proc=%x %s\n",
			    segPtr->segNum, *segTablePtr, tmpSegNum,
			    Proc_GetCurrentProc()->processID,
			    Proc_GetCurrentProc()->argString);
		    printf("  old seg = %x\n",
			    pmegArray[tmpSegNum].segInfo.segPtr->segNum);
		    printf("Freeing pmeg %d\n", *segTablePtr);
		}
		PMEGFree(*segTablePtr);
	    }
	    if (debugVmStubs) {
		printf("Multiple segs in hard segment: seg = %d, pmeg %d,%d, proc=%x %s\n",
		    segPtr->segNum, *segTablePtr, tmpSegNum,
		    Proc_GetCurrentProc()->processID,
		    Proc_GetCurrentProc()->argString);
	    }

	    *segTablePtr = (VMMACH_SEG_NUM) tmpSegNum;
	    pmegPtr = &pmegArray[*segTablePtr];
	    if (virtAddrPtr->segPtr->machPtr->pmegInfo.inuse) {
		printf("VmMach_PageValidate: segment sharing table overflow\n");
	    } else {
		pmegSegPtr = &virtAddrPtr->segPtr->machPtr->pmegInfo;
		pmegSegPtr->inuse = 1;
		pmegSegPtr->nextLink = pmegPtr->segInfo.nextLink;
		pmegPtr->segInfo.nextLink = pmegSegPtr;
		pmegSegPtr->segPtr = virtAddrPtr->segPtr;
		pmegSegPtr->hardSegNum = hardSeg;
	    }
	}
    }

    reLoadPMEG = FALSE;
    if (*segTablePtr == VMMACH_INV_PMEG) {
	int flags;
	/*
	 * If there is not already a pmeg for this hardware segment, then get
	 * one and initialize it.  If this is for the kernel then make
	 * sure that the pmeg cannot be taken away from the kernel.
	 * We make an exception for PMEGs allocated only for the block cache.
	 * If we fault on kernel pmeg we reload all the mappings for the
	 * pmeg because we can't tolerate "quick" faults in some places in the
	 * kernel.
	 */
	flags = 0;
	if (segPtr == vm_SysSegPtr) {
	   if (IN_FILE_CACHE_SEG(addr) && vmMachCanStealFileCachePmegs) {
	      /*
	       * In block cache virtual addresses.
	       */
	      reLoadPMEG = TRUE;
	   } else {
	      /*
	       * Normal kernel PMEGs must still be wired.
	       */
	      flags = PMEG_DONT_ALLOC;
	   }
	}
        *segTablePtr = PMEGGet(segPtr, hardSeg, flags);
	pmegPtr = &pmegArray[*segTablePtr];
    } else {
	pmegPtr = &pmegArray[*segTablePtr];
	if (pmegPtr->pageCount == 0) {
	    /*
	     * We are using a PMEG that had a pagecount of 0.  In this case
	     * it was put onto the end of the free pmeg list in anticipation
	     * of someone stealing this empty pmeg.  Now we have to move
	     * it off of the free list.
	     */
	    if (pmegPtr->flags & PMEG_DONT_ALLOC) {
		List_Remove((List_Links *)pmegPtr);
	    } else {
		List_Move((List_Links *)pmegPtr, LIST_ATREAR(pmegInuseList));
	    }
	}
    }
    hardPTE = VMMACH_RESIDENT_BIT | VirtToPhysPage(Vm_GetPageFrame(pte));
#ifdef sun4
    if (addr < (Address) VMMACH_DEV_START_ADDR) {
	    hardPTE &= ~VMMACH_DONT_CACHE_BIT;
    } else {
	hardPTE |= VMMACH_DONT_CACHE_BIT;
    }
#endif /* sun4 */
    if (segPtr == vm_SysSegPtr) {
	int	oldContext;
	/*
	 * Have to propagate the PMEG to all contexts.
	 */
	oldContext = VmMachGetContextReg();
	for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	    VmMachSetContextReg(i);
	    VmMachSetSegMap(addr, (int)*segTablePtr);
	}
	VmMachSetContextReg(oldContext);
	hardPTE |= VMMACH_KRW_PROT;
    } else {
	Proc_ControlBlock	*procPtr;
	VmProcLink		*procLinkPtr;
	VmMach_Context  	*contextPtr;

	procPtr = Proc_GetCurrentProc();
	if (virtAddrPtr->flags & USING_MAPPED_SEG) {
	    addr = (Address) (VMMACH_MAP_SEG_ADDR + 
				((unsigned int)addr & (VMMACH_SEG_SIZE - 1)));
	    /* PUT IT IN SOFTWARE OF MAP AREA FOR PROCESS */
	    procPtr->vmPtr->machPtr->contextPtr->map[MAP_SEG_NUM] =
		    *segTablePtr;
	} else{
	    /* update it for regular seg num */
	    procPtr->vmPtr->machPtr->contextPtr->map[hardSeg] = *segTablePtr;
	}
	VmMachSetSegMap(addr, (int)*segTablePtr);

        if (segPtr != (Vm_Segment *) NIL) {
            LIST_FORALL(segPtr->procList, (List_Links *)procLinkPtr) {
		if (procLinkPtr->procPtr->vmPtr != (Vm_ProcInfo *) NIL &&
			procLinkPtr->procPtr->vmPtr->machPtr !=
			(VmMach_ProcData *) NIL &&
			(contextPtr =
			procLinkPtr->procPtr->vmPtr->machPtr->contextPtr) !=
			(VmMach_Context *) NIL) {
		    contextPtr->map[hardSeg] = *segTablePtr;
		}
	    }
	}

	if ((pte & (VM_COW_BIT | VM_READ_ONLY_PROT)) ||
		(virtAddrPtr->flags & VM_READONLY_SEG)) {
	    hardPTE |= VMMACH_UR_PROT;
	} else {
	    hardPTE |= VMMACH_URW_PROT;
	}
    }
    tHardPTE = VmMachGetPageMap(addr);
    if (tHardPTE & VMMACH_RESIDENT_BIT) {
	hardPTE |= tHardPTE & (VMMACH_REFERENCED_BIT | VMMACH_MODIFIED_BIT);
	for (i = 1; i < VMMACH_CLUSTER_SIZE; i++ ) {
	    hardPTE |= VmMachGetPageMap(addr + i * VMMACH_PAGE_SIZE_INT) & 
			    (VMMACH_REFERENCED_BIT | VMMACH_MODIFIED_BIT);
	}
    } else {
	if (*segTablePtr == VMMACH_INV_PMEG) {
	    panic("Invalid pmeg\n");
	}
	pmegArray[*segTablePtr].pageCount++;
    }
    /* Flush something? */
    SET_ALL_PAGE_MAP(addr, hardPTE);
    if (reLoadPMEG) {
	/*
	 * Reload all pte's for this pmeg.
	 */
	unsigned int a = (hardSeg << VMMACH_SEG_SHIFT);
	int	pageCount = 0;
	for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG; i++ ) { 
	    ptePtr = VmGetPTEPtr(vm_SysSegPtr, (a >> VMMACH_PAGE_SHIFT));
	    if ((*ptePtr & VM_PHYS_RES_BIT)) {
		hardPTE = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | 
				VirtToPhysPage(Vm_GetPageFrame(*ptePtr));
		SET_ALL_PAGE_MAP(a, hardPTE);
		pageCount++;
	     }
	     a += VMMACH_PAGE_SIZE;
	}
	if (pmegPtr-pmegArray == VMMACH_INV_PMEG) {
	    panic("Invalid pmeg\n");
	}
	pmegPtr->pageCount = pageCount;
    }

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}


/*
 * ----------------------------------------------------------------------------
 *
 * PageInvalidate --
 *
 *      Invalidate a page for the given segment.  
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The page table and hardware segment tables associated with the segment
 *      are modified to invalidate the page.
 *
 * ----------------------------------------------------------------------------
 */
static void
PageInvalidate(virtAddrPtr, virtPage, segDeletion) 
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned 	int		virtPage;
    Boolean			segDeletion;
{
    register VmMach_SegData	*machPtr;
    register PMEG		*pmegPtr;
    VmMachPTE			hardPTE;
    int				pmegNum;
    Address			addr;
    int				i;
    Address			testVirtAddr;
    VmProcLink      		*flushProcLinkPtr;
    Vm_Segment      		*flushSegPtr;
    Vm_ProcInfo     		*flushVmPtr;
    VmMach_ProcData 		*flushMachPtr;
    VmMach_Context  		*flushContextPtr;
    unsigned int    		flushContext;
    unsigned int    		oldContext;

    refModMap[virtPage] = 0;
    if (segDeletion) {
	return;
    }
    machPtr = virtAddrPtr->segPtr->machPtr;
    pmegNum = *GetHardSegPtr(machPtr, PageToOffSeg(virtAddrPtr->page,
	    virtAddrPtr));
    if (pmegNum == VMMACH_INV_PMEG) {
	return;
    }
    testVirtAddr = (Address) (virtAddrPtr->page << VMMACH_PAGE_SHIFT);
    addr = ((virtAddrPtr->page << VMMACH_PAGE_SHIFT) &
				VMMACH_PAGE_MASK) + vmMachPTESegAddr;
#ifdef sun4
    if (!VMMACH_ADDR_CHECK(addr)) {
	panic("PageInvalidate: virt addr 0x%x falls into illegal range!\n",
		addr);
    }
#endif sun4
    hardPTE = VmMachReadPTE(pmegNum, addr);
    /*
     * Invalidate the page table entry.  There's no need to flush the page if
     * the invalidation is due to segment deletion, since the whole segment
     * will already have been flushed.  Flush the page in the context in which
     * it was validated.
     */
    if (!segDeletion && vmMachHasVACache) {
	int	flushedP = FALSE;

        flushSegPtr = virtAddrPtr->segPtr;
        if (flushSegPtr != (Vm_Segment *) NIL) {
            LIST_FORALL(flushSegPtr->procList, (List_Links *)flushProcLinkPtr) {
                flushVmPtr = flushProcLinkPtr->procPtr->vmPtr;
                if (flushVmPtr != (Vm_ProcInfo *) NIL) {
                    flushMachPtr = flushVmPtr->machPtr;
                    if (flushMachPtr != (VmMach_ProcData *) NIL) {
                        flushContextPtr = flushMachPtr->contextPtr;
                        if (flushContextPtr != (VmMach_Context *) NIL) {
                            flushContext = flushContextPtr->context;
                            /* save old context */
                            oldContext = VmMachGetContextReg();
                            /* move to page's context */
                            VmMachSetContextReg((int)flushContext);
                            /* flush page in its context */
                            FLUSH_ALL_PAGE(testVirtAddr);
                            /* back to old context */
                            VmMachSetContextReg((int)oldContext);
			    flushedP = TRUE;
                        }
                    }
                }
            }
        }
	if (!flushedP) {
	    FLUSH_ALL_PAGE(testVirtAddr);
	}
    }
    for (i = 0; i < VMMACH_CLUSTER_SIZE; i++, addr += VMMACH_PAGE_SIZE_INT) {
	VmMachWritePTE(pmegNum, addr, (VmMachPTE)0);
    }
    pmegPtr = &pmegArray[pmegNum];
    if (hardPTE & VMMACH_RESIDENT_BIT) {
	pmegPtr->pageCount--;
	if (pmegPtr->pageCount == 0) {
	    /*
	     * When the pageCount goes to zero, the pmeg is put onto the end
	     * of the free list so that it can get freed if someone else
	     * needs a pmeg.  It isn't freed here because there is a fair
	     * amount of overhead when freeing a pmeg so its best to keep
	     * it around in case it is needed again.
	     */
	    if (pmegPtr->flags & PMEG_DONT_ALLOC) {
		List_Insert((List_Links *)pmegPtr, 
			    LIST_ATREAR(pmegFreeList));
	    } else {
		List_Move((List_Links *)pmegPtr, 
			  LIST_ATREAR(pmegFreeList));
	    }
	}
    }
    return;
}



/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_PageInvalidate --
 *
 *      Invalidate a page for the given segment.  
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The page table and hardware segment tables associated with the segment
 *      are modified to invalidate the page.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_PageInvalidate(virtAddrPtr, virtPage, segDeletion) 
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned 	int		virtPage;
    Boolean			segDeletion;
{
    MASTER_LOCK(vmMachMutexPtr);

    PageInvalidate(virtAddrPtr, virtPage, segDeletion);

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_PinUserPages --
 *
 *	Force a user page to be resident in memory.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMach_PinUserPages(mapType, virtAddrPtr, lastPage)
    int		mapType;
    Vm_VirtAddr	*virtAddrPtr;
    int		lastPage;
{
    int				*intPtr;
    int				dummy;
    register VmMach_SegData	*machPtr;
    register int		firstSeg;
    register int		lastSeg;

    machPtr = virtAddrPtr->segPtr->machPtr;

    firstSeg = PageToOffSeg(virtAddrPtr->page, virtAddrPtr);
    lastSeg = PageToOffSeg(lastPage, virtAddrPtr);
    /*
     * Lock down the PMEG behind the first segment.
     */
    intPtr = (int *) (virtAddrPtr->page << VMMACH_PAGE_SHIFT);
    while (!PMEGLock(machPtr, firstSeg)) {
	/*
	 * Touch the page to bring it into memory.  We know that we can
	 * safely touch it because we wouldn't have been called if these
	 * weren't good addresses.
	 */
	dummy = *intPtr;
    }
    /*
     * Lock down the rest of the segments.
     */
    for (firstSeg++; firstSeg <= lastSeg; firstSeg++) {
	intPtr = (int *)(firstSeg << VMMACH_SEG_SHIFT);
	while (!PMEGLock(machPtr, firstSeg)) {
	    dummy = *intPtr;
	}
    }
#ifdef lint
    dummy = dummy;
#endif
    return;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_UnpinUserPages --
 *
 *	Allow a page that was pinned to be unpinned.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_UnpinUserPages(virtAddrPtr, lastPage)
    Vm_VirtAddr	*virtAddrPtr;
    int		lastPage;
{
    register int	firstSeg;
    register int	lastSeg;
    int			pmegNum;
    register VmMach_SegData	*machPtr;

    MASTER_LOCK(vmMachMutexPtr);

    machPtr = virtAddrPtr->segPtr->machPtr;
    firstSeg = PageToOffSeg(virtAddrPtr->page, virtAddrPtr);
    lastSeg = PageToOffSeg(lastPage, virtAddrPtr);
    for (; firstSeg <= lastSeg; firstSeg++) {
	pmegNum = *GetHardSegPtr(machPtr, firstSeg);
	if (pmegNum == VMMACH_INV_PMEG) {
	    MASTER_UNLOCK(vmMachMutexPtr);
	    panic("Pinned PMEG invalid???\n");
	    return;
	}
	pmegArray[pmegNum].lockCount--;
    }

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}


/*
 ----------------------------------------------------------------------
 *
 * VmMach_MapInDevice --
 *
 *	Map a device at some physical address into kernel virtual address.
 *	This is for use by the controller initialization routines.
 *	This routine looks for a free page in the special range of
 *	kernel virtual that is reserved for this kind of thing and
 *	sets up the page table so that it references the device.
 *
 * Results:
 *	The kernel virtual address needed to reference the device is returned.
 *
 * Side effects:
 *	The hardware page table is modified.  This may steal another
 *	page from kernel virtual space, unless a page can be cleverly re-used.
 *
 *----------------------------------------------------------------------
 */
Address
VmMach_MapInDevice(devPhysAddr, type)
    Address	devPhysAddr;	/* Physical address of the device to map in */
    int		type;		/* Value for the page table entry type field.
				 * This depends on the address space that
				 * the devices live in, ie. VME D16 or D32 */
{
    Address 		virtAddr;
    Address		freeVirtAddr = (Address)0;
    Address		freePMEGAddr = (Address)0;
    int			page;
    int			pageFrame;
    VmMachPTE		pte;

    /*
     * Get the page frame for the physical device so we can
     * compare it against existing pte's.
     */
    pageFrame = ((unsigned)devPhysAddr >> VMMACH_PAGE_SHIFT_INT)
	& VMMACH_PAGE_FRAME_FIELD;

    /*
     * Spin through the segments and their pages looking for a free
     * page or a virtual page that is already mapped to the physical page.
     */
    for (virtAddr = (Address)VMMACH_DEV_START_ADDR;
         virtAddr < (Address)VMMACH_DEV_END_ADDR; ) {
	if (VmMachGetSegMap(virtAddr) == VMMACH_INV_PMEG) {
	    /* 
	     * If we can't find any free mappings we can use this PMEG.
	     */
	    if (freePMEGAddr == 0) {
		freePMEGAddr = virtAddr;
	    }
	    virtAddr += VMMACH_SEG_SIZE;
	    continue;
	}
	/*
	 * Careful, use the correct page size when incrementing virtAddr.
	 * Use the real hardware size (ignore software klustering) because
	 * we are at a low level munging page table entries ourselves here.
	 */
	for (page = 0;
	     page < VMMACH_NUM_PAGES_PER_SEG_INT;
	     page++, virtAddr += VMMACH_PAGE_SIZE_INT) {
	    pte = VmMachGetPageMap(virtAddr);
	    if (!(pte & VMMACH_RESIDENT_BIT)) {
	        if (freeVirtAddr == 0) {
		    /*
		     * Note the unused page in this special area of the
		     * kernel virtual address space.
		     */
		    freeVirtAddr = virtAddr;
		}
	    } else if ((pte & VMMACH_PAGE_FRAME_FIELD) == pageFrame &&
		       VmMachGetPageType(pte) == type) {
		/*
		 * A page is already mapped for this physical address.
		 */
		return(virtAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK_INT));
	    }
	}
    }

    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | pageFrame;
#if defined(sun3) || defined(sun4)	/* Not just for porting purposes */
    pte |= VMMACH_DONT_CACHE_BIT;
#endif
    VmMachSetPageType(pte, type);
    if (freeVirtAddr != 0) {
	VmMachSetPageMap(freeVirtAddr, pte);
	/*
	 * Return the kernel virtual address used to access it.
	 */
	return(freeVirtAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK_INT));
    } else if (freePMEGAddr != 0) {
	int oldContext;
	int pmeg;
	int i;

	/*
	 * Map in a new PMEG so we can use it for mapping.
	 */
	pmeg = PMEGGet(vm_SysSegPtr, 
		       (int) ((unsigned)freePMEGAddr >> VMMACH_SEG_SHIFT),
		       PMEG_DONT_ALLOC);
	oldContext = VmMachGetContextReg();
	for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	    VmMachSetContextReg(i);
	    VmMachSetSegMap(freePMEGAddr, pmeg);
	}
	VmMachSetContextReg(oldContext);
	VmMachSetPageMap(freePMEGAddr, pte);
	return(freePMEGAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK_INT));
    } else {
	return((Address)NIL);
    }
}

/*
 ----------------------------------------------------------------------
 *
 * VmMach_MapInBigDevice --
 *
 *	Map a device at some physical address into kernel virtual address.
 *	This is for use by the controller initialization routines.
 *	This is suitable for devices that need to map in more than
 *	one page of contiguous memory.
 *	The routine sets up the page table so that it references the device.
 *
 * Results:
 *	The kernel virtual address needed to reference the device is returned.
 *	NIL is returned upon failure.
 *
 * Side effects:
 *	The hardware page table is modified.  This may steal
 *	pages from kernel virtual space, unless pages can be cleverly re-used.
 *
 *----------------------------------------------------------------------
 */
Address
VmMach_MapInBigDevice(devPhysAddr, numBytes, type)
    Address	devPhysAddr;	/* Physical address of the device to map in. */
    int		numBytes;	/* Bytes needed by device. */
    int		type;		/* Value for the page table entry type field.
				 * This depends on the address space that
				 * the devices live in, ie. VME D16 or D32. */
{
    Address 		virtAddr;
    Address		freeVirtAddr = (Address)0;
    Address		freePMEGAddr = (Address)0;
    int			page;
    int			pageFrame;
    VmMachPTE		pte;
    int			numPages;
    int			i;
    Boolean		foundPages;

    /*
     * Get the page frame for the physical device so we can
     * compare it against existing pte's.
     */
printf("VmMach_MapInBigDevice:devPhysAddr 0x%x, numBytes %d, type %d\n",
devPhysAddr, numBytes, type);
    pageFrame = ((unsigned)devPhysAddr >> VMMACH_PAGE_SHIFT_INT)
	& VMMACH_PAGE_FRAME_FIELD;

    numPages = (numBytes / VMMACH_PAGE_SIZE_INT) + 1;
    if (numPages > VMMACH_NUM_PAGES_PER_SEG_INT) {
	printf("Can only map in one segment's worth of pages right now.\n");
	return (Address) NIL;
    }
printf("numPages is %d\n", numPages);
    /* For only one pages, just call the old routine. */
    if (numPages <= 1) {
	return VmMach_MapInDevice(devPhysAddr, type);
    }

    /*
     * Spin through the segments and their pages looking for a free
     * page or a virtual page that is already mapped to the physical page.
     */
    for (virtAddr = (Address)VMMACH_DEV_START_ADDR;
         virtAddr < (Address)VMMACH_DEV_END_ADDR; ) {
printf("Trying virtAddr 0x%x\n", virtAddr);
	if (VmMachGetSegMap(virtAddr) == VMMACH_INV_PMEG) {
	    /* 
	     * If we can't find any free mappings we can use this PMEG.
	     */
	    if (freePMEGAddr == 0) {
printf("Got free pmeg.\n");
		freePMEGAddr = virtAddr;
	    }
	    virtAddr += VMMACH_SEG_SIZE;
	    continue;
	}
	/*
	 * Careful, use the correct page size when incrementing virtAddr.
	 * Use the real hardware size (ignore software klustering) because
	 * we are at a low level munging page table entries ourselves here.
	 */
	foundPages = FALSE;
	for (page = 0;
	     page < VMMACH_NUM_PAGES_PER_SEG_INT;
	     page++, virtAddr += VMMACH_PAGE_SIZE_INT) {

	    /* Are there enough pages left in the segment? */
	    if (VMMACH_NUM_PAGES_PER_SEG_INT - page < numPages) {
printf("Only %d pages left - go to next seg.\n", VMMACH_NUM_PAGES_PER_SEG_INT -
numPages);
		/* If we just continue, virtAddr will be incremented okay. */
		continue;
	    }

	    pte = VmMachGetPageMap(virtAddr);
	    if ((pte & VMMACH_RESIDENT_BIT) &&
		(pte & VMMACH_PAGE_FRAME_FIELD) == pageFrame &&
		       VmMachGetPageType(pte) == type) {
		/*
		 * A page is already mapped for this physical address.
		 */
		printf("A page is already mapped for this device!\n");
		return (Address) NIL;
#ifdef NOTDEF
		/*
		 * Instead, we could loop through trying to find all
		 * mapped pages, but I don't think we'll ever find any
		 * for these devices.
		 */
		return (virtAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK_INT));
#endif NOTDEF
	    }

	    if (!(pte & VMMACH_RESIDENT_BIT)) {
		/*
		 * Note the unused page in this special area of the
		 * kernel virtual address space.
		 */
		freeVirtAddr = virtAddr;
printf("Free page # %d at add 0x%x\n", page, virtAddr); 

		/* See if we have enough other consecutive pages. */
		for (i = 1; i < numPages; i++) {
		    pte = VmMachGetPageMap(virtAddr +
			    (i * VMMACH_PAGE_SIZE_INT));
		    /* If this is already taken, give up and continue. */
		    if (pte & VMMACH_RESIDENT_BIT) {
printf("Oops, page # %d is taken.\n", page + i);
			/* Maybe I should check if it's this device mapped? */
			break;
		    }
		}
		/* Did we find enough pages? */
		if (i == numPages) {
		    foundPages = TRUE;
printf("Found enough pages.\n");
		    break;
		}
		/* The address wasn't good. */
		freeVirtAddr = (Address) 0;

		/* So that we'll test the right page next time around. */
		page += i;
		virtAddr += (i * VMMACH_PAGE_SIZE_INT);
	    }
	}
	if (foundPages) {
printf("Yup, really found pages.\n");
	    break;
	}
    }

    /* Did we find a set of pages? */
    if (freeVirtAddr != 0) {
	virtAddr = freeVirtAddr;
printf("Using pages at addr 0x%x\n", virtAddr);
	for (i = 0; i < numPages; i++) {
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | pageFrame;
#if defined(sun3) || defined(sun4)
	    pte |= VMMACH_DONT_CACHE_BIT;
#endif
	    VmMachSetPageType(pte, type);
	    VmMachSetPageMap(virtAddr, pte);
	    pageFrame++;
	    virtAddr += VMMACH_PAGE_SIZE_INT;
	}

	/*
	 * Return the kernel virtual address used to access it.
	 */
	return (freeVirtAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK_INT));

    /* Or did we find a whole free pmeg? */
    } else if (freePMEGAddr != 0) {
	int oldContext;
	int pmeg;

	/*
	 * Map in a new PMEG so we can use it for mapping.
	 */
printf("Found a whole pmeg at 0x%x\n", freePMEGAddr);
	pmeg = PMEGGet(vm_SysSegPtr, 
		       (int) ((unsigned)freePMEGAddr >> VMMACH_SEG_SHIFT),
		       PMEG_DONT_ALLOC);
	oldContext = VmMachGetContextReg();
	for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	    VmMachSetContextReg(i);
	    VmMachSetSegMap(freePMEGAddr, pmeg);
	}
	VmMachSetContextReg(oldContext);

	virtAddr = freePMEGAddr;
	for (i = 0; i < numPages; i++) {
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | pageFrame;
#if defined(sun3) || defined(sun4)
	    pte |= VMMACH_DONT_CACHE_BIT;
#endif
	    VmMachSetPageType(pte, type);
	    VmMachSetPageMap(virtAddr, pte);
	    pageFrame++;
	    virtAddr += VMMACH_PAGE_SIZE_INT;
	}

	return (freePMEGAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK_INT));

    }

    /* Nothing was found. */
printf("Found nothing.\n");
    return (Address) NIL;
}


/*----------------------------------------------------------------------
 *
 * DevBufferInit --
 *
 *	Initialize a range of virtual memory to allocate from out of the
 *	device memory space.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The buffer struct is initialized and the hardware page map is zeroed
 *	out in the range of addresses.
 *
 *----------------------------------------------------------------------
 */
INTERNAL static void
DevBufferInit()
{
    Address		virtAddr;
    unsigned char	pmeg;
    int			oldContext;
    int			i;
    Address	baseAddr;
    Address	endAddr;

    /*
     * Round base up to next page boundary and end down to page boundary.
     */
    baseAddr = (Address)VMMACH_DMA_START_ADDR;
    endAddr = (Address)(VMMACH_DMA_START_ADDR + VMMACH_DMA_SIZE);

    /* 
     * Set up the hardware pages tables in the range of addresses given.
     */
    for (virtAddr = baseAddr; virtAddr < endAddr; ) {
	if (VmMachGetSegMap(virtAddr) != VMMACH_INV_PMEG) {
	    printf("DevBufferInit: DMA space already valid at 0x%x\n",
		   (unsigned int) virtAddr);
	}
	/* 
	 * Need to allocate a PMEG.
	 */
	pmeg = PMEGGet(vm_SysSegPtr, 
		       (int) ((unsigned)virtAddr >> VMMACH_SEG_SHIFT),
		       PMEG_DONT_ALLOC);
	oldContext = VmMachGetContextReg();
	for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	    VmMachSetContextReg(i);
	    VmMachSetSegMap(virtAddr, (int)pmeg);
	}
	VmMachSetContextReg(oldContext);
	virtAddr += VMMACH_SEG_SIZE;
    }
    return;
}


static	Boolean	dmaPageBitMap[VMMACH_DMA_SIZE / VMMACH_PAGE_SIZE_INT];

static Boolean dmaInitialized = FALSE;

/*
 ----------------------------------------------------------------------
 *
 * VmMach_DMAAlloc --
 *
 *	Allocate a set of virtual pages to a routine for mapping purposes.
 *	
 * Results:
 *	Pointer into kernel virtual address space of where to access the
 *	memory, or NIL if the request couldn't be satisfied.
 *
 * Side effects:
 *	The hardware page table is modified.
 *
 *----------------------------------------------------------------------
 */
ENTRY Address
VmMach_DMAAlloc(numBytes, srcAddr)
    int		numBytes;		/* Number of bytes to map in. */
    Address	srcAddr;	/* Kernel virtual address to start mapping in.*/
{
    Address	beginAddr;
    Address	endAddr;
    int		numPages;
    int		i, j;
    VmMachPTE	pte;
    Boolean	foundIt = FALSE;
    Address	newAddr;
 
    MASTER_LOCK(vmMachMutexPtr);
    if (!dmaInitialized) {
	/* Where to allocate the memory from. */
	dmaInitialized = TRUE;
	DevBufferInit();
    }

    /* calculate number of pages needed */
						/* beginning of first page */
    beginAddr = (Address) (((unsigned int)(srcAddr)) & ~VMMACH_OFFSET_MASK_INT);
						/* beginning of last page */
    endAddr = (Address) ((((unsigned int) srcAddr) + numBytes - 1) &
	    ~VMMACH_OFFSET_MASK_INT);
    numPages = (((unsigned int) endAddr) >> VMMACH_PAGE_SHIFT_INT) -
	    (((unsigned int) beginAddr) >> VMMACH_PAGE_SHIFT_INT) + 1;

    /* see if request can be satisfied */
    for (i = 0; i < (VMMACH_DMA_SIZE / VMMACH_PAGE_SIZE_INT); i++) {
	if (dmaPageBitMap[i] == 1) {
	    continue;
	}
	/*
	 * Must be aligned in the cache to avoid write-backs of stale data
	 * from other references to stuff on this page.
	 */
	newAddr = (Address)(VMMACH_DMA_START_ADDR + (i * VMMACH_PAGE_SIZE_INT));
	if (((unsigned int) newAddr & (VMMACH_CACHE_SIZE - 1)) !=
		((unsigned int) beginAddr & (VMMACH_CACHE_SIZE - 1))) {
	    continue;
	}
	for (j = 1; j < numPages &&
		((i + j) < (VMMACH_DMA_SIZE / VMMACH_PAGE_SIZE_INT)); j++) {
	    if (dmaPageBitMap[i + j] == 1) {
		break;
	    }
	}
	if (j == numPages &&
		((i + j) < (VMMACH_DMA_SIZE / VMMACH_PAGE_SIZE_INT))) {
	    foundIt = TRUE;
	    break;
	}
    }
    if (!foundIt) {
	MASTER_UNLOCK(vmMachMutexPtr);
	panic(
	    "VmMach_DMAAlloc: unable to satisfy request for %d bytes at 0x%x\n",
		numBytes, srcAddr);
#ifdef NOTDEF
	return (Address) NIL;
#endif NOTDEF
    }
    for (j = 0; j < numPages; j++) {
	dmaPageBitMap[i + j] = 1;	/* allocate page */
	pte = VmMachGetPageMap(srcAddr);
	pte |= VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT;
	VmMachSetPageMap(((i + j) * VMMACH_PAGE_SIZE_INT) +
		VMMACH_DMA_START_ADDR, pte);
	srcAddr += VMMACH_PAGE_SIZE_INT;
    }
    beginAddr = (Address) (VMMACH_DMA_START_ADDR + (i * VMMACH_PAGE_SIZE_INT) +
	    (((unsigned int) srcAddr) & VMMACH_OFFSET_MASK_INT));

    MASTER_UNLOCK(vmMachMutexPtr);
    return beginAddr;
}

/*
 ----------------------------------------------------------------------
 *
 * VmMach_DMAAllocContiguous --
 *
 *	WARNING:  this routine doesn't work yet!!
 *	Allocate a set of virtual pages to a routine for mapping purposes.
 *	
 * Results:
 *	Pointer into kernel virtual address space of where to access the
 *	memory, or NIL if the request couldn't be satisfied.
 *
 * Side effects:
 *	The hardware page table is modified.
 *
 *----------------------------------------------------------------------
 */
#ifndef sun4c
ReturnStatus
VmMach_DMAAllocContiguous(inScatGathPtr, scatGathLength, outScatGathPtr)
    register Net_ScatterGather	*inScatGathPtr;
    register int		scatGathLength;
    register Net_ScatterGather	*outScatGathPtr;
{
    Address	beginAddr = 0;
    Address	endAddr;
    int		numPages;
    int		i, j;
    VmMachPTE	pte;
    Boolean	foundIt = FALSE;
    int		virtPage;
    Net_ScatterGather		*inPtr;
    Net_ScatterGather		*outPtr;
    int				pageOffset;
    Address			srcAddr;
    Address			newAddr;

    if (!dmaInitialized) {
	dmaInitialized = TRUE;
	DevBufferInit();
    }
    /* calculate number of pages needed */
    inPtr = inScatGathPtr;
    outPtr = outScatGathPtr;
    numPages = 0;
    for (i = 0; i < scatGathLength; i++) {
	if (inPtr->length > 0) {
	    /* beginning of first page */
	    beginAddr = (Address) (((unsigned int)(inPtr->bufAddr)) & 
		    ~VMMACH_OFFSET_MASK_INT);
	    /* beginning of last page */
	    endAddr = (Address) ((((unsigned int) inPtr->bufAddr) + 
		inPtr->length - 1) & ~VMMACH_OFFSET_MASK_INT);
	    /* 
	     * Temporarily store the number of pages in the out scatter/gather
	     * array.
	     */
	    outPtr->length =
		    (((unsigned int) endAddr) >> VMMACH_PAGE_SHIFT_INT) -
		    (((unsigned int) beginAddr) >> VMMACH_PAGE_SHIFT_INT) + 1;
	} else {
	    outPtr->length = 0;
	}
	if ((i == 0) && (outPtr->length != 1)) {
	    panic("Help! Help! I'm being repressed!\n");
	}
	numPages += outPtr->length;
	inPtr++;
	outPtr++;
    }

    /* see if request can be satisfied */
    for (i = 0; i < (VMMACH_DMA_SIZE / VMMACH_PAGE_SIZE_INT); i++) {
	if (dmaPageBitMap[i] == 1) {
	    continue;
	}
	/*
	 * Must be aligned in the cache to avoid write-backs of stale data
	 * from other references to stuff on this page.
	 */
	newAddr = (Address)(VMMACH_DMA_START_ADDR + (i * VMMACH_PAGE_SIZE_INT));
	if (((unsigned int) newAddr & (VMMACH_CACHE_SIZE - 1)) !=
		((unsigned int) beginAddr & (VMMACH_CACHE_SIZE - 1))) {
	    continue;
	}
	for (j = 1; j < numPages &&
		((i + j) < (VMMACH_DMA_SIZE / VMMACH_PAGE_SIZE_INT)); j++) {
	    if (dmaPageBitMap[i + j] == 1) {
		break;
	    }
	}
	if (j == numPages &&
		((i + j) < (VMMACH_DMA_SIZE / VMMACH_PAGE_SIZE_INT))) {
	    foundIt = TRUE;
	    break;
	}
    }
    if (!foundIt) {
	return FAILURE;
    }
    pageOffset = i;
    inPtr = inScatGathPtr;
    outPtr = outScatGathPtr;
    for (i = 0; i < scatGathLength; i++) {
	srcAddr = inPtr->bufAddr;
	numPages = outPtr->length;
	for (j = 0; j < numPages; j++) {
	    dmaPageBitMap[pageOffset + j] = 1;	/* allocate page */
	    virtPage = ((unsigned int) srcAddr) >> VMMACH_PAGE_SHIFT;
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT |
		  VirtToPhysPage(Vm_GetKernPageFrame(virtPage));
	    SET_ALL_PAGE_MAP(((pageOffset + j) * VMMACH_PAGE_SIZE_INT) +
		    VMMACH_DMA_START_ADDR, pte);
	    srcAddr += VMMACH_PAGE_SIZE;
	}
	outPtr->bufAddr = (Address) (VMMACH_DMA_START_ADDR + 
		(pageOffset * VMMACH_PAGE_SIZE_INT) + 
		(((unsigned int) srcAddr) & VMMACH_OFFSET_MASK));
	pageOffset += numPages;
	outPtr->length = inPtr->length;
	inPtr++;
	outPtr++;
    }
    return SUCCESS;
}
#endif /* sun4c */


/*
 ----------------------------------------------------------------------
 *
 * VmMach_DMAFree --
 *
 *	Free a previously allocated set of virtual pages for a routine that
 *	used them for mapping purposes.
 *	
 * Results:
 *	None.
 *
 * Side effects:
 *	The hardware page table is modified.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_DMAFree(numBytes, mapAddr)
    int		numBytes;		/* Number of bytes to map in. */
    Address	mapAddr;	/* Kernel virtual address to unmap.*/
{
    Address	beginAddr;
    Address	endAddr;
    int		numPages;
    int		i, j;
 
    MASTER_LOCK(vmMachMutexPtr);
    /* calculate number of pages to free */
						/* beginning of first page */
    beginAddr = (Address) (((unsigned int) mapAddr) & ~VMMACH_OFFSET_MASK_INT);
						/* beginning of last page */
    endAddr = (Address) ((((unsigned int) mapAddr) + numBytes - 1) &
	    ~VMMACH_OFFSET_MASK_INT);
    numPages = (((unsigned int) endAddr) >> VMMACH_PAGE_SHIFT_INT) -
	    (((unsigned int) beginAddr) >> VMMACH_PAGE_SHIFT_INT) + 1;

    i = (((unsigned int) mapAddr) >> VMMACH_PAGE_SHIFT_INT) -
	(((unsigned int) VMMACH_DMA_START_ADDR) >> VMMACH_PAGE_SHIFT_INT);
    for (j = 0; j < numPages; j++) {
	dmaPageBitMap[i + j] = 0;	/* free page */
	if (vmMachHasVACache) {
	    VmMachFlushPage(mapAddr);
	}
	VmMachSetPageMap(mapAddr, (VmMachPTE) 0);
	mapAddr += VMMACH_PAGE_SIZE_INT;
    }
    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}



/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_MapKernelIntoUser --
 *
 *      Map a portion of kernel memory into the user's heap segment.  
 *	It will only map objects on hardware segment boundaries.  This is 
 *	intended to be used to map devices such as video memory.
 *
 *	NOTE: It is assumed that the user process knows what the hell it is
 *	      doing.
 *
 * Results:
 *      Return the virtual address that it chose to map the memory at.
 *
 * Side effects:
 *      The hardware segment table for the user process's segment is modified
 *	to map in the addresses.
 *
 * ----------------------------------------------------------------------------
 */
ReturnStatus
VmMach_MapKernelIntoUser(kernelVirtAddr, numBytes, userVirtAddr,
			 realVirtAddrPtr) 
    unsigned int	kernelVirtAddr;		/* Kernel virtual address
					 	 * to map in. */
    int	numBytes;				/* Number of bytes to map. */
    unsigned int	userVirtAddr; 		/* User virtual address to
					 	 * attempt to start mapping
						 * in at. */
    unsigned int	*realVirtAddrPtr;	/* Where we were able to start
					 	 * mapping at. */
{
    Address             newUserVirtAddr;
    ReturnStatus        status;

    status = VmMach_IntMapKernelIntoUser(kernelVirtAddr, numBytes,
            userVirtAddr, &newUserVirtAddr);

    if (status != SUCCESS) {
        return status;
    }

    return Vm_CopyOut(4, (Address) &newUserVirtAddr, (Address) realVirtAddrPtr);
}


/*
 * ----------------------------------------------------------------------------
 *
 * Vm_IntMapKernelIntoUser --
 *
 *      Map a portion of kernel memory into the user's heap segment.
 *      It will only map objects on hardware segment boundaries.  This is
 *      intended to be used to map devices such as video memory.
 *
 *      This routine can be called from within the kernel since it doesn't
 *      do a Vm_CopyOut of the new user virtual address.
 *
 *      NOTE: It is assumed that the user process knows what the hell it is
 *            doing.
 *
 * Results:
 *      SUCCESS or FAILURE status.
 *      Return the virtual address that it chose to map the memory at in
 *      an out parameter.
 *
 * Side effects:
 *      The hardware segment table for the user process's segment is modified
 *      to map in the addresses.
 *
 * ----------------------------------------------------------------------------
 */
ReturnStatus
VmMach_IntMapKernelIntoUser(kernelVirtAddr, numBytes, userVirtAddr, newAddrPtr)
    unsigned int        kernelVirtAddr;         /* Kernel virtual address
                                                 * to map in. */
    int numBytes;                               /* Number of bytes to map. */
    unsigned int        userVirtAddr;           /* User virtual address to
                                                 * attempt to start mapping
                                                 * in at. */
    Address             *newAddrPtr;            /* New user address. */
{
    int                         numSegs;
    int                         firstPage;
    int                         numPages;
    Proc_ControlBlock           *procPtr;
    register    Vm_Segment      *segPtr;
    int                         hardSegNum;
    int                         i;
    unsigned int                pte;

    procPtr = Proc_GetCurrentProc();
    segPtr = procPtr->vmPtr->segPtrArray[VM_HEAP];

    numSegs = numBytes >> VMMACH_SEG_SHIFT;
    numPages = numSegs * VMMACH_SEG_SIZE / VMMACH_PAGE_SIZE;

    /*
     * Make user virtual address hardware segment aligned (round up) and
     * make sure that there is enough space to map things.
     */
    hardSegNum =
            (unsigned int) (userVirtAddr + VMMACH_SEG_SIZE - 1) >> VMMACH_SEG_SHIFT;
    userVirtAddr = hardSegNum << VMMACH_SEG_SHIFT;
    if (hardSegNum + numSegs > VMMACH_NUM_SEGS_PER_CONTEXT) {
        return(SYS_INVALID_ARG);
    }

    /*
     * Make sure will fit into the kernel's VAS.  Assume that is hardware
     * segment aligned.
     */
    hardSegNum = (unsigned int) (kernelVirtAddr) >> VMMACH_SEG_SHIFT;
    if (hardSegNum + numSegs > VMMACH_NUM_SEGS_PER_CONTEXT) {
        return(SYS_INVALID_ARG);
    }

    /*
     * Invalidate all virtual memory for the heap segment of this process
     * in the given range of virtual addresses that we are to map.  This
     * assures us that there aren't any hardware pages allocated for this
     * segment in this range of addresses.
     */
    firstPage = (unsigned int) (userVirtAddr) >> VMMACH_PAGE_SHIFT;
    (void)Vm_DeleteFromSeg(segPtr, firstPage, firstPage + numPages - 1);

    /*
     * Now go into the kernel's hardware segment table and copy the
     * segment table entries into the heap segments hardware segment table.
     */

    bcopy((Address)GetHardSegPtr(vm_SysSegPtr->machPtr, hardSegNum),
        (Address)GetHardSegPtr(segPtr->machPtr,
                (unsigned int)userVirtAddr >> VMMACH_SEG_SHIFT),
                numSegs * sizeof (VMMACH_SEG_NUM));
    for (i = 0; i < numSegs * VMMACH_NUM_PAGES_PER_SEG_INT; i++) {
        pte = VmMachGetPageMap((Address)(kernelVirtAddr +
                (i * VMMACH_PAGE_SIZE_INT)));
        pte &= ~VMMACH_KR_PROT;
        pte |= VMMACH_URW_PROT;
        VmMachSetPageMap((Address)(kernelVirtAddr + (i*VMMACH_PAGE_SIZE_INT)),
                pte);
    }

    /*
     * Make sure this process never migrates.
     */
    Proc_NeverMigrate(procPtr);

    /*
     * Reinitialize this process's context using the new segment table.
     */
    VmMach_ReinitContext(procPtr);

    *newAddrPtr = (Address) userVirtAddr;
    return SUCCESS;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_FlushPage --
 *
 *	Flush the page at the given virtual address from all caches.  We
 *	don't have to do anything on the Sun-2 and Sun-3 workstations
 *	that we have.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The given page is flushed from the caches.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMach_FlushPage(virtAddrPtr, invalidate)
    Vm_VirtAddr	*virtAddrPtr;
    Boolean	invalidate;	/* Should invalidate the pte after flushing. */
{
    Address	virtAddr;
    int		i;

    /* on sun4, ignore invalidate parameter? */
    virtAddr = (Address) (virtAddrPtr->page << VMMACH_PAGE_SHIFT);
    if (vmMachHasVACache) {
	for (i = 0; i < VMMACH_CLUSTER_SIZE; ++i) {
	    VmMachFlushPage(virtAddr + i * VMMACH_PAGE_SIZE_INT);
	}
    }
    if (invalidate) {
	SET_ALL_PAGE_MAP(virtAddr, (VmMachPTE)0);
    }
    return;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_SetProtForDbg --
 *
 *	Set the protection of the kernel pages for the debugger.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The protection is set for the given range of kernel addresses.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_SetProtForDbg(readWrite, numBytes, addr)
    Boolean	readWrite;	/* TRUE if should make pages writable, FALSE
				 * if should make read-only. */
    int		numBytes;	/* Number of bytes to change protection for. */
    Address	addr;		/* Address to start changing protection at. */
{
    register	Address		virtAddr;
    register	VmMachPTE 	pte;
    register	int		firstPage;
    register	int		lastPage;
    int		oldContext;
    int		i;

    /*
     * This should only be called with kernel text pages so we modify the
     * PTE for the address in all the contexts. Note that we must flush
     * the page from the change before changing the protections to avoid
     * write back errors.
     */
    oldContext = VmMachGetContextReg();
    for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	VmMachSetContextReg(i);
	firstPage = (unsigned)addr >> VMMACH_PAGE_SHIFT_INT;
	lastPage = ((unsigned)addr + numBytes - 1) >> VMMACH_PAGE_SHIFT_INT;
	for (; firstPage <= lastPage; firstPage++) {
	    virtAddr = (Address) (firstPage << VMMACH_PAGE_SHIFT_INT);
	    pte = VmMachGetPageMap(virtAddr);
	    pte &= ~VMMACH_PROTECTION_FIELD;
	    pte |= readWrite ? VMMACH_KRW_PROT : VMMACH_KR_PROT;
	    if (vmMachHasVACache) {
		VmMachFlushPage(virtAddr);
	    }
	    VmMachSetPageMap(virtAddr, pte);
	}
    }
    VmMachSetContextReg(oldContext);
    return;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_Cmd --
 *
 *	Machine dependent vm commands.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
ReturnStatus
VmMach_Cmd(command, arg)
    int	command;
    int arg;
{
    switch (command & 0xf) {
    case 0:
	if (vmMachHasVACache) {
	    VmMach_FlushCurrentContext();
	}
	break;
    case 1:
	if (vmMachHasVACache) {
	    VmMachFlushSegment((Address)arg);
	}
	break;
    case 2:
	if (vmMachHasVACache) {
	    VmMachFlushPage((Address)arg);
	}
	break;
    case 3:
	VmMachDoNothing((Address)arg);
	break;
    case 4:
	vmMachHasHwFlush = arg;
	break;
    default:
	return GEN_INVALID_ARG;
    }
    return SUCCESS;

}

VmMachDoNothing(arg)
int arg;
{
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_HandleSegMigration --
 *
 *	Do machine-dependent aspects of segment migration.  On the sun4's,
 *	this means flush the segment from the virtually addressed cache.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_HandleSegMigration(segPtr)
    Vm_Segment		*segPtr;	/* Pointer to segment to be migrated. */
{
    Address	virtAddr;

    if (vmMachHasVACache) {
	virtAddr = (Address) (segPtr->offset << VMMACH_PAGE_SHIFT);
	VmMachFlushSegment(virtAddr);
    }

    return;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_FlushCode --
 *
 *      Does nothing on this machine.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      None.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMach_FlushCode(procPtr, virtAddrPtr, virtPage, numBytes)
    Proc_ControlBlock   *procPtr;
    Vm_VirtAddr         *virtAddrPtr;
    unsigned            virtPage;
    int                 numBytes;
{
}


/*
 * Dummy function which will turn out to be the function that the debugger
 * prints out on a backtrace after a trap.  The debugger gets confused
 * because trap stacks originate from assembly language stacks.  I decided
 * to make a dummy procedure because it was to confusing seeing the
 * previous procedure (VmMach_MapKernelIntoUser) on every backtrace.
 */
static void
VmMachTrap()
{
}

#ifndef sun4c

/*----------------------------------------------------------------------
 *
 * Dev32BitBufferInit --
 *
 *	Initialize a range of virtual memory to allocate from out of the
 *	device memory space.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The buffer struct is initialized and the hardware page map is zeroed
 *	out in the range of addresses.
 *
 *----------------------------------------------------------------------
 */
INTERNAL static void
Dev32BitDMABufferInit()
{
    Address		virtAddr;
    unsigned char	pmeg;
    int			oldContext;
    Address	baseAddr;
    Address	endAddr;

    if ((VMMACH_32BIT_DMA_SIZE & (VMMACH_CACHE_SIZE - 1)) != 0) {
	panic(
"Dev32BitDMABufferInit: 32-bit DMA area must be a multiple of cache size.\n");
    }

    VmMachSetup32BitDVMA(); 
    /*
     * Round base up to next page boundary and end down to page boundary.
     */
    baseAddr = (Address)VMMACH_32BIT_DMA_START_ADDR;
    endAddr = (Address)(VMMACH_32BIT_DMA_START_ADDR + VMMACH_32BIT_DMA_SIZE);

    /* 
     * Set up the hardware pages tables in the range of addresses given.
     */
    oldContext = VmMachGetContextReg();
    VmMachSetContextReg(0);
    for (virtAddr = baseAddr; virtAddr < endAddr; ) {
	if (VmMachGetSegMap(virtAddr) != VMMACH_INV_PMEG) {
	    printf("Dev32BitDMABufferInit: DMA space already valid at 0x%x\n",
		   (unsigned int) virtAddr);
	}
	/* 
	 * Need to allocate a PMEG.
	 */
	pmeg = PMEGGet(vm_SysSegPtr, 
		       (int) ((unsigned)virtAddr >> VMMACH_SEG_SHIFT),
		       PMEG_DONT_ALLOC);
	if (pmeg == VMMACH_INV_PMEG) {
	    panic("Dev32BitDMABufferInit: unable to get a pmeg.\n");
	}
        VmMachSetSegMap(virtAddr, (int)pmeg);
	virtAddr += VMMACH_SEG_SIZE;
    }
    VmMachSetContextReg(oldContext);
}


static	Boolean	userdmaPageBitMap[VMMACH_32BIT_DMA_SIZE / VMMACH_PAGE_SIZE_INT];


/*
 ----------------------------------------------------------------------
 *
 * VmMach_32BitDMAAlloc --
 *
 *	Allocate a set of virtual pages to a routine for mapping purposes.
 *	
 * Results:
 *	Pointer into kernel virtual address space of where to access the
 *	memory, or NIL if the request couldn't be satisfied.
 *
 * Side effects:
 *	The hardware page table is modified.
 *
 *----------------------------------------------------------------------
 */
ENTRY Address
VmMach_32BitDMAAlloc(numBytes, srcAddr)
    int		numBytes;		/* Number of bytes to map in. */
    Address	srcAddr;	/* Kernel virtual address to start mapping in.*/
{
    Address	beginAddr;
    Address	endAddr;
    int		numPages;
    int		i, j;
    VmMachPTE	pte;
    Boolean	foundIt = FALSE;
    static initialized = FALSE;
    unsigned	oldContext;
    int		align;
 
    MASTER_LOCK(vmMachMutexPtr);
    if (!initialized) {
	initialized = TRUE;
	Dev32BitDMABufferInit();
    }

    /* calculate number of pages needed */
						/* beginning of first page */
    beginAddr = (Address) (((unsigned int)(srcAddr)) & ~VMMACH_OFFSET_MASK_INT);
						/* beginning of last page */
    endAddr = (Address) ((((unsigned int) srcAddr) + numBytes - 1) &
	    ~VMMACH_OFFSET_MASK_INT);
    numPages = (((unsigned int) endAddr) >> VMMACH_PAGE_SHIFT_INT) -
	    (((unsigned int) beginAddr) >> VMMACH_PAGE_SHIFT_INT) + 1;

    /* set first addr to first entry that is also cache aligned */
    align = (unsigned int) beginAddr & (VMMACH_CACHE_SIZE - 1);
    align -= (unsigned int) VMMACH_32BIT_DMA_START_ADDR &
	    (VMMACH_CACHE_SIZE - 1);
    if ((int) align < 0) {
	align += VMMACH_CACHE_SIZE;
    }
    /* see if request can be satisfied, incrementing by cache size in loop */
    for (i = align / VMMACH_PAGE_SIZE_INT;
	    i < (VMMACH_32BIT_DMA_SIZE / VMMACH_PAGE_SIZE_INT);
	    i += (VMMACH_CACHE_SIZE / VMMACH_PAGE_SIZE_INT)) {
	if (userdmaPageBitMap[i] == 1) {
	    continue;
	}
	for (j = 1; (j < numPages) &&
		((i + j) < (VMMACH_32BIT_DMA_SIZE / VMMACH_PAGE_SIZE_INT));
		j++) {
	    if (userdmaPageBitMap[i + j] == 1) {
		break;
	    }
	}
	if ((j == numPages) &&
		((i + j) < (VMMACH_32BIT_DMA_SIZE / VMMACH_PAGE_SIZE_INT))) {
	    foundIt = TRUE;
	    break;
	}
    }

    if (!foundIt) {
	MASTER_UNLOCK(vmMachMutexPtr);
	panic(
    "VmMach_32BitDMAAlloc: unable to satisfy request for %d bytes at 0x%x\n",
		numBytes, srcAddr);
#ifdef NOTDEF
	return (Address) NIL;
#endif NOTDEF
    }
    oldContext = VmMachGetContextReg();
    VmMachSetContextReg(0);
    for (j = 0; j < numPages; j++) {
	userdmaPageBitMap[i + j] = 1;	/* allocate page */
	pte = VmMachGetPageMap(srcAddr);
	pte = (pte & ~VMMACH_PROTECTION_FIELD) | VMMACH_RESIDENT_BIT | 
		    VMMACH_URW_PROT;

	SET_ALL_PAGE_MAP(((i + j) * VMMACH_PAGE_SIZE_INT) +
		VMMACH_32BIT_DMA_START_ADDR, pte);
	srcAddr += VMMACH_PAGE_SIZE;
    }
    VmMachSetContextReg((int)oldContext);
    beginAddr = (Address) (VMMACH_32BIT_DMA_START_ADDR +
	    (i * VMMACH_PAGE_SIZE_INT) +
	    (((unsigned int) srcAddr) & VMMACH_OFFSET_MASK));

    /* set high VME addr bit */
    beginAddr = (Address)((unsigned) beginAddr | VMMACH_VME_ADDR_BIT);
    MASTER_UNLOCK(vmMachMutexPtr);
    return (Address) beginAddr;
}


/*
 ----------------------------------------------------------------------
 *
 * VmMach_32BitDMAFree --
 *
 *	Free a previously allocated set of virtual pages for a routine that
 *	used them for mapping purposes.
 *	
 * Results:
 *	None.
 *
 * Side effects:
 *	The hardware page table is modified.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_32BitDMAFree(numBytes, mapAddr)
    int		numBytes;		/* Number of bytes to map in. */
    Address	mapAddr;	/* Kernel virtual address to unmap.*/
{
    Address	beginAddr;
    Address	endAddr;
    int		numPages;
    int		i, j;
    int         oldContext;

    MASTER_LOCK(vmMachMutexPtr);
    /* calculate number of pages to free */
    /* Clear the VME high bit from the address */
    mapAddr = (Address) ((unsigned)mapAddr & ~VMMACH_VME_ADDR_BIT);
						/* beginning of first page */
    beginAddr = (Address) (((unsigned int) mapAddr) & ~VMMACH_OFFSET_MASK_INT);
						/* beginning of last page */
    endAddr = (Address) ((((unsigned int) mapAddr) + numBytes - 1) &
	    ~VMMACH_OFFSET_MASK_INT);
    numPages = (((unsigned int) endAddr) >> VMMACH_PAGE_SHIFT_INT) -
	    (((unsigned int) beginAddr) >> VMMACH_PAGE_SHIFT_INT) + 1;

    i = (((unsigned int) mapAddr) >> VMMACH_PAGE_SHIFT_INT) -
	(((unsigned int) VMMACH_32BIT_DMA_START_ADDR) >> VMMACH_PAGE_SHIFT_INT);
    oldContext = VmMachGetContextReg();
    VmMachSetContextReg(0);
    for (j = 0; j < numPages; j++) {
	userdmaPageBitMap[i + j] = 0;	/* free page */
	if (vmMachHasVACache) {
	    VmMachFlushPage(mapAddr);
	}
	SET_ALL_PAGE_MAP(mapAddr, (VmMachPTE) 0);
	mapAddr = (Address)((unsigned int) mapAddr + VMMACH_PAGE_SIZE_INT);
    }
    VmMachSetContextReg(oldContext);
    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}

#endif /* not sun4c */


#define CHECK(x) (((x)<0||(x)>=VMMACH_SHARED_NUM_BLOCKS)?\
	(panic("Alloc out of bounds"),0):0)
#define ALLOC(x,s)	(CHECK(x),sharedData->allocVector[(x)]=s)
#define FREE(x)		(CHECK(x),sharedData->allocVector[(x)]=0)
#define SIZE(x)		(CHECK(x),sharedData->allocVector[(x)])
#define ISFREE(x)	(CHECK(x),sharedData->allocVector[(x)]==0)



/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_Alloc --
 *
 *      Allocates a region of shared memory;
 *
 * Results:
 *      SUCCESS if the region can be allocated.
 *	The starting address is returned in addr.
 *
 * Side effects:
 *      The allocation vector is updated.
 *
 * ----------------------------------------------------------------------------
 */
static ReturnStatus
VmMach_Alloc(sharedData, regionSize, addr)
    VmMach_SharedData	*sharedData;	/* Pointer to shared memory info.  */
    int			regionSize;	/* Size of region to allocate. */
    Address		*addr;		/* Address of region. */
{
    int numBlocks = (regionSize+VMMACH_SHARED_BLOCK_SIZE-1) /
	    VMMACH_SHARED_BLOCK_SIZE;
    int i, blockCount, firstBlock;

    if (sharedData->allocVector == (int *)NULL || sharedData->allocVector ==
	    (int *)NIL) {
	dprintf("VmMach_Alloc: allocVector uninitialized!\n");
    }

    /*
     * Loop through the alloc vector until we find numBlocks free blocks
     * consecutively.
     */
    blockCount = 0;
    for (i=sharedData->allocFirstFree;
	    i<=VMMACH_SHARED_NUM_BLOCKS-1 && blockCount<numBlocks;i++) {
	if (ISFREE(i)) {
	    blockCount++;
	} else {
	    blockCount = 0;
	    if (i==sharedData->allocFirstFree) {
		sharedData->allocFirstFree++;
	    }
	}
    }
    if (blockCount < numBlocks) {
	dprintf("VmMach_Alloc: got %d blocks of %d of %d total\n",
		blockCount,numBlocks,VMMACH_SHARED_NUM_BLOCKS);
	return VM_NO_SEGMENTS;
    }
    firstBlock = i-blockCount;
    if (firstBlock == sharedData->allocFirstFree) {
	sharedData->allocFirstFree += blockCount;
    }
    *addr = (Address)(firstBlock*VMMACH_SHARED_BLOCK_SIZE +
	    VMMACH_SHARED_START_ADDR);
    for (i = firstBlock; i<firstBlock+numBlocks; i++) {
	ALLOC(i,numBlocks);
    }
    dprintf("VmMach_Alloc: got %d blocks at %d (%x)\n",
	    numBlocks,firstBlock,*addr);
    return SUCCESS;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_Unalloc --
 *
 *      Frees a region of shared address space.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The allocation vector is updated.
 *
 * ----------------------------------------------------------------------------
 */

static void
VmMach_Unalloc(sharedData, addr)
    VmMach_SharedData	*sharedData;	/* Pointer to shared memory info. */
    Address	addr;		/* Address of region. */
{
    int firstBlock = ((int)addr-VMMACH_SHARED_START_ADDR) /
	    VMMACH_SHARED_BLOCK_SIZE;
    int numBlocks;
    int i;

    if (firstBlock<0 || firstBlock>=VMMACH_SHARED_NUM_BLOCKS) {
	if (debugVmStubs) {
	    printf("VmMach_Unalloc: addr %x out of range\n", addr);
	}
	return;
    }

    numBlocks = SIZE(firstBlock);

    dprintf("VmMach_Unalloc: freeing %d blocks at %x\n",numBlocks,addr);
    if (firstBlock < sharedData->allocFirstFree) {
	sharedData->allocFirstFree = firstBlock;
    }
    for (i=0;i<numBlocks;i++) {
	if (ISFREE(i+firstBlock)) {
	    if (debugVmStubs) {
		printf("Freeing free shared address %d %d %x\n",i,i+firstBlock,
			(int)addr);
	    }
	    return;
	}
	FREE(i+firstBlock);
    }
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SharedStartAddr --
 *
 *      Determine the starting address for a shared segment.
 *
 * Results:
 *      Returns the proper start address for the segment.
 *
 * Side effects:
 *      Allocates part of the shared address space.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ReturnStatus
VmMach_SharedStartAddr(procPtr,size,reqAddr, fixed)
    Proc_ControlBlock	*procPtr;
    int             size;           /* Length of shared segment. */
    Address         *reqAddr;        /* Requested start address. */
    int		    fixed;	    /* 1 if fixed address requested. */
{
    int numBlocks = (size+VMMACH_SHARED_BLOCK_SIZE-1) /
	    VMMACH_SHARED_BLOCK_SIZE;
    int firstBlock = (((int)*reqAddr)-VMMACH_SHARED_START_ADDR) /
	    VMMACH_SHARED_BLOCK_SIZE;
    int i;
    VmMach_SharedData	*sharedData = &procPtr->vmPtr->machPtr->sharedData;

    if (fixed==0) {
	return VmMach_Alloc(sharedData, size, reqAddr);
    } else {
	for (i = firstBlock; i<firstBlock+numBlocks; i++) {
	    if (i>0) {
		ALLOC(i,numBlocks);
	    }
	}
	return SUCCESS;
    }
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SharedProcStart --
 *
 *      Perform machine dependent initialization of shared memory
 *	for this process.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The storage allocation structures are initialized.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_SharedProcStart(procPtr)
    Proc_ControlBlock	*procPtr;
{
    VmMach_SharedData	*sharedData = &procPtr->vmPtr->machPtr->sharedData;
    dprintf("VmMach_SharedProcStart: initializing proc's allocVector\n");
    if (sharedData->allocVector != (int *)NIL) {
	panic("VmMach_SharedProcStart: allocVector not NIL\n");
    }
    sharedData->allocVector =
	    (int *)malloc(VMMACH_SHARED_NUM_BLOCKS*sizeof(int));
    if (debugVmStubs) {
	printf("Initializing allocVector for %x to %x\n", procPtr->processID,
		sharedData->allocVector);
    }
    sharedData->allocFirstFree = 0;
    bzero((Address) sharedData->allocVector, VMMACH_SHARED_NUM_BLOCKS*
	    sizeof(int));
    procPtr->vmPtr->sharedStart = (Address) 0x00000000;
    procPtr->vmPtr->sharedEnd = (Address) 0xffff0000;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SharedSegFinish --
 *
 *      Perform machine dependent cleanup of shared memory
 *	for this segment.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The storage allocation structures are freed.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_SharedSegFinish(procPtr,addr)
    Proc_ControlBlock	*procPtr;
    Address		addr;
{
    VmMach_Unalloc(&procPtr->vmPtr->machPtr->sharedData,addr);
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SharedProcFinish --
 *
 *      Perform machine dependent cleanup of shared memory
 *	for this process.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The storage allocation structures are freed.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_SharedProcFinish(procPtr)
    Proc_ControlBlock	*procPtr;
{
    dprintf("VmMach_SharedProcFinish: freeing process's allocVector\n");
    if (debugVmStubs) {
	printf("VmMach_SharedProcFinish: freeing process's allocVector %x\n",
		procPtr->vmPtr->machPtr->sharedData.allocVector);
    }
    free((Address)procPtr->vmPtr->machPtr->sharedData.allocVector);
    procPtr->vmPtr->machPtr->sharedData.allocVector = (int *)NIL;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_CopySharedMem --
 *
 *      Copies machine-dependent shared memory data structures to handle
 *      a fork.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The new process gets a copy of the shared memory structures.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_CopySharedMem(parentProcPtr, childProcPtr)
    Proc_ControlBlock   *parentProcPtr; /* Parent process. */
    Proc_ControlBlock   *childProcPtr;  /* Child process. */
{
    VmMach_SharedData   *childSharedData =
            &childProcPtr->vmPtr->machPtr->sharedData;
    VmMach_SharedData   *parentSharedData =
            &parentProcPtr->vmPtr->machPtr->sharedData;

    VmMach_SharedProcStart(childProcPtr);

    bcopy((Address)parentSharedData->allocVector,
	    (Address)childSharedData->allocVector,
            VMMACH_SHARED_NUM_BLOCKS*sizeof(int));
    childSharedData->allocFirstFree = parentSharedData->allocFirstFree;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_LockCachePage --
 *
 *      Perform machine dependent locking of a kernel resident file cache
 *	page.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_LockCachePage(kernelAddress)
    Address	kernelAddress;	/* Address on page to lock. */
{
    Vm_VirtAddr	virtAddr;
    register  VMMACH_SEG_NUM	*segTablePtr, pmeg;
    register  int		hardSeg;
    Vm_PTE    *ptePtr;
    VmMachPTE		hardPTE;
    /*
     * Ignore pages not in cache pmeg range.
     */
    if (!IN_FILE_CACHE_SEG(kernelAddress)) {
	return;
    }

    MASTER_LOCK(vmMachMutexPtr);

    pmeg = VmMachGetSegMap(kernelAddress);
    if (pmeg == VMMACH_INV_PMEG) {
	int	oldContext, i;
	unsigned int a;
	/*
	 *  If not a valid PMEG install a new pmeg and load its mapping. 
	 */
	virtAddr.segPtr = vm_SysSegPtr;
	virtAddr.page = ((unsigned int) kernelAddress) >> VMMACH_PAGE_SHIFT;
	virtAddr.offset = 0;
	virtAddr.flags = 0;
	virtAddr.sharedPtr = (Vm_SegProcList *) NIL;
    
	hardSeg = PageToOffSeg(virtAddr.page, (&virtAddr));
	segTablePtr = (VMMACH_SEG_NUM *) 
			    GetHardSegPtr(vm_SysSegPtr->machPtr, hardSeg);
	if (*segTablePtr != VMMACH_INV_PMEG) {
	    panic("VmMach_LockCachePage: Bad segTable entry.\n");
	}
	*segTablePtr = pmeg = PMEGGet(vm_SysSegPtr, hardSeg, 0);
	/*
	 * Have to propagate the PMEG to all contexts.
	 */
	oldContext = VmMachGetContextReg();
	for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	    VmMachSetContextReg(i);
	    VmMachSetSegMap(kernelAddress, pmeg);
	}
	VmMachSetContextReg(oldContext);
	/*
	 * Reload the entire PMEG.
	 */
	a = (hardSeg << VMMACH_SEG_SHIFT);
	for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG; i++ ) { 
	    ptePtr = VmGetPTEPtr(vm_SysSegPtr, (a >> VMMACH_PAGE_SHIFT));
	    if ((*ptePtr & VM_PHYS_RES_BIT)) {
		hardPTE = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | 
				VirtToPhysPage(Vm_GetPageFrame(*ptePtr));
		SET_ALL_PAGE_MAP(a, hardPTE);
		if (pmeg==VMMACH_INV_PMEG) {
		    panic("Invalid pmeg\n");
		}
		pmegArray[pmeg].pageCount++;
	     }
	     a += VMMACH_PAGE_SIZE;
	}
    }
    pmegArray[pmeg].lockCount++;

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_UnlockCachePage --
 *
 *      Perform machine dependent unlocking of a kernel resident page.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_UnlockCachePage(kernelAddress)
    Address	kernelAddress;	/* Address on page to unlock. */
{
    register  VMMACH_SEG_NUM	pmeg;

    if (!IN_FILE_CACHE_SEG(kernelAddress)) {
	return;
    }

    MASTER_LOCK(vmMachMutexPtr);

    pmeg = VmMachGetSegMap(kernelAddress);

    pmegArray[pmeg].lockCount--;
    if (pmegArray[pmeg].lockCount < 0) {
	panic("VmMach_UnlockCachePage lockCount < 0\n");
    }

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_FlushCurrentContext --
 *
 *	Flush the current context from the cache.
 *
 *	void VmMach_FlushCurrentContext()
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	All data cached from the current context is flushed from the cache.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_FlushCurrentContext()
{
    if (vmMachHasVACache) {
	VmMachFlushCurrentContext();
    }
}

/*
 * ----------------------------------------------------------------------
 *
 * Vm_TouchPages --
 *
 *	Touch the range of pages.
 *
 *	ReturnStatus
 *	Vm_TouchPages(firstPage, numPages)
 *	    int	firstPage;	First page to touch.
 *	    int	numPages;	Number of pages to touch.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 * ----------------------------------------------------------------------
 */
ReturnStatus
Vm_TouchPages(firstPage, numPages)
    int firstPage, numPages;
{
    return VmMachTouchPages(firstPage * VMMACH_CLUSTER_SIZE,
	numPages * VMMACH_CLUSTER_SIZE);
}
@


9.43
log
@Changes for transparent server recovery.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/Cvsroot/kernel/vm/sun4.md/vmSun.c,v 9.42 92/07/30 17:23:10 mgbaker Exp $ SPRITE (Berkeley)";
d4009 2
d4019 1
d4031 1
d4037 1
d4055 2
d4086 1
d4094 1
d4102 1
d4114 1
d4122 1
d4147 1
d4175 1
@


9.42
log
@Fixed VME device mapping bug.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/Cvsroot/kernel/vm/sun4.md/vmSun.c,v 9.41 92/07/28 14:59:04 mgbaker Exp $ SPRITE (Berkeley)";
d41 1
d887 5
a891 1
	if (virtAddr >= (Address)MACH_CODE_START && 
@


9.41
log
@New routine for mapping in devices with more than one page of memory.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/Cvsroot/kernel/vm/sun4.md/vmSun.c,v 9.40 91/10/18 01:10:45 dlong Exp $ SPRITE (Berkeley)";
d3998 1
d4038 1
d4087 1
d4097 3
@


9.40
log
@sun4c sun4c2 merge
Fixes so that sun4c can use 8K VM pages
misc. cleanup
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.36 91/08/20 12:22:58 mgbaker Exp $ SPRITE (Berkeley)";
d3961 194
d4802 1
@


9.39
log
@Changed pmeg table info so it resides in the segment structure instead
of a dynamically allocated list.  This prevents deadlocks due to
mallocs at bad times.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.38 91/09/24 23:18:01 shirriff Exp Locker: shirriff $ SPRITE (Berkeley)";
d86 1
a88 4
#ifdef NOTDEF
static void MapColorMapAndIdRomAddr _ARGS_((void));
#endif /* NOTDEF */
#endif /* sun4c */
a101 6
/*
 * FlushWholeCache seems to be dead code.
 */
#if 0
static void FlushWholeCache _ARGS_((void));
#endif
d207 1
a210 1
#if (VMMACH_CLUSTER_SIZE == 1) 
d214 6
d227 6
d265 4
a268 1
static	PMEG   		pmegArray[VMMACH_NUM_PMEGS];
d274 6
d308 4
d313 1
d365 3
d370 1
d388 1
a388 1
    unsigned int	endVirPfNum;	/* Ending virtual page frame nmber
d408 7
d463 43
a505 10
    for (memPtr = *(romVectorPtr->availMemory); memPtr != (Mach_MemList *) 0;
	    memPtr = memPtr->next) {
	if (memPtr->size != 0) {
	    numFrames = memPtr->size / VMMACH_PAGE_SIZE;
	    lastMemBoard->startVirPfNum = nextVframeNum;
	    lastMemBoard->endVirPfNum = nextVframeNum + numFrames;
	    nextVframeNum += numFrames;
	    lastMemBoard->physStartAddr = memPtr->address >> VMMACH_PAGE_SHIFT;
	    lastMemBoard->physEndAddr = lastMemBoard->physStartAddr + numFrames;
	    lastMemBoard++;
d511 41
a551 1
#endif
d553 1
a553 8
    /* 
     * We used to map pages for vmMachKernMemSize of memory.  But now that
     * the kernel size is bigger, there may not be enough pmegs for that.
     * So for now we just map a number of pmegs that is safe and gives us
     * at least enough for the code we run on for now.  This amount is
     * 32 megabytes of pmegs.
     */
    kernPages = (32 * 1024 * 1024) / VMMACH_PAGE_SIZE_INT;
d561 6
a566 2
	 i < kernPages;
	 i++, virtAddr += VMMACH_PAGE_SIZE_INT) {
d568 12
a579 7
        VmMachSetPageMap(virtAddr, 
	    (VmMachPTE)(VMMACH_KRW_PROT | VMMACH_RESIDENT_BIT |
		    VMMACH_DONT_CACHE_BIT | i));
#else
        VmMachSetPageMap(virtAddr, 
	    (VmMachPTE)(VMMACH_KRW_PROT | VMMACH_RESIDENT_BIT | i));
#endif /* sun4 */
d585 9
d607 1
a607 1
    *numKernPagesPtr = GetNumPages();
d640 16
a655 3
    for (memPtr = *(romVectorPtr->availMemory); memPtr != (Mach_MemList *) 0;
	    memPtr = memPtr->next) {
	memory += memPtr->size;
d657 1
a657 1
    return (memory / VMMACH_PAGE_SIZE);
d689 3
a691 2
	if (pfNum < mb->endVirPfNum) {
	    break;
d694 2
a695 1
    return (mb->physStartAddr + pfNum - mb->startVirPfNum);
d724 2
a725 1
	    break;
d728 2
a729 1
    return (pfNum - mb->physStartAddr + mb->startVirPfNum);
a732 1
#ifdef sun4c
d756 6
a764 1
#endif /* sun4c */
a823 4
#ifndef sun4c
    register 	VMMACH_SEG_NUM	*segTablePtr;
#endif sun4c
    register 	VmMachPTE	pte;
a828 1
#ifdef sun4c
a829 1
#endif sun4c
d867 19
a885 2
	 i < firstFreePage;
	 virtAddr += VMMACH_PAGE_SIZE, i++) {
d888 1
a888 2
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KR_PROT | 
			  VirtToPhysPage(i) * VMMACH_CLUSTER_SIZE;
d890 1
a890 2
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | 
			  VirtToPhysPage(i) * VMMACH_CLUSTER_SIZE;
d896 1
a896 1
        }
d903 1
a903 1
    SET_ALL_PAGE_MAP((Address)mach_StackBottom, (VmMachPTE)VirtToPhysPage(0));
d910 1
a910 1
	SET_ALL_PAGE_MAP(virtAddr, (VmMachPTE)VirtToPhysPage(0));
d928 1
a928 1
#ifndef sun4c
d952 2
a953 2
	    if (virtAddr >= (Address) VMMACH_BOTTOM_OF_HOLE && 
		    virtAddr <= (Address) VMMACH_TOP_OF_HOLE) {
d956 1
d962 1
d964 2
d985 1
d989 3
a991 1
    VmMachClearCacheTags();
a995 14
#ifdef NOTDEF
    /*
     * Initialize map of invalid pmegs for later copying.
     */
    for (i = 0; i < VMMACH_NUM_SEGS_PER_CONTEXT; i++) {
	copyMap[i] = VMMACH_INV_PMEG;
    }
#endif NOTDEF
#ifdef NOTDEF
/* This is broken for now */
#ifdef sun4c
    MapColorMapAndIdRomAddr();
#endif sun4c
#endif NOTDEF
d1073 1
d1075 1
a1081 6
	    Address	addr;

#ifndef sun4c
	    VmMachSetUserContext(j);
#endif
	    addr = (Address) (i << VMMACH_SEG_SHIFT);
a1082 3
#ifndef sun4c
	    VmMachSetSegMap(addr, VMMACH_INV_PMEG);
#else
a1084 1
#endif
a1086 1
    VmMachSetUserContext(VMMACH_KERN_CONTEXT);
a1169 10
#ifdef PRINT_ZAP
		    int z;
		    /* 
		     * We didn't find any valid mappings in the PMEG or the PMEG
		     * is in DMA space so delete it.
		     */
		    printf("Zapping segment at virtAddr %x\n", virtAddr);
		    for (z = 0; z < 100000; z++) {
		    }
#endif
a1206 93
#ifdef NOTDEF
/* This is broken for now */
#ifdef sun4c
static char	colorArray[3 * 0x1000];
#ifdef ID
static char	idArray[2 * 0x1000];
#endif ID

/*
 * ----------------------------------------------------------------------------
 *
 * MapColorMapAndIdRomAddr --
 *
 *      Get the physical address of the color map for a sparc station and map
 *	it to a virtual address.  Also map the id words for the video
 *	subsystem.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Page maps are altered.  Several pages of physical memory are wasted.
 *
 * ----------------------------------------------------------------------------
 */
static void
MapColorMapAndIdRomAddr()
{
    VmMachPTE		colorPte;
    VmMachPTE		idPte;
    unsigned int	otherbits;
    Address		colorVirtAddr;
    Address		idVirtAddr;

    colorPte = VmMachGetPageMap(DEV_FRAME_BUF_ADDR);
    otherbits = colorPte &(~VMMACH_PAGE_FRAME_FIELD);
    colorPte &= VMMACH_PAGE_FRAME_FIELD;
    if ((colorPte & 0xfff) != 0x800) {
	panic("MapColorMapAndIdRomAddr: frame buffer addr is weird: 0x%x\n",
		colorPte);
    }
    /* Change last bits to color map offset and id rom offset */
    colorPte &= 0xfffff000;
#ifdef ID
    idPte = colorPte;
#endif
    colorPte |= 0x400;
    colorPte |= otherbits;
#ifdef ID
    idPte |= otherbits;		/* 0 offset */
#endif

    /* allocate 2 pages on a page boundary */
#ifdef NOTDEF
    colorVirtAddr = (Address) malloc(3 * 0x1000);
#else
    colorVirtAddr = colorArray;
#endif
    ((unsigned int)colorVirtAddr) &= 0xfffff000;

    /* and another 2 pages */
#ifdef NOTDEF
    idVirtAddr = (Address) malloc(3 * 0x1000);
#else NOTDEF
#ifdef ID
    idVirtAddr = idArray;
#endif ID
#endif NOTDEF
#ifdef ID
    ((unsigned int)idVirtAddr) &= 0xfffff000;
#endif

    /* This wastes 3 pages of physical memory.  That's a shame. */
    VmMachSetPageMap(colorVirtAddr, colorPte);
    VmMachSetPageMap(colorVirtAddr + 0x1000, colorPte + 1);

#ifdef ID
    /* This wastes 3 pages of physical memory again. */
    VmMachSetPageMap(idVirtAddr, idPte);
    VmMachSetPageMap(idVirtAddr + 0x1000, idPte + 1);
    printf("colorPte 0x%x, idPte 0x%x, colorVA 0x%x, idVA 0x%x\n",
	colorPte, idPte, colorVirtAddr, idVirtAddr);
#else
    printf("colorPte 0x%x, colorVA 0x%x\n",
	colorPte, colorVirtAddr);
#endif

    return;
}
#endif sun4c
#endif NOTDEF


d1288 2
a1289 2
            (segPtr->offset%VMMACH_NUM_PAGES_PER_SEG)) /
            VMMACH_NUM_PAGES_PER_SEG;
a1387 1

d1525 1
a1525 1
    struct VmMach_PMEGseg		*curSeg, *nextSeg;
d1566 3
a1568 1
		    VmMachFlushSegment(virtAddr);
d1591 3
a1593 1
				VmMachFlushSegment(virtAddr);
d1603 3
a1605 1
			    VmMachFlushSegment((Address)VMMACH_MAP_SEG_ADDR);
d1678 1
a1678 1
    for (i = 0; i < (VMMACH_SEG_SIZE / VMMACH_PAGE_SIZE_INT); i++) {
d1711 6
a1716 4
    for (i = 0; i < (VMMACH_SEG_SIZE / VMMACH_PAGE_SIZE_INT); i++) {
	pte = VmMachGetPageMap(virtAddr + (i * VMMACH_PAGE_SIZE_INT));
	if (pte & VMMACH_RESIDENT_BIT) {
	    VmMachFlushPage(virtAddr + (i * VMMACH_PAGE_SIZE_INT));
d1943 1
a1943 1
	if (stolenContext) {
d2100 1
a2132 5
#ifdef sun4
#ifdef NOTDEF
    pte |= VMMACH_DONT_CACHE_BIT;
#endif
#endif /* sun4 */
d2134 1
a2134 1
    VmMachSetPageMap(virtAddr, pte);
d2172 1
d2193 1
d2240 1
d2440 2
a2441 2
         scatGathLength > 0;
         scatGathLength--, inScatGathPtr++, outScatGathPtr++) {
d2452 2
a2453 2
        outScatGathPtr->bufAddr =
            mapAddr + ((unsigned)inScatGathPtr->bufAddr & VMMACH_OFFSET_MASK);
d2483 3
a2485 1
	    VmMachFlushPage(mapAddr);
d2494 3
a2496 1
		VmMachFlushPage(mapAddr);
d2508 4
a2511 2
	    VmMachFlushPage(inScatGathPtr->bufAddr);
	    VmMachFlushPage(mapAddr);
d2521 4
a2524 2
		VmMachFlushPage(endAddr);
		VmMachFlushPage(mapAddr);
a2594 138
/*
 * FlushWholeCache seems to be dead code.
 */
#if 0
/*
 * We use this array to flush the cache by touching entries at the correct
 * offsets to clear the corresponding parts of the direct-mapped cache.
 */
volatile char	cacheFlusherArray[VMMACH_NUM_CACHE_LINES *
	VMMACH_CACHE_LINE_SIZE * 2];


/*
 *----------------------------------------------------------------------
 *
 * FlushWholeCache --
 *
 *	Flush the whole cache.  It is important that the compiler not
 *	optimize out this loop!
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Cache flushed.
 *
 *----------------------------------------------------------------------
 */
static void
FlushWholeCache()
{
    int	i;
    char junk;
    register volatile char *cacheFlusherArrayPtr;

    cacheFlusherArrayPtr = cacheFlusherArray;
    for (i = 0; i < VMMACH_CACHE_SIZE; i += VMMACH_CACHE_LINE_SIZE) {
	junk = cacheFlusherArrayPtr[i];
    }
    return;
}
#endif

#if 0 /* dead code shirriff 9/90 */
char	bigBuf[VMMACH_PAGE_SIZE_INT * 2];
ReturnStatus
VmMachTestCacheFlush()
{
    unsigned int	pte1;
    unsigned int	pte2;
    unsigned int	savePte1;
    unsigned int	savePte2;
    Address	virtAddr1;
    Address	virtAddr2;
    unsigned	int	saveValue11, saveValue12, saveValue21, saveValue22;
    unsigned	int	saveValue3;
    unsigned	int	endSave3;
    unsigned	int	endSave11, endSave12, endSave21, endSave22;


    /* first addr at first page */
    virtAddr1 = (Address) bigBuf;
    /* first addr's pte */
    pte1 = VmMachGetPageMap(virtAddr1);
    pte2 = VmMachGetPageMap(virtAddr1 + 254);

    /* second addr at second page */
    virtAddr2 = (Address) (bigBuf + VMMACH_PAGE_SIZE_INT);
    /* save second addr's pte */
    savePte1 = VmMachGetPageMap(virtAddr2);
    savePte2 = VmMachGetPageMap(virtAddr2 + 254);

    /* flush second page's virtAddrs out of cache */
    VmMachFlushPage(virtAddr2);
    VmMachFlushPage(virtAddr2 + 254);
    /* give second addr the same pte as first */
    VmMachSetPageMap(virtAddr2, pte1);
    VmMachSetPageMap(virtAddr2 + 254, pte2);

    /* get current virtAddr1 value */
    saveValue11 = *virtAddr1;
    endSave11 = *(virtAddr1 + 254);

    /* get current virtAddr2 value - after flush, so really from memory */
    saveValue21 = *virtAddr2;
    endSave21 = *(virtAddr2 + 254);

    /* flush virtAddr2 from memory again */
    VmMachFlushPage(virtAddr2);
    VmMachFlushPage(virtAddr2 + 254);

    /* is something weird?  saveValue12 should equal saveValue11 */
    saveValue12 = *virtAddr1;
    endSave12 = *(virtAddr1 + 254);

    /* give virtAddr1 new value */
    *virtAddr1 = ~saveValue11;
    *(virtAddr1 + 254) = ~endSave11;

    /* should only be in cache */
    saveValue3 = *virtAddr2;
    endSave3 = *(virtAddr2 + 254);
    if (saveValue3 == ~saveValue11) {
	return FAILURE;
    }
    if (endSave3 == ~endSave11) {
	return FAILURE;
    }
    /* get rid of virtAddr2 again */
    VmMachFlushPage(virtAddr2);
    VmMachFlushPage(virtAddr2 + 254);

    /* Now try icky range flush */
    VmMach_FlushByteRange(virtAddr1, 254);

    /* Did it get to memory? */
    saveValue22 = *virtAddr2;
    endSave22 = *(virtAddr2 + 254);

    /* restore pte of second addr */
    VmMachSetPageMap(virtAddr2, savePte1);
    VmMachSetPageMap(virtAddr2 + 254, savePte2);

    if (saveValue22 != ~saveValue11) {
	return FAILURE;
    }
    if (endSave22 != ~endSave11) {
	return FAILURE;
    }
#ifdef lint
    endSave21 = endSave21;
    endSave12 = endSave12;
    saveValue21 = saveValue21;
    saveValue12 = saveValue12;
#endif
    return SUCCESS;
}
#endif /* dead code */
d2698 3
a2700 1
	if (fromProcPtr->vmPtr->machPtr->contextPtr != (VmMach_Context *)NIL) {
d2729 3
a2731 1
	VmMachFlushSegment((Address) VMMACH_MAP_SEG_ADDR);
d2843 3
a2845 1
	if (toProcPtr->vmPtr->machPtr->contextPtr != (VmMach_Context *)NIL) {
d2875 3
a2877 1
	VmMachFlushSegment((Address) VMMACH_MAP_SEG_ADDR);
d2971 8
a2978 5
		    /* Flush this segment in all contexts */
		    oldContext =  VmMachGetContextReg();
		    for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
			VmMachSetContextReg(i);
			VmMachFlushSegment(virtAddr);
a2979 1
		    VmMachSetContextReg(oldContext);
d3089 8
a3096 5
	    /* Flush this page in all contexts */
	    oldContext =  VmMachGetContextReg();
	    for (j = 0; j < VMMACH_NUM_CONTEXTS; j++) {
		VmMachSetContextReg(j);
		VmMachFlushPage(testVirtAddr);
a3097 1
	    VmMachSetContextReg(oldContext);
d3373 1
a3373 1
    struct	VmMach_PMEGseg		*pmegSegPtr;
d3563 1
a3563 1
	for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG_INT; i++ ) { 
d3571 1
a3571 1
	     a += VMMACH_PAGE_SIZE_INT;
d3647 1
a3647 1
    if (!segDeletion) {
d3665 1
a3665 1
                            VmMachFlushPage(testVirtAddr);
d3675 1
a3675 1
	    VmMachFlushPage(testVirtAddr);
d3880 2
a3881 1
    pageFrame = (unsigned)devPhysAddr >> VMMACH_PAGE_SHIFT_INT;
d3921 1
a3921 1
		return(virtAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK));
d3936 1
a3936 1
	return(freeVirtAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK));
d3955 1
a3955 1
	return(freePMEGAddr + ((int)devPhysAddr & VMMACH_OFFSET_MASK));
d4107 1
a4107 1
	SET_ALL_PAGE_MAP(((i + j) * VMMACH_PAGE_SIZE_INT) +
d4109 1
a4109 1
	srcAddr += VMMACH_PAGE_SIZE;
d4112 1
a4112 1
	    (((unsigned int) srcAddr) & VMMACH_OFFSET_MASK));
d4135 1
d4243 1
d4286 4
a4289 2
	VmMachFlushPage(mapAddr);
	SET_ALL_PAGE_MAP(mapAddr, (VmMachPTE) 0);
a4295 39
#if 0 /* dead code shirriff 9/90 */

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_GetDevicePage --
 *
 *      Allocate and validate a page at the given virtual address.  It is
 *	assumed that this page does not fall into the range of virtual 
 *	addresses used to allocate kernel code and data and that there is
 *	already a PMEG allocate for it.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The hardware segment table for the kernel is modified to validate the
 *	the page.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_GetDevicePage(virtAddr) 
    Address	virtAddr; /* Virtual address where a page has to be 
			   * validated at. */
{
    VmMachPTE	pte;
    int		page;

    page = Vm_KernPageAllocate();
    if (page == -1) {
	panic("Vm_GetDevicePage: Couldn't get memory\n");
    }
    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | VirtToPhysPage(page) |
	    VMMACH_DONT_CACHE_BIT;
    SET_ALL_PAGE_MAP(virtAddr, pte);
    return;
}
#endif
d4329 1
a4329 1
						 * mapping at. */
a4389 5
#ifdef NOTDEF
    /* for debugging */
    printf("KernelVA 0x%x, numBytes 0x%x, userVA 0x%x.\n",
            kernelVirtAddr, numBytes, userVirtAddr);
#endif /* NOTDEF */
a4453 6

#ifdef NOTDEF
    /* for debugging */
    printf("From Map kernel into user: new user addr is 0x%x.\n",
            userVirtAddr);
#endif /* NOTDEF */
d4482 1
d4486 5
a4490 1
    VmMachFlushPage(virtAddr);
d4492 1
a4492 1
	VmMachSetPageMap(virtAddr, (VmMachPTE)0);
d4536 2
a4537 2
	firstPage = (unsigned)addr >> VMMACH_PAGE_SHIFT;
	lastPage = ((unsigned)addr + numBytes - 1) >> VMMACH_PAGE_SHIFT;
d4539 1
a4539 1
	    virtAddr = (Address) (firstPage << VMMACH_PAGE_SHIFT);
d4543 4
a4546 2
	    VmMachFlushPage(virtAddr);
	    SET_ALL_PAGE_MAP(virtAddr, pte);
d4575 27
a4601 1
    return(GEN_INVALID_ARG);
d4604 4
d4631 4
a4634 2
    virtAddr = (Address) (segPtr->offset << VMMACH_PAGE_SHIFT);
    VmMachFlushSegment(virtAddr);
a4676 75

/*
 * This looks like dead code.
 */
#if 0
/*
 *----------------------------------------------------------------------
 *
 * ByteFill --
 *
 *	Fill numBytes at the given address.  This routine is optimized to do 
 *      4-byte fills.  However, if the address is odd then it is forced to
 *      do single byte fills.
 *
 * Results:
 *	numBytes bytes of the fill byte are placed at *destPtr at the 
 *	given address.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */

static void
ByteFill(fillByte, numBytes, destPtr)
    register unsigned char fillByte;	/* The byte to be filled in. */
    register int numBytes;	/* The number of bytes to be filled in. */
    Address destPtr;		/* Where to fill. */
{
    register unsigned int fillInt = 
	(fillByte) | (fillByte << 8) | (fillByte << 16) | (fillByte << 24);

    register int *dPtr = (int *) destPtr;
    
    /*
     * If the address is on an aligned boundary then fill in as much
     * as we can in big transfers (and also avoid loop overhead by
     * storing many fill ints per iteration).  Once we have less than
     * 4 bytes to fill then it must be done by byte copies.
     */
    /*
     * On the sun4, I should try double-words...
     */
#define WORDMASK	0x1

    if (((int) dPtr & WORDMASK) == 0) {
	while (numBytes >= 32) {
	    *dPtr++ = fillInt;
	    *dPtr++ = fillInt;
	    *dPtr++ = fillInt;
	    *dPtr++ = fillInt;
	    *dPtr++ = fillInt;
	    *dPtr++ = fillInt;
	    *dPtr++ = fillInt;
	    *dPtr++ = fillInt;
	    numBytes -= 32;
	}
	while (numBytes >= 4) {
	    *dPtr++ = fillInt;
	    numBytes -= 4;
	}
	destPtr = (char *) dPtr;
    }

    /*
     * Fill in the remaining bytes
     */

    while (numBytes > 0) {
	*destPtr++ = fillByte;
	numBytes--;
    }
}
#endif
d4898 3
a4900 1
	VmMachFlushPage(mapAddr);
a5205 1

d5272 1
a5272 1
	for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG_INT; i++ ) { 
d5283 1
a5283 1
	     a += VMMACH_PAGE_SIZE_INT;
d5327 53
@


9.38
log
@Fixed lint complaints.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.37 91/09/10 18:30:34 rab Exp Locker: shirriff $ SPRITE (Berkeley)";
a230 8
/*
 * PMEG segment info list entry.
 */
struct PMEGseg {
    struct PMEGseg	*nextLink;	/* Linked list ptr. */
    struct Vm_Segment	*segPtr;	/* Software segment. */
    int			hardSegNum;	/* Hardware segment number. */
};
d238 1
a238 1
    struct PMEGseg		segInfo;	/* Info on software segment. */
a860 1

d917 1
a917 1
	pmegPtr->segInfo.nextLink = (struct PMEGseg *)NIL;
d1222 1
d1264 3
a1266 1
    newSegTableSize = segPtr->ptSize / VMMACH_NUM_PAGES_PER_SEG;
d1503 1
a1503 1
    struct PMEGseg		*curSeg, *nextSeg;
d1527 1
a1527 1
	for (curSeg = &pmegPtr->segInfo; curSeg != (struct PMEGseg *)NIL;) {
d1586 1
a1586 1
		free((char *)curSeg);
d1590 1
a1590 1
	pmegPtr->segInfo.nextLink = (struct PMEGseg *)NIL;
d1712 1
a1712 1
    struct PMEGseg	*segPtr, *nextPtr;
d1731 1
a1731 1
	for (segPtr = &pmegPtr->segInfo; segPtr != (struct PMEGseg *)NIL;) {
d1741 1
a1741 1
		free((char *)segPtr);
d1746 1
a1746 1
    pmegPtr->segInfo.nextLink = (struct PMEGseg *) NIL;
d3462 1
a3462 1
    struct	PMEGseg		*pmegSegPtr;
d3516 10
a3525 5
	    pmegSegPtr = (struct PMEGseg *)malloc(sizeof(struct PMEGseg));
	    pmegSegPtr->nextLink = pmegPtr->segInfo.nextLink;
	    pmegPtr->segInfo.nextLink = pmegSegPtr;
	    pmegSegPtr->segPtr = virtAddrPtr->segPtr;
	    pmegSegPtr->hardSegNum = hardSeg;
@


9.37
log
@Fixed lint errors and removed tracing.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.36 91/08/20 12:22:58 mgbaker Exp Locker: rab $ SPRITE (Berkeley)";
d105 4
d110 1
a1601 2
	    Boolean	printedStealPMEG = FALSE;

d2459 1
a2459 1
	    (unsigned int) mapAddr &= ~VMMACH_OFFSET_MASK;
d2565 4
d2605 1
d5018 1
a5018 1
    (unsigned) beginAddr |= VMMACH_VME_ADDR_BIT;
d5071 1
a5071 1
	(unsigned int) mapAddr += VMMACH_PAGE_SIZE_INT;
@


9.36
log
@Fixed code to match comment about mapping 32 Megabytes of pages
consecutively.  With the bug, we weren't.  It's not clear on what
sun architecture this would have been a problem, though.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.35 91/08/19 18:36:08 shirriff Exp Locker: mgbaker $ SPRITE (Berkeley)";
a29 1
#include <vmTrace.h>
a109 2
static void VmMachTracePage _ARGS_((register VmMachPTE pte,
	unsigned int pageNum));
a363 3
static	Boolean		printedSegTrace;
static	PMEG		*tracePMEGPtr;

a1365 7
    if (vm_Tracing) {
	Vm_TraceSegDestroy	segDestroy;

	segDestroy.segNum = segPtr->segNum;
	VmStoreTraceRec(VM_TRACE_SEG_DESTROY_REC, sizeof(segDestroy),
			(Address)&segDestroy, TRUE);
    }
a1606 14
		    if (vm_Tracing) {
			if (!printedStealPMEG) {
			    short	stealPMEGRec;

			    printedStealPMEG = TRUE;
			    printedSegTrace = FALSE;
			    tracePMEGPtr = pmegPtr;
			    VmStoreTraceRec(VM_TRACE_STEAL_PMEG_REC,
					    sizeof(short), 
					    (Address)&stealPMEGRec,TRUE);
			}
			VmMachTracePage(hardPTE,
			    (unsigned int) (VMMACH_NUM_PAGES_PER_SEG_INT - i));
		    } else {
a1608 1
		    }
a1835 8
		if (vm_Tracing) {
		    Vm_ProcInfo	*vmPtr;

		    vmPtr = procPtr->vmPtr;
		    vmPtr->segPtrArray[VM_CODE]->traceTime = vmTraceTime;
		    vmPtr->segPtrArray[VM_HEAP]->traceTime = vmTraceTime;
		    vmPtr->segPtrArray[VM_STACK]->traceTime = vmTraceTime;
		}
a3088 8
		    Vm_TracePTEChange	pteChange;
		    if (vm_Tracing) {
			pteChange.changeType = VM_TRACE_SET_SEG_PROT;
			pteChange.segNum = segPtr->segNum;
			pteChange.pageNum = firstPage;
			pteChange.softPTE = FALSE;
			pteChange.beforePTE = pte;
		    }
a3090 6
		    if (vm_Tracing) {
			pteChange.afterPTE = pte;
			VmStoreTraceRec(VM_TRACE_PTE_CHANGE_REC,
					 sizeof(Vm_TracePTEChange),
					 (Address)&pteChange, TRUE);
		    }
d3097 1
a3097 1
#endif sun4
a3146 1
    Vm_TracePTEChange		pteChange;
d3149 1
a3149 1
#endif sun4
a3167 7
	    if (vm_Tracing) {
		pteChange.changeType = VM_TRACE_SET_PAGE_PROT;
		pteChange.segNum = virtAddrPtr->segPtr->segNum;
		pteChange.pageNum = virtAddrPtr->page;
		pteChange.softPTE = FALSE;
		pteChange.beforePTE = hardPTE;
	    }
a3170 6
	    if (vm_Tracing) {
		pteChange.afterPTE = hardPTE;
		VmStoreTraceRec(VM_TRACE_PTE_CHANGE_REC,
				 sizeof(Vm_TracePTEChange), 
				 (Address)&pteChange, TRUE);
	    }
d3177 1
a3177 1
#endif sun4
d3328 1
a3328 1

a3356 1
    Vm_TracePTEChange		pteChange;
a3370 11
	    if (vm_Tracing) {
		pteChange.changeType = VM_TRACE_CLEAR_REF_BIT;
		pteChange.segNum = virtAddrPtr->segPtr->segNum;
		pteChange.pageNum = virtAddrPtr->page;
		pteChange.softPTE = FALSE;
		pteChange.beforePTE = pte;
		pteChange.afterPTE = pte & ~VMMACH_REFERENCED_BIT;
		VmStoreTraceRec(VM_TRACE_PTE_CHANGE_REC,
				 sizeof(Vm_TracePTEChange), 
				 (Address)&pteChange, TRUE);
	    }
d3377 1
a3405 1
    Vm_TracePTEChange		pteChange;
a3419 11
	    if (vm_Tracing) {
		pteChange.changeType = VM_TRACE_CLEAR_MOD_BIT;
		pteChange.segNum = virtAddrPtr->segPtr->segNum;
		pteChange.pageNum = virtAddrPtr->page;
		pteChange.softPTE = FALSE;
		pteChange.beforePTE = pte;
		pteChange.afterPTE = pte & ~VMMACH_MODIFIED_BIT;
		VmStoreTraceRec(VM_TRACE_PTE_CHANGE_REC,
				sizeof(Vm_TracePTEChange), 
				(Address)&pteChange, TRUE);
	    }
d3426 1
a3558 3
#ifndef CLEAN
	    VmCheckListIntegrity((List_Links *)pmegPtr);
#endif
a3562 3
#ifndef CLEAN
		VmCheckListIntegrity((List_Links *)pmegPtr);
#endif
a3604 3
#ifndef CLEAN
	    VmCheckListIntegrity((List_Links *)segPtr->procList);
#endif
a3636 18
    if (vm_Tracing) {
	Vm_TracePTEChange	pteChange;

	pteChange.changeType = VM_TRACE_VALIDATE_PAGE;
	pteChange.segNum = segPtr->segNum;
	pteChange.pageNum = virtAddrPtr->page;
	pteChange.softPTE = FALSE;
	if (tHardPTE & VMMACH_RESIDENT_BIT) {
	    pteChange.beforePTE = tHardPTE;
	} else {
	    pteChange.beforePTE = 0;
	}
	pteChange.afterPTE = hardPTE;
	VmStoreTraceRec(VM_TRACE_PTE_CHANGE_REC,
			sizeof(Vm_TracePTEChange), 
			(Address)&pteChange, TRUE);
    }

d3662 1
a3693 1
    Vm_TracePTEChange		pteChange;
a3722 10
    if (vm_Tracing) {
	pteChange.changeType = VM_TRACE_INVALIDATE_PAGE;
	pteChange.segNum = virtAddrPtr->segPtr->segNum;
	pteChange.pageNum = virtAddrPtr->page;
	pteChange.softPTE = FALSE;
	pteChange.beforePTE = hardPTE;
	pteChange.afterPTE = 0;
	VmStoreTraceRec(VM_TRACE_PTE_CHANGE_REC, sizeof(Vm_TracePTEChange),
			(Address)&pteChange, TRUE);
    }
a3733 3
#ifndef CLEAN
	    VmCheckListIntegrity((List_Links *)flushSegPtr->procList);
#endif
a3780 3
#ifndef CLEAN
	    VmCheckListIntegrity((List_Links *)pmegPtr);
#endif
d3783 1
d3815 1
d3875 1
d3920 1
d4096 1
d4367 1
a4367 1
	(unsigned int) mapAddr += VMMACH_PAGE_SIZE_INT;
d4409 1
d4445 1
a4445 1
					 	 * mapping at. */
a4585 115
 * ----------------------------------------------------------------------------
 *
 * VmMach_Trace --
 *
 *      Scan through all of the PMEGs generating trace records for those pages
 *	that have been referenced or modified since the last time that we
 *	checked.
 *  
 * Results:
 *      None.
 *
 * Side effects:
 *	None.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_Trace()
{
    register	PMEG			*pmegPtr;
    register	Vm_Segment		*segPtr;
    register	int			pmegNum;
    register	int			curTraceTime;

    MASTER_LOCK(vmMachMutexPtr);

    /*
     * Save the current trace time and then increment it to ensure that
     * any segments that get used while we are scanning memory won't get 
     * missed.
     */
    curTraceTime = vmTraceTime;
    vmTraceTime++;

    /*
     * Spin through all of the pmegs.
     */
    for (pmegNum = 0, pmegPtr = pmegArray;
	 pmegNum < VMMACH_NUM_PMEGS;
	 pmegPtr++, pmegNum++) {
	segPtr = pmegPtr->segInfo.segPtr;
	if ((pmegPtr->flags & PMEG_NEVER_FREE) ||
	    segPtr == (Vm_Segment *)NIL ||
	    segPtr->traceTime < curTraceTime) {
	    continue;
	}
	vmTraceStats.machStats.pmegsChecked++;
	printedSegTrace = FALSE;
	tracePMEGPtr = pmegPtr;
	VmMachTracePMEG(pmegNum);
    }

    MASTER_UNLOCK(vmMachMutexPtr);

    VmCheckTraceOverflow();
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMachTracePage --
 *
 *      Generate a trace record for the given page.
 *  
 * Results:
 *      None.
 *
 * Side effects:
 *	None.
 *
 * ----------------------------------------------------------------------------
 */
static void
VmMachTracePage(pte, pageNum)
    register	VmMachPTE	pte;	/* Page table entry to be traced. */
    unsigned	int		pageNum;/* Inverse of page within PMEG. */
{
    Vm_TraceSeg			segTrace;
    Vm_TracePage		pageTrace;
    register	PMEG		*pmegPtr;
    register	Vm_Segment	*segPtr = (Vm_Segment *)NIL;

    refModMap[PhysToVirtPage(pte & VMMACH_PAGE_FRAME_FIELD)] |=
			pte & (VMMACH_REFERENCED_BIT | VMMACH_MODIFIED_BIT);
    if (!printedSegTrace) {
	/*
	 * Trace of the segment.
	 */
	printedSegTrace = TRUE;
	pmegPtr = tracePMEGPtr;
	segPtr = pmegPtr->segInfo.segPtr;
	segTrace.hardSegNum = pmegPtr->segInfo.hardSegNum;
	segTrace.softSegNum = segPtr->segNum;
	segTrace.segType = segPtr->type;
	segTrace.refCount = segPtr->refCount;
	VmStoreTraceRec(VM_TRACE_SEG_REC, sizeof(Vm_TraceSeg), 
			(Address)&segTrace, FALSE);
    }

    /*
     * Trace the page.
     */
    pageTrace = VMMACH_NUM_PAGES_PER_SEG_INT - pageNum;
    if (pte & VMMACH_REFERENCED_BIT) {
	pageTrace |= VM_TRACE_REFERENCED;
    } 
    if (pte & VMMACH_MODIFIED_BIT) {
	pageTrace |= VM_TRACE_MODIFIED;
    }
    VmStoreTraceRec(0, sizeof(Vm_TracePage), (Address)&pageTrace, FALSE);
}


/*
d4670 1
d5074 1
a5074 1
	panic("Alloc out of bounds"):0)
d5367 1
a5367 24
/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_CopySharedMem --
 *
 *      Copies machine-dependent shared memory data structures to handle
 *      a fork.
 *
 * Results:
 *      None. (This routine is stubbed out for the sun4.
 *
 * Side effects:
 *      None.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMachTracePMEG(pmeg)
int pmeg;
{
    panic("VmMachTracePMEG called.\n");
}

d5387 1
a5387 1
    register  Vm_VirtAddr	virtAddr;
@


9.35
log
@These are the changes from 9.29 to 9.30.  I lost these changes when
I installed the Unix compatibility changes.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.34 91/08/12 22:19:25 shirriff Exp Locker: shirriff $ SPRITE (Berkeley)";
d465 1
a465 1
     * at least enough for the * code we run on for now.  This amount is
d468 1
a468 1
    kernPages = 32 / VMMACH_PAGE_SIZE_INT;
@


9.34
log
@Fix to shared memory allocation.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.33 91/08/09 15:03:08 shirriff Exp Locker: shirriff $ SPRITE (Berkeley)";
d354 1
a354 1
int	vmMachKernMemSize = 32 * 1024 * 1024;
d461 8
a468 1
    kernPages = vmMachKernMemSize / VMMACH_PAGE_SIZE_INT;
@


9.33
log
@Changed routien for allocating shared memory.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.32 91/07/26 17:12:30 shirriff Exp Locker: shirriff $ SPRITE (Berkeley)";
d5455 1
a5455 2
    int firstBlock = (((int)*reqAddr)-VMMACH_SHARED_START_ADDR+
	    VMMACH_SHARED_BLOCK_SIZE-1) /
@


9.32
log
@Large install for unix compatibility.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm.ken/sun4c.md/RCS/vmSun.c,v 1.1 91/05/05 16:40:44 shirriff Exp Locker: shirriff $ SPRITE (Berkeley)";
d5447 1
a5447 1
VmMach_SharedStartAddr(procPtr,size,reqAddr)
d5451 1
d5453 18
a5470 1
    return VmMach_Alloc(&procPtr->vmPtr->machPtr->sharedData, size, reqAddr);
@


9.31
log
@Fixing a problem where we try to map pages for too many pmegs.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.30 91/03/29 18:46:09 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d69 2
d230 9
d244 1
a244 4
    struct      Vm_Segment      *segPtr;        /* Back pointer to segment that
                                                   this cluster is in */
    int				hardSegNum;	/* The hardware segment number
						   for this pmeg. */
d354 1
a354 1
int	vmMachKernMemSize = 40 * 1024 * 1024;
d461 1
a461 8
    /* 
     * We used to map pages for vmMachKernMemSize of memory.  But now that
     * the kernel size is bigger, there may not be enough pmegs for that.
     * So for now we just map a number of pmegs that is safe and gives us
     * at least enough for the * code we run on for now.  This amount is
     * 32 megabytes of pmegs.
     */
    kernPages = 32 / VMMACH_PAGE_SIZE_INT;
d918 1
a918 1
	pmegPtr->segPtr = (Vm_Segment *) NIL;
d920 1
d974 2
a975 2
	pmegArray[pageCluster].segPtr = vm_SysSegPtr;
	pmegArray[pageCluster].hardSegNum = i;
d993 1
a993 1
    pmegArray[VMMACH_INV_PMEG].segPtr = vm_SysSegPtr;
d1033 3
a1035 2
			    pmegArray[pageCluster].segPtr = vm_SysSegPtr;
			    pmegArray[pageCluster].hardSegNum = i;
d1082 1
a1082 1
	if (pmegPtr->segPtr == (Vm_Segment *) NIL 
d1376 3
d1510 1
d1529 2
a1530 1
    if (pmegPtr->segPtr != (Vm_Segment *) NIL) {
d1534 6
a1539 9
	vmStat.machDepStat.stealPmeg++;
	segPtr = pmegPtr->segPtr;
	*GetHardSegPtr(segPtr->machPtr, pmegPtr->hardSegNum) = VMMACH_INV_PMEG;
	virtAddr = (Address) (pmegPtr->hardSegNum << VMMACH_SEG_SHIFT);
	/*
	 * Delete the pmeg from all appropriate contexts.
	 */
	oldContext = VmMachGetContextReg();
        if (segPtr->type == VM_SYSTEM) {
d1541 1
a1541 2
	     * For cache accesses of data with the supervisor tag set,
	     * the flush only needs to be done in one context.
d1543 44
a1586 28
	    numValidPages = GetNumValidPages(virtAddr);
	    if (numValidPages >
		    (VMMACH_CACHE_SIZE / VMMACH_PAGE_SIZE_INT)) {
		VmMachFlushSegment(virtAddr);
	    } else {
		/* flush the pages */
		FlushValidPages(virtAddr);
	    }

	    for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
		VmMachSetContextReg(i);
		VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);
	    }
        } else {
	    for (i = 1, contextPtr = &contextArray[1];
		 i < VMMACH_NUM_CONTEXTS; 
		 i++, contextPtr++) {
		if (contextPtr->flags & CONTEXT_IN_USE) {
		    if (contextPtr->map[pmegPtr->hardSegNum] == pmegNum) {
			VmMachSetContextReg(i);
			contextPtr->map[pmegPtr->hardSegNum] = VMMACH_INV_PMEG;
			numValidPages = GetNumValidPages(virtAddr);
			if (numValidPages >
				(VMMACH_CACHE_SIZE / VMMACH_PAGE_SIZE_INT)) {
			    VmMachFlushSegment(virtAddr);
			} else {
			    /* flush the pages */
			    FlushValidPages(virtAddr);
a1587 1
			VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);
a1588 7
		    if (contextPtr->map[MAP_SEG_NUM] == pmegNum) {
			VmMachSetContextReg(i);
			contextPtr->map[MAP_SEG_NUM] = VMMACH_INV_PMEG;
			VmMachFlushSegment((Address)VMMACH_MAP_SEG_ADDR);
			VmMachSetSegMap((Address)VMMACH_MAP_SEG_ADDR,
					VMMACH_INV_PMEG);
		    }
d1591 7
a1597 1
        }
d1638 2
a1639 2
    pmegPtr->segPtr = softSegPtr;
    pmegPtr->hardSegNum = hardSegNum;
d1736 1
d1754 18
a1771 1
    pmegPtr->segPtr = (Vm_Segment *) NIL;
d2569 1
d2572 7
a2579 1
	transVirtAddrPtr->segPtr = procPtr->vmPtr->machPtr->mapSegPtr;
d2768 14
d2908 13
d3539 1
d3544 1
d3565 1
d3567 33
d3720 3
d3761 3
d4339 1
a4339 1
    Address	beginAddr;
d4742 1
a4742 1
	segPtr = pmegPtr->segPtr;
d4783 1
a4783 1
    register	Vm_Segment	*segPtr;
d4793 2
a4794 2
	segPtr = pmegPtr->segPtr;
	segTrace.hardSegNum = pmegPtr->hardSegNum;
d5303 7
a5309 4
#define ALLOC(x,s)	(sharedData->allocVector[(x)]=s)
#define FREE(x)		(sharedData->allocVector[(x)]=0)
#define SIZE(x)		(sharedData->allocVector[(x)])
#define ISFREE(x)	(sharedData->allocVector[(x)]==0)
a5310 1

d5402 1
a5402 1
    int numBlocks = SIZE(firstBlock);
d5405 10
a5414 1
    dprintf("VmMach_Unalloc: freeing %d blocks at %x\n",firstBlock,addr);
d5420 4
a5423 2
	    printf("Freeing free shared address %d %d %d\n",i,i+firstBlock,
		    (int)addr);
d5482 4
d5489 2
a5490 3
    procPtr->vmPtr->sharedStart = (Address) VMMACH_SHARED_START_ADDR;
    procPtr->vmPtr->sharedEnd = (Address) VMMACH_SHARED_START_ADDR +
	    VMMACH_USER_SHARED_PAGES*VMMACH_PAGE_SIZE;
d5538 4
d5676 3
a5725 3



@


9.30
log
@Removed some unused variables.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.29 90/12/07 13:46:37 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d346 1
a346 1
int	vmMachKernMemSize = 32 * 1024 * 1024;
d453 8
a460 1
    kernPages = vmMachKernMemSize / VMMACH_PAGE_SIZE_INT;
@


9.29
log
@Cache flush optimization to speed up pmeg stealing.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.28 90/12/06 17:59:47 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d685 1
d687 1
d2389 1
d2392 2
a4226 1
    static initialized = FALSE;
@


9.28
log
@Cache flushing changes.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.27 90/10/19 15:50:03 jhh Exp Locker: mgbaker $ SPRITE (Berkeley)";
d1526 13
a1540 8
		numValidPages = GetNumValidPages(virtAddr);
		if (numValidPages >
			(VMMACH_CACHE_SIZE / VMMACH_PAGE_SIZE_INT)) {
		    VmMachFlushSegment(virtAddr);
		} else {
		    /* flush the pages */
		    FlushValidPages(virtAddr);
		}
@


9.27
log
@started to add routine to map regions into contiguous DVMA space
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.26 90/09/20 16:30:24 mgbaker Exp Locker: jhh $ SPRITE (Berkeley)";
d80 2
a103 1
static void VmMachFlushCacheRange _ARGS_((Address startAddr, Address endAddr));
d1494 1
d1528 8
a1535 1
		VmMachFlushSegment(virtAddr);
d1546 8
a1553 1
			VmMachFlushSegment(virtAddr);
d1620 65
a2540 57
 * VmMachFlushCacheRange --
 *
 *	Flush the cache in the given range of addresses, inclusive,
 *	no matter what context we are in.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Cache flushed in range of addresses.
 *
 *----------------------------------------------------------------------
 */
static void
VmMachFlushCacheRange(startAddr, endAddr)
    Address	startAddr;		/* first address to flush */
    Address	endAddr;		/* last address to flush */
{
    char	*beginFlush, *endFlush;
    char	dummy;
    char	*cacheFlushPtr;
#define	 VMMACH_CACHE_MASK (VMMACH_CACHE_SIZE - 1)
#define	 VMMACH_LINE_MASK (VMMACH_CACHE_LINE_SIZE - 1)

    if ((unsigned int )endAddr -
	    (unsigned int )startAddr >= VMMACH_CACHE_SIZE) {
	FlushWholeCache();

	return;
    }
    
	
    /* Offset into array where entry maps to 1st entry of cache. */
    cacheFlushPtr = (char *) (((unsigned int) (cacheFlusherArray +
	    VMMACH_CACHE_SIZE)) & ~VMMACH_CACHE_MASK);
    /* Get bits that map address into cache location. */
    beginFlush = (char *)((((unsigned int)startAddr) & ~VMMACH_LINE_MASK) &
	    VMMACH_CACHE_MASK);
    endFlush = (char *)((((unsigned int) endAddr) & ~VMMACH_LINE_MASK) &
	    VMMACH_CACHE_MASK);
    beginFlush = cacheFlushPtr + ((unsigned int) beginFlush);
    endFlush = cacheFlushPtr + ((unsigned int) endFlush);

    while (beginFlush <= endFlush) {
	dummy = *beginFlush;
	beginFlush += VMMACH_CACHE_LINE_SIZE;
    }
#ifdef lint
    dummy = dummy;
#endif
    return;
}


/*
 *----------------------------------------------------------------------
 *
d2638 1
a2638 1
    VmMachFlushCacheRange(virtAddr1, virtAddr1 + 254);
d2991 1
d3004 7
a3010 4
		    /* flush a range, so we don't care what context we're in. */
		    VmMachFlushCacheRange(virtAddr, (Address)
			    (((((unsigned int) virtAddr) + VMMACH_SEG_SIZE) &
			    ~(VMMACH_SEG_SIZE - 1)) - 1));
d3107 2
d3148 7
a3154 4
	    /* flush a range so we don't care what context we're in. */
	    VmMachFlushCacheRange(testVirtAddr, (Address)
		    (((((unsigned int) testVirtAddr) + VMMACH_PAGE_SIZE &
		    ~(VMMACH_PAGE_SIZE - 1)) - 1)));
@


9.26
log
@Fixed a procedure name.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.25 90/09/20 16:27:47 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d23 19
a41 19
#include "sprite.h"
#include "vmSunConst.h"
#include "machMon.h"
#include "vm.h"
#include "vmInt.h"
#include "vmMach.h"
#include "vmMachInt.h"
#include "vmTrace.h"
#include "list.h"
#include "mach.h"
#include "proc.h"
#include "sched.h"
#include "stdlib.h"
#include "sync.h"
#include "sys.h"
#include "dbg.h"
#include "net.h"
#include "stdio.h"
#include "bstring.h"
d4057 1
a4085 1
    static initialized = FALSE;
d4089 1
a4089 1
    if (!initialized) {
d4091 1
a4091 1
	initialized = TRUE;
d4152 126
@


9.25
log
@Fixed a procedure name.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.24 90/09/12 13:31:50 mendel Exp Locker: mgbaker $ SPRITE (Berkeley)";
d2690 1
a2690 1
    MachFlushWindowsToStack();
d2818 1
a2818 1
    MachFlushWindowsToStack();
@


9.24
log
@Added back VmMach_LockCachePage() and VmMach_UnlockCachePage() routines.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.23 90/09/11 10:47:15 shirriff Exp $ SPRITE (Berkeley)";
d1815 1
a1815 1
	    VmMachFlushCurrentContext();
@


9.23
log
@Added function prototyping.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm.ken/sun4.md/RCS/vmSun.c,v 9.22 90/08/14 18:53:46 mgbaker Exp Locker: shirriff $ SPRITE (Berkeley)";
d5296 123
@


9.22
log
@Added /dev/fb support.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.21 90/07/16 11:10:07 mendel Exp Locker: mgbaker $ SPRITE (Berkeley)";
a25 1
#include "vmMachInt.h"
d28 2
d40 2
d79 35
a113 2
static void SegDelete();
static void WriteHardMapSeg();
a353 9
static void	MMUInit();
static int	PMEGGet();
static void	PMEGFree();
static Boolean	PMEGLock();
static void	SetupContext();
static void	VmMachTracePage();
static void	PageInvalidate();
static void FlushWholeCache();

d514 1
a514 1
int
d550 1
a550 1
int
d582 1
a582 1
int
d615 1
a615 1
void
d617 1
a617 1
    unsigned	char	context;
d619 1
a619 1
    unsigned	char	pmeg;
a1225 2
static void	CopySegData();

d2473 1
a2473 1
void
d2544 1
d2638 1
d2962 1
a2962 1
    register	Boolean		skipSeg;
d4203 1
d4240 1
d4704 1
a5235 1
    procPtr->vmPtr->machPtr->sharedData.allocVector;
d5267 2
a5268 1
    bcopy(parentSharedData->allocVector, childSharedData->allocVector,
a5271 1

d5276 1
a5276 1
 * VmMach_LockCachePage --
d5278 2
a5279 2
 *      Perform machine dependent locking of a kernel resident file cache
 *	page.
d5282 1
a5282 1
 *      None.
a5284 79
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_LockCachePage(kernelAddress)
    Address	kernelAddress;	/* Address on page to lock. */
{
    register  Vm_VirtAddr	virtAddr;
    register  VMMACH_SEG_NUM	*segTablePtr, pmeg;
    register  int		hardSeg, i;
    Vm_PTE    *ptePtr;
    VmMachPTE		hardPTE;
    /*
     * Ignore pages not in cache pmeg range.
     */
    if (!IN_FILE_CACHE_SEG(kernelAddress)) {
	return;
    }

    MASTER_LOCK(vmMachMutexPtr);

    pmeg = VmMachGetSegMap(kernelAddress);
    if (pmeg == VMMACH_INV_PMEG) {
	int	oldContext, i;
	unsigned int a;
	/*
	 *  If not a valid PMEG install a new pmeg and load its mapping. 
	 */
	virtAddr.segPtr = vm_SysSegPtr;
	virtAddr.page = ((unsigned int) kernelAddress) >> VMMACH_PAGE_SHIFT;
	virtAddr.offset = 0;
	virtAddr.flags = 0;
	virtAddr.sharedPtr = (Vm_SegProcList *) NIL;
    
	hardSeg = PageToOffSeg(virtAddr.page, (&virtAddr));
	segTablePtr = (VMMACH_SEG_NUM *) 
			    GetHardSegPtr(vm_SysSegPtr->machPtr, hardSeg);
	if (*segTablePtr != VMMACH_INV_PMEG) {
	    panic("VmMach_LockCachePage: Bad segTable entry.\n");
	}
	*segTablePtr = pmeg = PMEGGet(vm_SysSegPtr, hardSeg, 0);
	/*
	 * Have to propagate the PMEG to all contexts.
	 */
	oldContext = VmMachGetContextReg();
	for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	    VmMachSetContextReg(i);
	    VmMachSetSegMap(kernelAddress, pmeg);
	}
	VmMachSetContextReg(oldContext);
	/*
	 * Reload the entire PMEG.
	 */
	a = (hardSeg << VMMACH_SEG_SHIFT);
	for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG_INT; i++ ) { 
	    ptePtr = VmGetPTEPtr(vm_SysSegPtr, (a >> VMMACH_PAGE_SHIFT));
	    if ((*ptePtr & VM_PHYS_RES_BIT)) {
		hardPTE = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | 
				VirtToPhysPage(Vm_GetPageFrame(*ptePtr));
		SET_ALL_PAGE_MAP(a, hardPTE);
		pmegArray[pmeg].pageCount++;
	     }
	     a += VMMACH_PAGE_SIZE_INT;
	}
    }
    pmegArray[pmeg].lockCount++;

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_UnlockCachePage --
 *
 *      Perform machine dependent unlocking of a kernel resident page.
 *
 * Results:
a5286 2
 * Side effects:
 *
d5289 1
d5291 2
a5292 2
VmMach_UnlockCachePage(kernelAddress)
    Address	kernelAddress;	/* Address on page to unlock. */
d5294 1
a5294 17
    register  VMMACH_SEG_NUM	pmeg;

    if (!IN_FILE_CACHE_SEG(kernelAddress)) {
	return;
    }

    MASTER_LOCK(vmMachMutexPtr);

    pmeg = VmMachGetSegMap(kernelAddress);

    pmegArray[pmeg].lockCount--;
    if (pmegArray[pmeg].lockCount < 0) {
	panic("VmMach_UnlockCachePage lockCount < 0\n");
    }

    MASTER_UNLOCK(vmMachMutexPtr);
    return;
a5295 2


@


9.21
log
@Added code for wiring down file cache pages.
@
text
@d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.19 90/03/14 09:16:49 mendel Exp $ SPRITE (Berkeley)";
d4217 1
a4217 1
 * Vm_MapKernelIntoUser --
d4247 2
a4248 8
    int				numSegs;
    int				firstPage;
    int				numPages;
    Proc_ControlBlock		*procPtr;
    register	Vm_Segment	*segPtr;
    int				hardSegNum;
    int				i;
    unsigned int		pte;
d4250 61
d4318 1
a4318 1
     * Make user virtual address hardware segment aligned (round up) and 
d4321 2
a4322 2
    hardSegNum = 
	    (unsigned int) (userVirtAddr + VMMACH_SEG_SIZE - 1) >> VMMACH_SEG_SHIFT;
d4325 1
a4325 1
	return(SYS_INVALID_ARG);
d4334 1
a4334 1
	return(SYS_INVALID_ARG);
d4350 1
d4352 3
a4354 3
	(Address)GetHardSegPtr(segPtr->machPtr,
		(unsigned int)userVirtAddr >> VMMACH_SEG_SHIFT),
		numSegs * sizeof (VMMACH_SEG_NUM));
d4356 6
a4361 6
	pte = VmMachGetPageMap((Address)(kernelVirtAddr +
		(i * VMMACH_PAGE_SIZE_INT)));
	pte &= ~VMMACH_KR_PROT;
	pte |= VMMACH_URW_PROT;
	VmMachSetPageMap((Address)(kernelVirtAddr + (i*VMMACH_PAGE_SIZE_INT)),
		pte);
d4368 2
a4369 2
    
    /* 
d4374 8
a4381 1
    return(Vm_CopyOut(4, (Address) &userVirtAddr, (Address) realVirtAddrPtr));
@


9.21.1.1
log
@Contains asplos statistics-gathering code.
@
text
@d9 8
a16 2
 * Copyright (C) 1985 Regents of the University of California
 * All rights reserved.
d20 1
a20 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.20 90/06/21 14:00:39 shirriff Exp $ SPRITE (Berkeley)";
d40 6
a45 20
/*
 * Temporary statistics for asplos paper.
 */
unsigned int	vm_contextFlushes = 0;
unsigned int	vm_segmentFlushes = 0;
unsigned int	vm_pageFlushes = 0;
unsigned int	vm_byteRangeFlushes = 0;
unsigned int	vm_contextContextFlushes = 0;
unsigned int	vm_contextSegmentFlushes = 0;
unsigned int	vm_contextPageFlushes = 0;
unsigned int	vm_pageContextFlushes = 0;
unsigned int	vm_pageSegmentFlushes = 0;
unsigned int	vm_pagePageFlushes = 0;
unsigned int	vm_interruptContextFlushes = 0;
unsigned int	vm_interruptSegmentFlushes = 0;
unsigned int	vm_interruptPageFlushes = 0;
unsigned int	vm_sysContextFlushes[SYS_NUM_SYSCALLS];
unsigned int	vm_sysSegmentFlushes[SYS_NUM_SYSCALLS];
unsigned int	vm_sysPageFlushes[SYS_NUM_SYSCALLS];

d52 1
d279 17
d323 3
a325 2
void		VmMachTracePage();
void		PageInvalidate();
d397 1
a397 1
#ifdef multiprocessor
d2461 1
a2461 1
	VmMachFlushWholeCache();
d2492 1
a2492 1
 * VmMachFlushWholeCache --
d2505 2
a2506 2
void
VmMachFlushWholeCache()
a3389 1
    register  int		newPMEG;
d3392 4
a3395 2
    Address			addr;
    int				i;
d3402 1
a3402 1
    if (!VMMACH_ADDR_CHECK(addr)) {
d3418 1
d3420 1
d3425 4
d3430 1
d3432 11
a3442 3
	    newPMEG = PMEGGet(segPtr, hardSeg, PMEG_DONT_ALLOC);
	} else {
	    newPMEG = PMEGGet(segPtr, hardSeg, 0);
d3444 2
a3445 1
        *segTablePtr = newPMEG;
d3455 1
d3457 1
d3462 1
d3464 1
d3507 1
d3509 1
d3559 21
a3580 3
    MASTER_UNLOCK(vmMachMutexPtr);
}

d3598 1
a3598 1
INTERNAL void
d3661 1
d3663 1
d3711 1
d3713 1
d4396 1
a4396 1
INTERNAL void
d5179 123
@


9.20
log
@Added VmMach_CopySharedMem to copy memory on a fork.
@
text
@d9 8
a16 2
 * Copyright (C) 1985 Regents of the University of California
 * All rights reserved.
d40 6
d52 1
d279 17
d323 3
a325 2
void		VmMachTracePage();
void		PageInvalidate();
d397 1
a397 1
#ifdef multiprocessor
d2461 1
a2461 1
	VmMachFlushWholeCache();
d2492 1
a2492 1
 * VmMachFlushWholeCache --
d2505 2
a2506 2
void
VmMachFlushWholeCache()
a3389 1
    register  int		newPMEG;
d3392 4
a3395 2
    Address			addr;
    int				i;
d3402 1
a3402 1
    if (!VMMACH_ADDR_CHECK(addr)) {
d3418 1
d3420 1
d3425 4
d3430 1
d3432 11
a3442 3
	    newPMEG = PMEGGet(segPtr, hardSeg, PMEG_DONT_ALLOC);
	} else {
	    newPMEG = PMEGGet(segPtr, hardSeg, 0);
d3444 2
a3445 1
        *segTablePtr = newPMEG;
d3455 1
d3457 1
d3462 1
d3464 1
d3507 1
d3509 1
d3559 21
a3579 3

    MASTER_UNLOCK(vmMachMutexPtr);
}
d3598 1
a3598 1
INTERNAL void
d3661 1
d3663 1
d3711 1
d3713 1
d4396 1
a4396 1
INTERNAL void
d5179 123
@


9.19
log
@Modified VmMach_GetContext() to return an error (rather than seg faulting) 
when passed a process without a context laoded.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.18 90/01/29 19:34:43 mgbaker Exp Locker: mendel $ SPRITE (Berkeley)";
d5069 33
@


9.18
log
@Bug fix to cache-flushing code and new net packet-mapping code for the sun4c
since its network driver accesses don't go through the cache.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.17 89/12/12 18:06:16 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d1356 2
a1357 1
 *	Context number for process.
d1368 3
a1370 1
    return procPtr->vmPtr->machPtr->contextPtr->context;
@


9.17
log
@Took out debugging stuff.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.16 89/11/30 11:54:23 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d2244 1
a2244 1
#ifdef NOTDEF
d2246 2
a2247 2
     * I need to test out the other code to see if it works on the sun4c, so
     * for the mean-time use the old code for the sun4c.
d2251 20
a2270 24
	 scatGathLength--, inScatGathPtr++, outScatGathPtr++) {
	outScatGathPtr->length = inScatGathPtr->length;
	if (inScatGathPtr->length == 0) {
	    continue;
	}
	/*
	 * Map the piece of the packet in.  Note that we know that a packet
	 * piece is no longer than 1536 bytes so we know that we will need
	 * at most two page table entries to map a piece in.
	 */
	VmMachFlushPage(inScatGathPtr->bufAddr);
	VmMachFlushPage(mapAddr);
	VmMachSetPageMap(mapAddr, VmMachGetPageMap(inScatGathPtr->bufAddr));
	outScatGathPtr->bufAddr = 
	    mapAddr + ((unsigned)inScatGathPtr->bufAddr & VMMACH_OFFSET_MASK);
	mapAddr += VMMACH_PAGE_SIZE_INT;
	endAddr = inScatGathPtr->bufAddr + inScatGathPtr->length - 1;
	if (((unsigned)inScatGathPtr->bufAddr & ~VMMACH_OFFSET_MASK_INT) !=
	    ((unsigned)endAddr & ~VMMACH_OFFSET_MASK_INT)) {
	    VmMachFlushPage(endAddr);
	    VmMachFlushPage(mapAddr);
	    VmMachSetPageMap(mapAddr, VmMachGetPageMap(endAddr));
	    mapAddr += VMMACH_PAGE_SIZE_INT;
	}
d2333 1
a2333 1
#endif /* NOTDEF */
d3993 1
a3993 1
    endAddr = (Address) ((((unsigned int) srcAddr) + numBytes) &
d4080 1
a4080 1
    endAddr = (Address) ((((unsigned int) mapAddr) + numBytes) &
d4721 1
a4721 1
    endAddr = (Address) ((((unsigned int) srcAddr) + numBytes) &
d4821 1
a4821 1
    endAddr = (Address) ((((unsigned int) mapAddr) + numBytes) &
@


9.16
log
@This version contains lots of debugging code, but works on the sun4c.
I'm about to strip out debugging code, but I may break something and am
keeping this for reference.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.15 89/11/28 12:07:13 brent Exp Locker: mgbaker $ SPRITE (Berkeley)";
a328 15
#ifdef sun4c
/*
 * For debugging window overflow problem.
 */
void
CallForOverflow()
{
    volatile	int	foo;
    int	i;

    i = foo;
    return;
}
#endif /* sun4c */

a1397 14
#ifdef sun4c
typedef	struct lilac {
    Vm_Segment	*fromSegPtr;
    Vm_Segment	*toSegPtr;
    int		fromHardSegNum;
    int		toHardSegNum;
    int		pmegNum;
} StolenPmeg;

StolenPmeg	stolenArray[100];
int		stolenIndex = 0;
#endif /* sun4c */


a1459 12
#ifdef sun4c
	/* log from segPtr, to segPtr fromHardSegNum and virtAddr */
	stolenArray[stolenIndex].fromSegPtr = segPtr;
	stolenArray[stolenIndex].toSegPtr = softSegPtr;
	stolenArray[stolenIndex].fromHardSegNum = pmegPtr->hardSegNum;
	stolenArray[stolenIndex].toHardSegNum = hardSegNum;
	stolenArray[stolenIndex].pmegNum = pmegNum;
	stolenIndex++;
	if (stolenIndex >= 100) {
	    stolenIndex = 0;
	}
#endif sun4c
a1465 3
#ifdef sun4c
		CallForOverflow();
#endif
a1475 3
#ifdef sun4c
		CallForOverflow();
#endif
a1481 3
#ifdef sun4c
		CallForOverflow();
#endif
a1525 12
#ifdef sun4c
	ptePtr = pteArray;
	VmMachReadAndZeroPMEG(pmegNum, ptePtr);
	for (i = 0;
	     i < VMMACH_NUM_PAGES_PER_SEG_INT;
	     i++, ptePtr++) {
	    hardPTE = *ptePtr;
	    if (hardPTE & VMMACH_RESIDENT_BIT) {
		panic("resident ickiness.\n");
	    }
	}
#endif
a2654 1
#ifndef sun4c
a2656 3
#else
	    status = FAILURE;
#endif /* sun4c */
a2672 3
#ifdef sun4c
		CallForOverflow();
#endif
a2782 1
#ifndef sun4c
a2784 3
#else
	    status = FAILURE;
#endif /* sun4c */
a2800 3
#ifdef sun4c
		CallForOverflow();
#endif
a3453 5

#ifdef NOTDEF
	procPtr = Proc_GetCurrentProc();
	procPtr->vmPtr->machPtr->contextPtr->map[hardSeg] = *segTablePtr;
#endif NOTDEF
@


9.15
log
@Mary checking this in for Brent.  He fixed a masterlock deadlock.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.14 89/11/27 14:54:20 mgbaker Exp $ SPRITE (Berkeley)";
d329 15
d1507 3
d1520 3
d1529 3
a2716 1
#ifdef NOTDEF
a2722 4
#else
	    status = VmMachQuickNDirtyCopy(bytesToCopy, fromAddr, toAddr,
		fromContext, toContext);
#endif /* NOTDEF */
d2739 3
a2851 1
#ifdef NOTDEF
a2857 4
#else
	    status = VmMachQuickNDirtyCopy(bytesToCopy, fromAddr, toAddr,
		    fromContext, toContext);
#endif /* NOTDEF */
d2874 3
@


9.14
log
@Migration, debugging, etc.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.13 89/11/21 19:37:28 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d3973 1
a3973 1
ENTRY static void
a3982 2
    MASTER_LOCK(vmMachMutexPtr);

a4010 2

    MASTER_UNLOCK(vmMachMutexPtr);
d4033 1
a4033 1
Address
d4130 1
a4130 1
void
d4695 1
a4695 1
ENTRY static void
a4703 1
    MASTER_LOCK(vmMachMutexPtr);
d4723 1
a4723 1
	    printf("DevBufferInit: DMA space already valid at 0x%x\n",
a4738 2

    MASTER_UNLOCK(vmMachMutexPtr);
d4761 1
a4761 1
Address
d4868 1
a4868 1
void
@


9.13
log
@Checkin in preparation for fixing a horrible bug.

@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.12 89/10/30 20:32:20 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d1400 2
a1401 2
    Address	fromSegPtr;
    Address	toSegPtr;
a1845 1
	/* XXX No need to flush anything? */
d2693 1
d2700 4
d2830 1
d2837 4
d2917 5
a3319 1
	    /* XXX Need to flush it? */
a3379 1
	    /* XXXX Should I flush something here? */
d3435 3
d3439 1
a3488 1
	    /* XXXX Necessary to flush something ? */
d3495 2
d3498 1
d3502 6
a3508 1
	/* XXX necessary to flush something? */
d3510 2
d3514 16
a3535 17
#ifdef NOTDEF
#ifdef sun4c
	{
	    int	contextNum;
	    int	segNum;
	    Address	virtAddr;

	    virtAddr = (Address) (virtAddrPtr->page << VMMACH_PAGE_SHIFT);
	    contextNum = VmMachGetContextReg();
	    segNum = VmMachGetSegMap(virtAddr);
	    if ((contextArray[contextNum].map[((unsigned int) virtAddr)
		    >> VMMACH_SEG_SHIFT]) != segNum) {
		panic("VmMach_PageValidate: hard/soft segNums different.\n");
	    }
	}
#endif /* sun4c */
#endif /* NOTDEF */
@


9.12
log
@Backing out broken color map allocation for sparcstations.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.11 89/10/30 18:03:58 shirriff Exp Locker: mgbaker $ SPRITE (Berkeley)";
a33 1
#ifndef multiprocessor
a34 7
#undef MASTER_LOCK
#define MASTER_LOCK(mutexPtr)
#undef MASTER_UNLOCK
#define MASTER_UNLOCK(mutexPtr)

#else

a40 2
#endif

a64 6
#ifdef NOTDEF
/* This is broken for now */
#ifdef sun4c
static	void	MapColorMapAndIdRomAddr();
#endif
#endif NOTDEF
d1398 14
d1474 12
d1551 13
a1563 1
	 }
d2437 2
a2438 1
char	cacheFlusherArray[VMMACH_NUM_CACHE_LINES * VMMACH_CACHE_LINE_SIZE * 2];
d2468 8
d2497 17
d2518 2
d2521 1
d2523 1
a2523 1
	cacheFlusherArray[i] = 33;
d2694 1
d2697 3
d2826 1
d2829 3
d3495 17
d4519 29
d4550 1
a4550 1
 *      Machine dependent vm commands.
@


9.11
log
@Added PageToOffSeg macro.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.10 89/10/22 23:43:26 shirriff Exp Locker: shirriff $ SPRITE (Berkeley)";
d75 2
d80 1
d804 2
d808 2
a809 1
#endif
d1039 2
d1045 1
a1045 1
#endif NOTDEF
d1102 1
a1102 1
#else
d1105 2
a1106 2
#endif
#endif
d1129 1
@


9.10
log
@Made changes for shared memory.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.7 89/10/18 10:44:01 mgbaker Exp Locker: shirriff $ SPRITE (Berkeley)";
d63 8
d3000 2
a3001 2
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page-
	    segOffset(virtAddrPtr)+virtAddrPtr->segPtr->offset));
d3087 2
a3088 2
	pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page-
		segOffset(virtAddrPtr)+virtAddrPtr->segPtr->offset));
d3161 2
a3162 2
	pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page-
		segOffset(virtAddrPtr)+virtAddrPtr->segPtr->offset));
d3217 2
a3218 2
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page-
	    segOffset(virtAddrPtr)+virtAddrPtr->segPtr->offset));
d3278 2
a3279 2
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page-
	    segOffset(virtAddrPtr)+virtAddrPtr->segPtr->offset));
d3356 1
a3356 2
    hardSeg = PageToSeg(virtAddrPtr->page - segOffset(virtAddrPtr) +
	    segPtr->offset);
d3506 2
a3507 2
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page-
	    segOffset(virtAddrPtr)+virtAddrPtr->segPtr->offset));
d3657 2
a3658 4
    firstSeg = PageToSeg(virtAddrPtr->page-segOffset(virtAddrPtr)+
	    virtAddrPtr->segPtr->offset);
    lastSeg = PageToSeg(lastPage-segOffset(virtAddrPtr)+
	    virtAddrPtr->segPtr->offset);
d3714 2
a3715 4
    firstSeg = PageToSeg(virtAddrPtr->page-segOffset(virtAddrPtr)+
	    virtAddrPtr->segPtr->offset);
    lastSeg = PageToSeg(lastPage-segOffset(virtAddrPtr)+
	    virtAddrPtr->segPtr->offset);
d3899 1
a3899 1
	    VmMachSetSegMap(virtAddr, pmeg);
d4181 2
a4182 1
	pte = VmMachGetPageMap(kernelVirtAddr + (i * VMMACH_PAGE_SIZE_INT));
@


9.9
log
@Added MapColorMapAndIdRomAddr
(checked in by shirriff)
@
text
@d261 1
d264 7
d1394 1
d1788 9
a1796 1

d2992 2
a2993 1
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page));
d3079 2
a3080 1
	pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page));
d3153 2
a3154 1
	pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page));
d3209 2
a3210 1
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page));
d3270 2
a3271 1
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page));
d3318 1
d3348 2
a3349 1
    hardSeg = PageToSeg(virtAddrPtr->page);
d3373 1
d3378 1
d3414 2
a3415 1
	if (pte & (VM_COW_BIT | VM_READ_ONLY_PROT)) {
d3499 2
a3500 1
    pmegNum = *GetHardSegPtr(machPtr, PageToSeg(virtAddrPtr->page));
d3535 1
d3583 1
d3650 4
a3653 2
    firstSeg = PageToSeg(virtAddrPtr->page);
    lastSeg = PageToSeg(lastPage);
d3709 4
a3712 2
    firstSeg = PageToSeg(virtAddrPtr->page);
    lastSeg = PageToSeg(lastPage);
a4537 29

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SharedStartAddr --
 *
 *      Determine the starting address for a shared segment.
 *
 * Results:
 *      Returns the proper start address for the segment.
 *
 * Side effects:
 *      None.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ReturnStatus
VmMach_SharedStartAddr(procPtr,size,reqAddr)
Proc_ControlBlock	*procPtr;
int		size;		/* Length of shared segment. */
Address		*reqAddr;	/* Requested start address. */
{
    return SUCCESS;
}

void VmMach_SharedSegFinish() {}
void VmMach_SharedProcStart() {}
void VmMach_SharedProcFinish() {}
d4772 225
@


9.8
log
@Lint changes.
@
text
@d67 3
d630 3
d785 3
d1016 90
@


9.7
log
@Working 32-bit dvma in place.  PMEG stealing bug fixed.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.6 89/10/11 17:36:01 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d169 1
a169 1
	VmMachSetPageMap((virtAddr), (pte)); \
a285 1
static void	ByteFill();
a705 1
	VMMACH_SEG_NUM	pmeg;
d722 1
a722 1
	    VmMachSetSegMap(virtAddr, *segTablePtr);
a807 3
#if defined(sun3) || defined(sun4)
    int				dontUse;
#endif
d1232 1
a1232 1
	    PMEGFree((unsigned int) *pmegPtr);
d1375 1
a1375 1
			VmMachFlushSegment(VMMACH_MAP_SEG_ADDR);
a1454 1
    int		flushedIt = 0;
a1465 2
	int		i;
	unsigned int	pte;
d1645 1
a1645 1
	VmMachSetContextReg(contextPtr->context);
d1692 1
a1692 1
	VmMachSetContextReg(contextPtr->context);
d1766 1
a1766 1
#if defined(sun2) || defined(sun4)
d1850 1
a1850 1
				((unsigned)virtAddr) >> VMMACH_SEG_SHIFT,
d1984 1
a1984 1
	    VmMachSetSegMap(virtAddr, *(segTablePtr + i));
d1996 1
a1996 1
	    VmMachSetSegMap(virtAddr, *(segTablePtr + i));
d2014 1
a2014 1
	    VmMachSetSegMap(virtAddr, *(segTablePtr + i));
d2026 1
a2026 1
	    VmMachSetSegMap(virtAddr, *(segTablePtr + i));
d2307 1
a2307 1
    char	readChar;
d2324 1
a2324 1
	readChar = *beginFlush;
d2327 3
d2429 6
d2512 1
a2512 1
	    VmMachSetContextReg(toContext);
d2529 1
a2529 1
		    fromProcPtr->vmPtr->machPtr->contextPtr->context);
d2640 1
a2640 1
	    VmMachSetContextReg(fromContext);
d2656 2
a2657 1
	    VmMachSetContextReg(toProcPtr->vmPtr->machPtr->contextPtr->context);
d2717 1
a2717 1
	 *GetHardSegPtr(machPtr->mapSegPtr->machPtr, machPtr->mapHardSeg));
d2776 1
a2776 1
		    VmMachSetSegMap(vmMachPTESegAddr, *pmegNumPtr);
d3274 1
a3274 1
	    VmMachSetSegMap(addr, *segTablePtr);
d3286 1
a3286 1
	VmMachSetSegMap(addr, *segTablePtr);
d3419 1
a3419 1
                            VmMachSetContextReg(flushContext);
d3423 1
a3423 1
                            VmMachSetContextReg(oldContext);
d3544 3
d4048 2
a4049 1
	VmMachSetPageMap(kernelVirtAddr + (i * VMMACH_PAGE_SIZE_INT), pte);
d4332 4
d4404 1
d4421 1
a4458 1
    int			i;
d4494 1
a4494 1
        VmMachSetSegMap(virtAddr, pmeg);
a4533 1
    Address	newAddr;
a4565 2
	newAddr = (Address) (VMMACH_32BIT_DMA_START_ADDR +
		(i * VMMACH_PAGE_SIZE_INT));
d4601 3
a4603 2
    VmMachSetContextReg(oldContext);
    beginAddr = (Address) (VMMACH_32BIT_DMA_START_ADDR + (i * VMMACH_PAGE_SIZE_INT) +
@


9.6
log
@New 32-bit dvma for sun4's seems to work.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.5 89/10/10 14:30:34 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d2136 1
a2136 1
#ifdef sun4c
d2229 1
a2229 1
#endif /* sun4c */
d4427 1
a4427 1
 * DevBufferInit --
d4480 3
@


9.5
log
@Fixed bug that was killing the sparcstations and sometimes the sun4's.
A line was accidentally deleted in PMEGGet() causing the context not to
be reset when a pmeg was stolen.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.4 89/09/18 21:01:38 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
a4423 2
#define	VMMACH_USER_DMA_START_ADDR	0x40000
#define	VMMACH_USER_DMA_SIZE		0x100000
d4442 1
a4442 1
DevUserDMABufferInit()
d4452 4
d4457 1
a4457 1
    VmMachSetupUserDVMA(); 
d4461 2
a4462 2
    baseAddr = (Address)VMMACH_USER_DMA_START_ADDR;
    endAddr = (Address)(VMMACH_USER_DMA_START_ADDR + VMMACH_USER_DMA_SIZE);
d4489 1
a4489 1
static	Boolean	userdmaPageBitMap[VMMACH_USER_DMA_SIZE / VMMACH_PAGE_SIZE_INT];
d4495 1
a4495 1
 * VmMach_UserDMAAlloc --
d4509 1
a4509 1
VmMach_UserDMAAlloc(numBytes, srcAddr)
d4521 2
a4522 1
    int		oldContext;
d4527 1
a4527 1
	DevUserDMABufferInit();
d4539 11
a4549 2
    /* see if request can be satisfied */
    for (i = 0; i < (VMMACH_USER_DMA_SIZE / VMMACH_PAGE_SIZE_INT); i++) {
d4553 2
a4554 9
	/*
	 * Must be aligned in the cache to avoid write-backs of stale data
	 * from other references to stuff on this page.
	 */
	newAddr = (Address)(VMMACH_USER_DMA_START_ADDR + (i * VMMACH_PAGE_SIZE_INT));
	if (((unsigned int) newAddr & (VMMACH_CACHE_SIZE - 1)) !=
		((unsigned int) beginAddr & (VMMACH_CACHE_SIZE - 1))) {
	    continue;
	}
d4556 2
a4557 1
		((i + j) < (VMMACH_USER_DMA_SIZE / VMMACH_PAGE_SIZE_INT)); j++) {
d4563 1
a4563 1
		((i + j) < (VMMACH_USER_DMA_SIZE / VMMACH_PAGE_SIZE_INT))) {
d4568 1
d4572 1
a4572 1
	    "VmMach_DMAAlloc: unable to satisfy request for %d bytes at 0x%x\n",
d4587 1
a4587 1
		VMMACH_USER_DMA_START_ADDR, pte);
d4591 1
a4591 1
    beginAddr = (Address) (VMMACH_USER_DMA_START_ADDR + (i * VMMACH_PAGE_SIZE_INT) +
d4594 2
d4597 1
a4597 1
    return (Address) ((unsigned) beginAddr | 0x80000000);
d4604 1
a4604 1
 * VmMach_UserDMAFree --
d4618 1
a4618 1
VmMach_UserDMAFree(numBytes, mapAddr)
d4630 2
a4631 1
    mapAddr = (Address) ((unsigned)mapAddr & ~0x80000000);
d4641 1
a4641 1
	(((unsigned int) VMMACH_USER_DMA_START_ADDR) >> VMMACH_PAGE_SHIFT_INT);
@


9.4
log
@Checked in by shirriff for mgbaker.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.3 89/09/14 19:02:53 shirriff Exp Locker: mgbaker $ SPRITE (Berkeley)";
d1359 1
a1473 1
	Address		addr;
a1477 28
#ifdef NOTDEF
	for (i = 0; i < VMMACH_NUM_PAGES_PER_SEG_INT; i++) {
	    addr = vmMachPTESegAddr +
		    ((i << VMMACH_PAGE_SHIFT) & VMMACH_PAGE_MASK);
	    pte = VmMachReadPTE(pmegNum, addr);
	    if ((pte & VMMACH_DONT_CACHE_BIT) == 0) {
		int	j;
		int	oldContext;

		oldContext = VmMachGetContextReg();
		addr = (Address) (pmegPtr->hardSegNum << VMMACH_SEG_SHIFT);
		for (j = 0; j < VMMACH_NUM_CONTEXTS; j++) {
		    VmMachSetContextReg(j);
		    if (!flushedIt) {
			VmMachFlushSegment(addr);
			flushedIt = TRUE;
		    }
		}
		VmMachSetContextReg(oldContext);
		break;
	    }
	}
#else
	addr = (Address) (pmegPtr->hardSegNum << VMMACH_SEG_SHIFT);
#ifdef BADFLUSH
	VmMachFlushSegment(addr);
#endif
#endif
a1487 3
#ifdef NOTDEF
    if (pmegPtr->pageCount == 0 && !(pmegPtr->flags & PMEG_DONT_ALLOC)) {
#else
a1488 1
#endif /* sun4 */
d1837 1
d1839 1
d2095 3
a2097 2
	       * SunOS doesn't allow the network pages to be cached for the
	       * sun4c.  I should check if this is really a problem or not.
a2316 1
#define	 VMMACH_CACHE_SIZE (VMMACH_CACHE_LINE_SIZE * VMMACH_NUM_CACHE_LINES)
d2338 11
d4422 224
@


9.3
log
@Mary checking this in for Ken.  He's added some shared segment stubs.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.2 89/09/14 15:19:03 mgbaker Exp Locker: shirriff $ SPRITE (Berkeley)";
d2504 1
a2504 1
     * allow windows to be flushed after the copy and overwrite stuff?
d2530 2
d2626 11
d2658 2
d3850 2
a3851 1
	for (j = 1; j < numPages; j++) {
d3856 2
a3857 1
	if (j == numPages) {
d3864 4
d3869 1
@


9.2
log
@Adding new quick n' dirty cross-context copy.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.1 89/09/13 16:58:01 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d4395 1
a4395 1
 * VmMach_SharedStart --
d4407 3
a4409 3
Address
VmMach_SharedStart(reqAddr,size)
Address		reqAddr;	/* Requested start address. */
d4411 1
d4413 1
a4413 1
    return reqAddr;
d4416 3
@


9.1
log
@In PageInvalidate I now check to make sure the cache flush is done in
the same context as the one in which the page was validated.  This means
no unnecessary flushes are done at a higher level (such as in AllocCheck,
which invalidates pages in a different context) and so that there shouldn't
be any cache write-back errors from pages that didn't get flushed in their
context before they were invalidated.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 9.0 89/09/12 15:24:20 douglis Stable Locker: mgbaker $ SPRITE (Berkeley)";
d2510 1
a2510 1
     * Do a hardware segments worth at a time until done.
d2513 24
a2548 5
	segOffset = (unsigned int)fromAddr & (VMMACH_SEG_SIZE - 1);
	bytesToCopy = VMMACH_SEG_SIZE - segOffset;
	if (bytesToCopy > numBytes) {
	    bytesToCopy = numBytes;
	}
d2625 1
a2625 1
     * Do a hardware segments worth at a time until done.
d2628 24
a2661 5
	}
	segOffset = (unsigned int)toAddr & (VMMACH_SEG_SIZE - 1);
	bytesToCopy = VMMACH_SEG_SIZE - segOffset;
	if (bytesToCopy > numBytes) {
	    bytesToCopy = numBytes;
@


9.0
log
@Changing version numbers.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.24 89/09/12 14:17:28 mgbaker Exp Locker: douglis $ SPRITE (Berkeley)";
a2946 3
	 * On the sun4 we must flush this page from the cache, but we must do
	 * it in the context to which it was allocated.  If it's in a shared
	 * context, we must do this in all the contexts.
a2947 32
	VmProcLink	*procLinkPtr;
	Vm_Segment	*segPtr;
	Vm_ProcInfo	*vmPtr;
	VmMach_ProcData	*machPtr;
	VmMach_Context	*contextPtr;
	unsigned int	context;
	unsigned int	oldContext;

	segPtr = virtAddrPtr->segPtr;
	if (segPtr != (Vm_Segment *) NIL) {
	    LIST_FORALL(segPtr->procList, (List_Links *) procLinkPtr) {
		vmPtr = procLinkPtr->procPtr->vmPtr;
		if (vmPtr != (Vm_ProcInfo *) NIL) {
		    machPtr = vmPtr->machPtr;
		    if (machPtr != (VmMach_ProcData *) NIL) {
			contextPtr = machPtr->contextPtr;
			if (contextPtr != (VmMach_Context *) NIL) {
			    context = contextPtr->context;
			    /* save old context */
			    oldContext = VmMachGetContextReg();
			    /* move to page's context */
			    VmMachSetContextReg(context);
			    /* flush page in its context */
			    VmMachFlushPage(virtAddrPtr->page <<
				    VMMACH_PAGE_SHIFT);
			    /* back to old context */
			    VmMachSetContextReg(oldContext);
			}
		    }
		}
	    }
	}
d3324 7
d3364 2
a3365 1
     * will already have been flushed.
d3368 29
a3396 1
	VmMachFlushPage(testVirtAddr);
@


1.24
log
@Fixed stale cache data problem in DMAAlloc.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.23 89/09/08 16:22:47 mgbaker Exp $ SPRITE (Berkeley)";
@


1.23
log
@Fix to VmAllocCheck to flush a page in the correct context before it's
invalidated.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.22 89/08/30 12:25:46 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d167 1
d169 4
d178 1
a178 1

a3762 1
    int		virtPage;
d3764 1
d3776 1
a3776 1
						/* begging of last page */
d3787 9
a3810 1
	VmMachFlushPage(srcAddr);
d3812 2
a3813 3
	virtPage = ((unsigned int) srcAddr) >> VMMACH_PAGE_SHIFT;
	pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT |
	      VirtToPhysPage(Vm_GetKernPageFrame(virtPage));
d3867 1
a3867 1
	SET_ALL_PAGE_MAP(mapAddr, (VmMachPTE) VirtToPhysPage(0));
@


1.22
log
@Changes for latest installed kernel.  More performance opts.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.21 89/08/25 13:42:07 mendel Exp Locker: mgbaker $ SPRITE (Berkeley)";
d22 1
d226 1
a226 1
    int				context;/* Which context this is. */
a1690 3
#ifdef NOTDEF
	    j = ((unsigned int)mach_KernStart) >> VMMACH_SEG_SHIFT;
#else
d1693 2
a1694 1
	     * hole in the address space, this will save something like 30ms!
a1696 1
#endif
a1723 9
#ifdef NOTDEF
	VmMachSegMapCopy(contextPtr->map, 0, mach_KernStart);
	XXX
	/*
	 * Since user addresses are never higher than the bottom of the
	 * hole in the address space, this will save something like 30ms!
	 */
	VmMachSegMapCopy(contextPtr->map, 0, VMMACH_BOTTOM_OF_HOLE);
#else
a1724 1
#endif
d2160 5
a2164 1
#ifdef NOTDEF
d2253 1
a2253 1
#endif NOTDEF
d2942 3
d2946 32
d3761 1
d3793 1
d3808 2
d3840 1
d3858 1
d4339 23
@


1.21
log
@Made the kernel text segment write-protected.  
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.20 89/08/25 12:27:01 mgbaker Exp Locker: mendel $ SPRITE (Berkeley)";
d1728 1
a1728 1
#else
d1734 2
d2170 3
a2172 1

d2200 62
@


1.20
log
@Before further flushing changes.  This is the latest installed version.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.19 89/08/10 00:12:21 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
a662 1
#ifdef NOTDEF		/* for debugging */
a663 3
#else
	    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | 
#endif NOTDEF
a1352 1
	oldContext = VmMachGetContextReg();
d4083 2
d4086 19
a4104 11
#ifdef NOTDEF	/* nop for debugging */
    firstPage = (unsigned)addr >> VMMACH_PAGE_SHIFT;
    lastPage = ((unsigned)addr + numBytes - 1) >> VMMACH_PAGE_SHIFT;
    for (; firstPage <= lastPage; firstPage++) {
	virtAddr = (Address) (firstPage << VMMACH_PAGE_SHIFT);
	pte = VmMachGetPageMap(virtAddr);
	pte &= ~VMMACH_PROTECTION_FIELD;
	pte |= readWrite ? VMMACH_KRW_PROT : VMMACH_KR_PROT;
	/* Could just flush in current context? XXX */
	VmMachFlushCacheRange(virtAddr, virtAddr + numBytes - 1);
	SET_ALL_PAGE_MAP(virtAddr, pte);
d4106 1
a4106 1
#endif NOTDEF	/* nop for debugging */
@


1.19
log
@Stable for both sun4 and sun4c.
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.18 89/08/09 15:48:25 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d771 9
a1392 1
	    /* XXX Okay since it was flushed when freed? */
d1477 1
d1499 6
d1644 1
d1676 1
d1685 3
a1687 1
	VmMachFlushCurrentContext();
d1695 1
d1697 7
d1731 1
d1733 7
a1768 1
    int		oldContext;
d3297 3
a3299 1
     * Invalidate the page table entry.
d3301 3
a3303 1
    VmMachFlushPage(testVirtAddr);
a3746 2
    Boolean	foundIt = FALSE;
    int		virtPage;
@


1.18
log
@Works for both sun4 and sun4c
@
text
@d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4c.md/vmSun.c,v 1.2 89/08/09 12:40:22 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d216 1
a216 1
    List_Links		     links;	  /* Links so that the contexts can be
d218 5
a222 3
    struct Proc_ControlBlock *procPtr;	/* A pointer to the process table entry
					   for the process that is running in
					   this context. */
d225 2
a226 3
    unsigned short 	     map[VMMACH_NUM_SEGS_PER_CONTEXT];
    int			     context;	/* Which context this is. */
    int			     flags;	/* Defined below. */
d407 1
a407 1
	    (sizeof (short) * VMMACH_NUM_SEGS_PER_CONTEXT));
d615 1
a615 1
    register 	unsigned short	*segTablePtr;
d630 1
a630 1
	    (unsigned short *) ((Address)sysMachPtr + sizeof(VmMach_SegData));
d704 2
a705 2
	int	segNum;
	unsigned short	pmeg;
d797 2
a798 2
    register	unsigned short	*segTablePtr;
    int				pageCluster;
a866 4
#ifdef NOTDEF
	    /*
	     * Why is this notdef'd?  I must invalidate the stuff sometime!
	     */
a869 1
#endif
d1034 1
a1034 1
	(segTableSize * sizeof (short)));
d1039 1
a1039 1
	    (unsigned short *) ((Address)segDataPtr + sizeof(VmMach_SegData));
d1090 1
a1090 1
		sizeof (short)));
d1093 1
a1093 2
    newSegDataPtr->segTablePtr =
	    (unsigned short *) ((Address)newSegDataPtr +
d1134 1
a1134 1
		oldSegDataPtr->numSegs * sizeof (short));
d1149 1
a1149 1
	    oldSegDataPtr->numSegs * sizeof (short));
d1216 1
a1216 1
    register	unsigned short 	*pmegPtr;
d1222 1
a1222 1
    for (i = 0, pmegPtr = (unsigned short *) machPtr->segTablePtr;
d1226 1
a1226 1
	    PMEGFree((int) *pmegPtr);
d1537 1
a1537 1
    int pmegNum;
d1684 1
a1684 1
	    segDataPtr->numSegs * sizeof (short));
d1689 1
a1689 1
		segDataPtr->numSegs * sizeof (short));
d1694 1
a1694 1
		segDataPtr->numSegs * sizeof (short));
d1976 2
a1977 2
    unsigned short		pmeg;
    register unsigned short	*segTablePtr;
d2625 1
a2625 1
    register	unsigned short	*pmegNumPtr;
d2635 2
a2636 1
    pmegNumPtr = (unsigned short *) GetHardSegPtr(segPtr->machPtr, PageToSeg(firstPage)) - 1;
d3075 1
a3075 1
    register  unsigned  short	*segTablePtr;
d3099 1
a3099 1
    segTablePtr = (unsigned short *) GetHardSegPtr(segPtr->machPtr, hardSeg);
d3854 1
a3854 1
		numSegs * sizeof (short));
@


1.17
log
@Stable kernel.
@
text
@d1 2
a2 1
/* vmSunMach.c -
d14 1
a14 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.16 89/07/31 12:58:37 mgbaker Exp $ SPRITE (Berkeley)";
d50 1
d52 5
a56 3
 * For debugging stuff, put values into a circular buffer.  After each value,
 * stamp a special mark, which gets overwritten by next value, so we
 * always know where the end of the list is.
d58 3
a60 8
extern	int	debugCounter;
extern	int	debugSpace[];
#define DEBUG_ADD(thing)        \
    debugSpace[debugCounter++] = (int)(thing);  \
    if (debugCounter >= 500) {  \
        debugCounter = 0;       \
    }                           \
    debugSpace[debugCounter] = (int)(0x11100111);
a248 6
 * Macros to translate from a virtual page to a physical page and back.
 */
#define	VirtToPhysPage(pfNum) ((pfNum) << VMMACH_CLUSTER_SHIFT)
#define	PhysToVirtPage(pfNum) ((pfNum) >> VMMACH_CLUSTER_SHIFT)

/*
d255 3
a257 2
 * The maximum amount of kernel code + data available.
 * Mike, where did you get these magic numbers???
d263 1
a263 1
int	vmMachKernMemSize = 8192 * 1024;
d291 26
d349 4
d358 24
d433 2
d437 1
a437 1
 *     The number of physical pages.
d447 106
a552 5
#ifdef sun2
    return(*romVectorPtr->memorySize / VMMACH_PAGE_SIZE);
#else
    return(*romVectorPtr->memoryAvail / VMMACH_PAGE_SIZE);
#endif
d554 2
d667 1
a667 1
			  i * VMMACH_CLUSTER_SIZE;
d670 1
a670 1
			  i * VMMACH_CLUSTER_SIZE;
d683 1
a683 1
    SET_ALL_PAGE_MAP((Address)mach_StackBottom, (VmMachPTE)0);
d690 1
a690 1
	SET_ALL_PAGE_MAP(virtAddr, (VmMachPTE)0);
d703 3
d709 1
d711 1
a711 1
	for (virtAddr = (Address)mach_KernStart,
d713 4
a716 2
	     virtAddr < (Address)(VMMACH_NUM_SEGS_PER_CONTEXT*VMMACH_SEG_SIZE);
	     virtAddr += VMMACH_SEG_SIZE, segTablePtr++) {
d723 18
d766 1
d768 1
d858 1
d860 1
d863 1
d865 9
d1242 1
a1242 2
 * VmMach_GetC
 ontext --
d2104 7
d2115 1
a2301 4
    DEBUG_ADD(0x11111111);
    DEBUG_ADD(virtAddr1);
    DEBUG_ADD(pte1);
    DEBUG_ADD(pte2);
a2307 2
    DEBUG_ADD(virtAddr2);
    DEBUG_ADD(savePte2);
a2318 2
    DEBUG_ADD(saveValue11);
    DEBUG_ADD(endSave11);
a2322 1
    DEBUG_ADD(endSave21);
a2330 2
    DEBUG_ADD(saveValue12);
    DEBUG_ADD(endSave12);
a2334 2
    DEBUG_ADD(*virtAddr1);
    DEBUG_ADD(*(virtAddr1 + 254));
a2354 2
    DEBUG_ADD(saveValue22);
    DEBUG_ADD(endSave22);
d2414 5
d2421 1
a2517 3
#ifdef sun4
    MachFlushWindowsToStack();
#endif
d3730 1
a3730 1
	SET_ALL_PAGE_MAP(mapAddr, (VmMachPTE) 0);
d3799 1
a3799 1
						 * to map in. */
d3801 2
a3802 2
    unsigned int	userVirtAddr;		/* User virtual address to
						 * attempt to start mapping
d3805 1
a3805 1
						 * mapping at. */
d3860 4
a3863 4
        pte = VmMachGetPageMap(kernelVirtAddr + (i * VMMACH_PAGE_SIZE_INT));
        pte &= ~VMMACH_KR_PROT;
        pte |= VMMACH_URW_PROT;
        VmMachSetPageMap(kernelVirtAddr + (i * VMMACH_PAGE_SIZE_INT), pte);
d3865 1
@


1.16
log
@Checking in for Mendel to increase kernel memory.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.15 89/07/18 13:12:36 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d267 2
a268 1
#else 
d271 3
d2230 3
a2232 1

d2329 3
d3612 8
a3619 5
    int	kernelVirtAddr;		/* Kernel virtual address to map in. */
    int	numBytes;		/* Number of bytes to map. */
    int	userVirtAddr;		/* User virtual address to attempt to start 
				   mapping in at. */
    int	realVirtAddrPtr;	/* Where we were able to start mapping at. */
d3627 2
d3671 8
a3678 2
		(unsigned int)userVirtAddr >> VMMACH_SEG_SHIFT), numSegs);

@


1.15
log
@Time for a checkin.  Caching has been turned on in the buffer pool.  I
can't believe it wasn't turned on already.  No wonder things were slow.

@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.14 89/06/16 08:55:14 mendel Exp Locker: mgbaker $ SPRITE (Berkeley)";
a371 7
#ifdef NOTDEF
    /*
     * Turn on caching.
     */
    VmMachClearCacheTags();
    VmMachInitSystemEnableReg();
#endif NOTDEF
a464 6
#ifdef NOTDEF
    /*
     * Will this flush enough?
     */
    VmMachFlushCurrentContext();
#endif NOTDEF
a519 3
#ifdef NOTDEF
	VmMachFlushPage(virtAddr);
#endif NOTDEF
a525 3
#ifdef NOTDEF
    VmMachFlushPage((Address)mach_StackBottom);
#endif NOTDEF
a532 3
#ifdef NOTDEF
	VmMachFlushPage(virtAddr);
#endif NOTDEF
a677 3
#ifdef NOTDEF
	    VmMachFlushSegment(addr);
#endif NOTDEF
a704 3
#ifdef NOTDEF
	VmMachFlushSegment(addr);
#endif NOTDEF
a756 3
#ifdef NOTDEF
			VmMachFlushPage(virtAddr);
#endif NOTDEF
a773 3
#ifdef NOTDEF
		    VmMachFlushSegment(virtAddr);
#endif NOTDEF
d1261 1
d1292 4
a1295 1
		    VmMachFlushSegment(addr);
@


1.14
log
@Added network mapping, moved kernel down 128+ megabytes, grew kernel 
memory. 
,
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.13 89/06/02 10:23:03 mendel Exp Locker: mendel $ SPRITE (Berkeley)";
d65 2
a1004 1
void	SegDelete();
a1842 1
	printf("InitNetMem: 0x%x pmeg %d\n",virtAddr, *(segTablePtr + i));
a1875 1
	printf("InitNetMem: 0x%x pmeg %d\n",virtAddr, *(segTablePtr + i));
a1887 1
    printf("InitNetMem: netMemAddr 0x%x netLastPage 0x%x\n", netMemAddr,netLastPage);
a1940 1
    printf("VmMach_NetMemAlloc: Alloc 0x%x pte 0x%x\n", virtAddr,pte);
a2211 4


void	WriteHardMapSeg();

d2261 12
a2272 5
	/* Flush segment in context of fromProcPtr. */
	oldContext = VmMachGetContextReg();
	VmMachSetContextReg(fromProcPtr->vmPtr->machPtr->contextPtr->context);
	VmMachFlushSegment(fromAddr);
	VmMachSetContextReg(oldContext);
d2357 11
a2367 5
	/* Flush segment in context of toProcPtr. */
	oldContext = VmMachGetContextReg();
	VmMachSetContextReg(toProcPtr->vmPtr->machPtr->contextPtr->context);
	VmMachFlushSegment(toAddr);
	VmMachSetContextReg(oldContext);
d2970 3
a2972 1
    if (addr >= vmStackEndAddr) {
a2973 2
    } else {
	hardPTE &= ~VMMACH_DONT_CACHE_BIT;
d3346 1
a3346 1
#ifdef sun3 || sun4		/* Not just for porting purposes */
d3919 27
@


1.13
log
@Clean pages allocated to DMA space upon boot.  Deleted unused DMA mapping
code.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.11 89/05/04 23:29:40 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d266 1
a266 1
int	vmMachKernMemSize = 4096 * 1024;
d1658 1
a1658 1
#if defined(sun2) || defined(sun4)
d1689 36
a1724 1
#endif /* sun2 or sun4 */
d1750 1
a1750 1
#if defined(sun2) || defined(sun4)
a1792 1
#ifdef sun3
d1816 1
a1816 1
    unsigned char		pmeg;
d1819 2
d1825 1
a1825 1
     * Allocate two pmegs, one for memory and one for mapping.
d1827 4
a1830 1
    segNum = VMMACH_NET_MAP_START >> VMMACH_SEG_SHIFT;
d1833 1
a1833 1
	 i < 2;
d1839 13
d1853 1
d1856 1
a1856 1
     * Propagate the new pmeg mappings to all contexts.
d1858 25
a1882 3
    for (i = 0; i < VMMACH_NUM_CONTEXTS; i++) {
	if (i == VMMACH_KERN_CONTEXT) {
	    continue;
d1884 1
a1884 4
	VmMachSetContextReg(i);
	VmMachSetSegMap((Address)VMMACH_NET_MAP_START, *segTablePtr);
	VmMachSetSegMap((Address)(VMMACH_NET_MAP_START + VMMACH_SEG_SIZE),
			*(segTablePtr + 1));
d1886 1
a1886 1
    VmMachSetContextReg(VMMACH_KERN_CONTEXT);
d1888 2
a1889 1
    netLastPage = ((unsigned)(VMMACH_NET_MEM_START) >> VMMACH_PAGE_SHIFT) - 1;
d1925 1
a1925 3
     * Panic if we are out of memory.  We are out of memory if we have filled
     * up a whole PMEG minus one page.  We have to leave one page at the
     * end because this is used to initialize the INTEL chip.
d1927 1
a1927 2
    if (netMemAddr > (Address) (VMMACH_NET_MEM_START + VMMACH_SEG_SIZE - 
				VMMACH_PAGE_SIZE)) {
d1943 1
a1943 3
#ifdef sun4
	pte |= VMMACH_DONT_CACHE_BIT;
#endif /* sun4 */
d1988 2
d1997 2
a2004 1
#endif
d2215 1
a2215 1
    
@


1.12
log
@*** empty log message ***
@
text
@d752 31
a782 23
	pageCluster = VmMachGetSegMap(virtAddr);
	if (pageCluster != VMMACH_INV_PMEG) {
	    inusePMEG = FALSE;
	    for (j = 0; 
	         j < VMMACH_NUM_PAGES_PER_SEG_INT; 
		 j++, virtAddr += VMMACH_PAGE_SIZE_INT) {
		pte = VmMachGetPageMap(virtAddr);
		if ((pte & VMMACH_RESIDENT_BIT) &&
		    (pte & (VMMACH_TYPE_FIELD|VMMACH_PAGE_FRAME_FIELD)) != 0) {
		    /*
		     * A PMEG contains a valid mapping if the resident
		     * bit is set and the page frame and type field
		     * are non-zero.  On Sun 2/50's the PROM sets
		     * the resident bit but leaves the page frame equal
		     * to zero.
		     */
		    if (!inusePMEG) {
			pmegArray[pageCluster].segPtr = vm_SysSegPtr;
			pmegArray[pageCluster].hardSegNum = i;
			pmegArray[pageCluster].flags = PMEG_NEVER_FREE;
			inusePMEG = TRUE;
		    }
		} else {
d784 1
a784 1
		    VmMachFlushPage(virtAddr);
d786 2
a787 1
		    VmMachSetPageMap(virtAddr, (VmMachPTE)0);
d789 4
a792 5
	    }
	    virtAddr -= VMMACH_SEG_SIZE;
	    if (!inusePMEG ||
	        (virtAddr >= (Address)VMMACH_DMA_START_ADDR &&
		 virtAddr < (Address)(VMMACH_DMA_START_ADDR+VMMACH_DMA_SIZE))) {
d794 8
a801 8
		int z;
		/* 
		 * We didn't find any valid mappings in the PMEG or the PMEG
		 * is in DMA space so delete it.
		 */
		printf("Zapping segment at virtAddr %x\n", virtAddr);
		for (z = 0; z < 100000; z++) {
		}
d804 1
a804 1
		VmMachFlushSegment(virtAddr);
d806 3
a808 2
		VmMachSetSegMap(virtAddr, VMMACH_INV_PMEG);
		pageCluster = VMMACH_INV_PMEG;
d1080 2
a1081 1
 * VmMach_GetContext --
d3342 2
a3343 1
	    panic("DevBufferInit: DMA space already valid\n");
a3362 99

/*
 ----------------------------------------------------------------------
 *
 * VmMach_DevBufferAlloc --
 *
 *	Allocate the amount of memory requested out of the range of kernel 
 *	virtual addresses given in *vmDevBufPtr.  The request is rounded up
 *	to the next page boundary.
 *
 * Results:
 *	Pointer into kernel virtual address space of where to access the
 *	memory.  NIL is returned if there is not enough memory.
 *
 * Side effects:
 *	The range struct is updated to reflect the current amount of memory 
 *	left.
 *
 *----------------------------------------------------------------------
 */
Address
VmMach_DevBufferAlloc(vmDevBufPtr, numBytes)
    VmMach_DevBuffer	*vmDevBufPtr;	/* Where to allocate the memory from. */
    int			numBytes;	/* Number of bytes to allocate. */
{
#ifdef NOTDEF
    Address		retAddr;
    static initialized = FALSE;

    if (!initialized) {
	DevBufferInit(vmDevBufPtr);
	initialized = TRUE;
    }

    retAddr = vmDevBufPtr->baseAddr;
    vmDevBufPtr->baseAddr += numBytes;

    if (vmDevBufPtr->baseAddr > vmDevBufPtr->endAddr) {
	return((Address) NIL);
    }

    /*
     * Round up to next page boundary.
     */

    vmDevBufPtr->baseAddr = 
(Address) (((int) vmDevBufPtr->baseAddr - 1 + VMMACH_PAGE_SIZE) & ~VMMACH_OFFSET_MASK);
    return(retAddr);
#endif NOTDEF
}


/*
 ----------------------------------------------------------------------
 *
 * VmMach_DevBufferMap --
 *
 *	Map the given range of kernel virtual addresses into the kernel's
 *	VAS starting at the given kernel virtual address.  It is assumed
 *	that there are already PMEGS allocated for these pages.
 *	
 *	IMPORTANT: The kernel VAS where to map into must be an address 
 *		   returned from VmMach_DevBufferAlloc.
 *
 * Results:
 *	Pointer into kernel virtual address space of where to access the
 *	memory.
 *
 * Side effects:
 *	The hardware page table is modified.
 *
 *----------------------------------------------------------------------
 */
Address
VmMach_DevBufferMap(numBytes, startAddr, mapAddr)
    int		numBytes;	/* Number of bytes to map in. */
    Address	startAddr;	/* Kernel virtual address to start mapping in.*/
    Address	mapAddr;	/* Where to map bytes in. */
{
#ifdef NOTDEF
    Address 		virtAddr;
    VmMachPTE		pte;
    int			numPages;
    int			virtPage;

    numPages = (((int) startAddr & (VMMACH_PAGE_SIZE - 1)) + numBytes - 1) / 
			VMMACH_PAGE_SIZE + 1;
    for (virtAddr = mapAddr, 
	     virtPage = (unsigned int) (startAddr) >> VMMACH_PAGE_SHIFT;
	 numPages > 0;
	 virtAddr += VMMACH_PAGE_SIZE, virtPage++, numPages--) {
	pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT |
	      VirtToPhysPage(Vm_GetKernPageFrame(virtPage));
	SET_ALL_PAGE_MAP(virtAddr, pte);
    }

    return(mapAddr + ((int) (startAddr) & VMMACH_OFFSET_MASK));
#endif NOTDEF
}
@


1.11
log
@Twenty-third Kernel.  Can read and write disks and tape now.  New interrupt
handler interface, device mapping and probing, etc.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.10 89/05/02 23:26:11 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d166 3
a168 3
    int	i; \
    for (i = 0; i < VMMACH_CLUSTER_SIZE; i++) { \
	VmMachSetPageMap((virtAddr) + i * VMMACH_PAGE_SIZE_INT, (pte) + i); \
d2030 105
@


1.10
log
@New device interface, etc.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.9 89/04/30 18:18:41 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d273 1
a273 1
#define	MAP_SEG_NUM (VMMACH_MAP_SEG_ADDR >> VMMACH_SEG_SHIFT)
d516 1
d518 3
d2009 3
d2015 1
a2015 2
	    (VMMACH_NUM_CACHE_LINES * VMMACH_CACHE_LINE_SIZE))) &
	    ~((VMMACH_NUM_CACHE_LINES * VMMACH_CACHE_LINE_SIZE) - 1));
d2017 4
a2020 4
    beginFlush = (char *)((((unsigned int)startAddr) & ~VMMACH_CACHE_LINE_SIZE)
	    & ((VMMACH_NUM_CACHE_LINES * VMMACH_CACHE_LINE_SIZE) - 1));
    endFlush = (char *)(((((unsigned int) endAddr) & ~VMMACH_CACHE_LINE_SIZE)) &
	    ((VMMACH_NUM_CACHE_LINES * VMMACH_CACHE_LINE_SIZE) - 1));
d2022 1
a2022 1
    endFlush = cacheFlushPtr + ((unsigned int) beginFlush);
d3382 1
d3416 1
a3416 1
	virtPage = (unsigned int) (srcAddr) >> VMMACH_PAGE_SHIFT;
d3466 2
a3467 2
    i = ((unsigned int) mapAddr) >> VMMACH_PAGE_SHIFT_INT -
	(VMMACH_DMA_START_ADDR >> VMMACH_PAGE_SHIFT_INT);
d3469 1
a3469 1
	dmaPageBitMap[i + j] == 0;	/* free page */
d3786 1
d3795 1
a3795 1
	VmMachFlushCacheRange(virtAddr, virtAddr + numBytes);
d3798 1
@


1.9
log
@Kernel 22: User-level caching works, but only with COW turned off.
However, it appears that COW may be a lose on the sun4 with the current way
all the mapping games are done.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.8 89/04/29 19:52:03 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d1182 1
a1182 1
/* XXX */	VmMachFlushSegment(virtAddr);
d1193 1
a1193 1
/* XXX */		VmMachFlushSegment(virtAddr);
d1199 1
a1199 1
/* XXX */		VmMachFlushSegment(VMMACH_MAP_SEG_ADDR);
d1494 1
a1494 1
/* XXX */ VmMachFlushCurrentContext();
a1570 7
#ifdef GOO
/* XXX */ oldContext = VmMachGetContextReg();
    VmMachSetContextReg(contextPtr->context);
    /* Here too??  This is called by things that don't call SetupContext... */
    VmMachFlushCurrentContext();
    VmMachSetContextReg(oldContext);
#endif GOO
a1650 4

DEBUG_ADD(0x7777777);
DEBUG_ADD(virtAddr);

a1657 2
DEBUG_ADD(0x71717171);
DEBUG_ADD(allocatedPMEG);
a1662 2
DEBUG_ADD(0x81818181);
DEBUG_ADD(intelSavedPTE);
a1673 1
DEBUG_ADD(intelPage);
a1704 1
DEBUG_ADD(0x88888888)
d1710 1
a1710 1
/* XXXX */	VmMachSetPageMap(virtAddr, (VmMachPTE)0);
a1711 1
DEBUG_ADD(0x87878787)
a1729 1
DEBUG_ADD(allocatedPMEG)
a1737 2
DEBUG_ADD(0x79797979)
DEBUG_ADD(intelSavedPTE)
a1739 1
DEBUG_ADD(intelPage)
d2079 1
a2079 1
/* XXX */	VmMachFlushSegment(fromAddr);
d2108 1
a2108 1
/* XXX */	VmMachFlushSegment((Address) VMMACH_MAP_SEG_ADDR);
d2168 1
d2198 1
a2198 1
/* XXX */	VmMachFlushSegment((Address) VMMACH_MAP_SEG_ADDR);
d2286 1
a2286 1
/* XXX */	    VmMachFlushCacheRange(virtAddr, (Address)
a2331 3
#ifdef GOO
/* XXX */	    VmMachFlushPage(pageVirtAddr);
#endif GOO
d2424 1
a2424 1
/* XXX */  VmMachFlushCacheRange(testVirtAddr, (Address)
d2906 1
a2906 1
/* XXX */    VmMachFlushPage(testVirtAddr);
d2908 1
a2908 1
/* XXX */	VmMachWritePTE(pmegNum, addr, (VmMachPTE)0);
d3199 1
a3199 2
DevBufferInit(vmDevBufPtr)
    VmMach_DevBuffer *vmDevBufPtr;	/* Buffer struct to initialize. */
d3205 2
d3213 2
a3214 2
    vmDevBufPtr->baseAddr = (Address)VMMACH_DMA_START_ADDR;
    vmDevBufPtr->endAddr = (Address)(VMMACH_DMA_START_ADDR + VMMACH_DMA_SIZE);
d3219 1
a3219 2
    for (virtAddr = vmDevBufPtr->baseAddr;
	 virtAddr < vmDevBufPtr->endAddr; ) {
d3266 1
d3289 1
d3320 1
d3338 130
d3502 2
a3503 1
    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_PROT | VirtToPhysPage(page);
d3744 1
a3744 1
/* XXX */    VmMachFlushPage(virtAddr);
d3786 2
a3787 5
#ifdef NOTDEF
/* XXX */	VmMachFlushPage(virtAddr);
#else
/* XXX */	VmMachFlushCacheRange(virtAddr, virtAddr + numBytes);
#endif NOTDEF
@


1.8
log
@Caching of kernel stacks now works.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.7 89/04/25 15:23:33 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
a31 10
extern	Address	vmStackBaseAddr;
extern	Address	vmStackEndAddr;
/*
 * Temporary macro for circular debugging buffer.
 */
extern	int	debugCounter;
extern	int	debugSpace[];
extern	Address	theAddrOfVmPtr;
extern	Address	theAddrOfMachPtr;

d63 2
d522 1
a522 2
	    if (virtAddr >= vmStackEndAddr ||
		    virtAddr < (Address) MACH_KERN_START) {
a1405 2
	theAddrOfVmPtr = (Address)(&(procPtr->vmPtr));
	theAddrOfMachPtr = (Address)(&(procPtr->vmPtr->machPtr));
d2347 1
a2347 2
		    if (virtAddr >= vmStackEndAddr ||
			    virtAddr < (Address) MACH_KERN_START) {
d2441 1
a2441 2
	    if (testVirtAddr >= vmStackEndAddr  ||
		    testVirtAddr < (Address) MACH_KERN_START) {
d2796 1
a2796 1
    if (addr >= vmStackEndAddr || addr < (Address) MACH_KERN_START) {
@


1.7
log
@Caching kernel text and heap segments works!
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.6 89/04/23 22:07:50 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d33 1
d59 15
d346 1
a346 1
#ifdef sun4	/* while porting */
d378 1
d384 1
d478 6
a525 5
#ifdef sun4
#ifdef NOTDEF
	    pte &= ~VMMACH_DONT_CACHE_BIT
#endif NOTDEF
#endif /* sun4 */
d529 2
a530 2
#ifdef sun4	/* while porting */
	    if (virtAddr >= vmStackBaseAddr ||
d536 3
d545 3
d555 3
d564 1
d568 1
a568 1
     * Finally copy the kernels context to each of the other contexts.
d579 4
d606 6
d698 2
d701 6
a706 1
	    VmMachSetSegMap((Address)(i << VMMACH_SEG_SHIFT), VMMACH_INV_PMEG);
d730 7
a736 1
	VmMachSetSegMap((Address)(i << VMMACH_SEG_SHIFT), VMMACH_INV_PMEG);
d780 3
d800 3
d1064 1
d1076 23
d1191 1
d1202 1
d1208 1
d1223 1
d1298 1
d1300 25
d1505 1
d1543 1
d1571 1
d1582 8
d1670 3
d1676 1
d1680 2
d1687 2
d1696 2
a1697 2
#ifdef sun4	/* while porting */
	    pte |= VMMACH_DONT_CACHE_BIT;
d1699 2
d1732 1
d1737 2
a1738 5
#ifdef sun4
	VmMachSetPageMap(virtAddr, (VmMachPTE)VMMACH_DONT_CACHE_BIT);
#else
	VmMachSetPageMap(virtAddr, (VmMachPTE)0);
#endif sun4
d1740 1
d1759 1
d1767 3
d1772 1
d1892 2
a1893 2
#ifdef sun4	/* while porting */
	    pte |= VMMACH_DONT_CACHE_BIT;
d2007 51
d2066 1
a2066 1
 *	Copy from another processes address space into the current address
d2098 1
d2104 1
d2109 5
d2141 1
d2187 1
d2198 4
d2228 3
d2317 4
d2358 2
a2359 2
		    if (virtAddr < (Address) MACH_KERN_START ||
			    virtAddr >= vmStackBaseAddr) {
d2365 3
d2453 2
a2454 2
	    if (testVirtAddr < (Address) MACH_KERN_START ||
		    testVirtAddr >= vmStackBaseAddr) {
d2460 4
d2660 1
a2660 6
#ifdef sun4
#ifdef NOTDEF
	    pte |= VMMACH_DONT_CACHE_BIT;
#endif NOTDEF
#endif sun4

d2720 1
a2720 5
#ifdef sun4
#ifdef NOTDEF
	    pte |= VMMACH_DONT_CACHE_BIT;
#endif NOTDEF
#endif sun4
d2808 6
a2813 6
#ifdef sun4	/* while porting */
	    if (addr < (Address) MACH_KERN_START || addr >= vmStackBaseAddr) {
		hardPTE |= VMMACH_DONT_CACHE_BIT;
	    } else {
		hardPTE &= ~VMMACH_DONT_CACHE_BIT;
	    }
d2823 1
d2835 1
d2873 1
d2909 1
d2920 1
d2943 1
d2945 1
a2945 5
#ifdef sun4
	VmMachWritePTE(pmegNum, addr, (VmMachPTE)VMMACH_DONT_CACHE_BIT);
#else
	VmMachWritePTE(pmegNum, addr, (VmMachPTE)0);
#endif sun4
d3643 9
d3689 5
@


1.6
log
@Taking out ifdef garbage and cleaning things up.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.5 89/04/21 23:33:16 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d32 1
d362 5
d502 4
a505 2
#ifdef sun4	/* while porting */
	    pte |= VMMACH_DONT_CACHE_BIT;
d511 4
a514 1
	    pte |= VMMACH_DONT_CACHE_BIT;
d646 1
a646 1
     * Invalidate all hardware segments from segment 1 up to the beginning
a727 4
#ifdef sun4				/* while porting */
		    VmMachSetPageMap(virtAddr,
			    (VmMachPTE)VMMACH_DONT_CACHE_BIT);
#else
a728 1
#endif sun4
d2150 6
a2155 1
		    pte |= VMMACH_DONT_CACHE_BIT;
d2207 3
d2218 3
d2242 6
a2247 1
	    hardPTE |= VMMACH_DONT_CACHE_BIT;
d2446 1
d2448 1
d2511 1
d2513 1
d2603 5
a2607 1
	    hardPTE |= VMMACH_DONT_CACHE_BIT;
@


1.5
log
@Twentieth Kernel:  I think we've finally squished the vm bugs that
were causing various user processes to die shortly after forking.

@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.4 89/03/06 12:01:22 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
a212 1
#ifdef sun4
a213 3
#else
    unsigned char 	     map[VMMACH_NUM_SEGS_PER_CONTEXT];
#endif /* sun4 */
a341 1
#ifdef sun4
a343 4
#else
    sysMachPtr = (VmMach_SegData *)Vm_BootAlloc(sizeof(VmMach_SegData) + 
					    VMMACH_NUM_SEGS_PER_CONTEXT);
#endif sun4
a445 1
#ifdef sun4
a446 3
#else
    register 	unsigned char	*segTablePtr;
#endif /* sun4 */
a459 1
#ifdef sun4
d462 2
a463 5
    {
	int	i;
	for (i = 0; i < VMMACH_NUM_SEGS_PER_CONTEXT; i++) {
	    sysMachPtr->segTablePtr[i] = VMMACH_INV_PMEG;
	}
a464 6
#else
    sysMachPtr->segTablePtr =
	    (unsigned char *) ((Address)sysMachPtr + sizeof(VmMach_SegData));
    ByteFill(VMMACH_INV_PMEG, VMMACH_NUM_SEGS_PER_CONTEXT,
	      (Address)sysMachPtr->segTablePtr);
#endif /* sun4 */
a587 1
#ifdef sun4
a588 3
#else
    register	unsigned char	*segTablePtr;
#endif /* sun4 */
d638 1
a638 3
#ifdef sun4
    for (; i <
	    ((((unsigned int) mach_KernStart) & VMMACH_ADDR_MASK) >>
d642 5
a653 5
#else
    for (; i < ((unsigned int)mach_KernStart >> VMMACH_SEG_SHIFT); i++) {
	VmMachSetSegMap((Address)(i << VMMACH_SEG_SHIFT), VMMACH_INV_PMEG);
    }
#endif sun4
a686 4
#ifdef NOTDEF
    /* I tried using this for sun4, but it's bad. */
    for (; i <= (((unsigned int) 0xffffffff) / VMMACH_SEG_SIZE); i++, segTablePtr++) {
#endif NOTDEF
d717 1
a717 1
#ifdef sun4
d746 1
a746 1
#if defined (sun3) 	/* Will this apply to sun4??? */
d797 1
d806 1
a806 3
#ifdef sun4
    segDataPtr = 
	(VmMach_SegData *)malloc(sizeof(VmMach_SegData) +
a807 4
#else
    segDataPtr = 
	(VmMach_SegData *)malloc(sizeof(VmMach_SegData) + segTableSize);
#endif sun4
a810 1
#ifdef sun4
d813 2
a814 5
    {
	int	i;
	for (i = 0; i < segTableSize; i++) {
	    segDataPtr->segTablePtr[i] = VMMACH_INV_PMEG;
	}
a815 5
#else
    segDataPtr->segTablePtr =
	    (unsigned char *) ((Address)segDataPtr + sizeof(VmMach_SegData));
    ByteFill(VMMACH_INV_PMEG, segTableSize, (Address)segDataPtr->segTablePtr);
#endif /* sun4 */
a823 1
#ifdef sun4
a824 3
#else
    segPtr->maxAddr = (Address)0x7fffffff;
#endif /* sun4 */
a865 1
#ifdef sun4
a868 4
#else
    newSegDataPtr->segTablePtr =
	    (unsigned char *) ((Address)newSegDataPtr + sizeof(VmMach_SegData));
#endif /* sun4 */
d897 2
a905 1
#ifdef sun4
d909 1
a909 4
	{
	    int	i;
	    int	j;
	    j = newSegDataPtr->numSegs - oldSegDataPtr->numSegs;
d911 3
a913 4
	    for (i = 0; i < j; i++) {
		newSegDataPtr->segTablePtr[oldSegDataPtr->numSegs + i]
			= VMMACH_INV_PMEG;
	    }
a914 7
#else
	bcopy((Address)oldSegDataPtr->segTablePtr,
		(Address)newSegDataPtr->segTablePtr, oldSegDataPtr->numSegs);
	ByteFill(VMMACH_INV_PMEG,
	  newSegDataPtr->numSegs - oldSegDataPtr->numSegs,
	  (Address)(newSegDataPtr->segTablePtr + oldSegDataPtr->numSegs));
#endif /* sun4 */
a919 1
#ifdef sun4
d924 1
a924 4
	{
	    int	i;
	    int	j;
	    j = newSegDataPtr->numSegs - oldSegDataPtr->numSegs;
d926 2
a927 3
	    for (i = 0; i < j; i++) {
		newSegDataPtr->segTablePtr[i] = VMMACH_INV_PMEG;
	    }
a928 9
#else
	bcopy((Address)oldSegDataPtr->segTablePtr,
	    (Address)(newSegDataPtr->segTablePtr + 
	    newSegDataPtr->numSegs - oldSegDataPtr->numSegs),
	    oldSegDataPtr->numSegs);
	ByteFill(VMMACH_INV_PMEG, 
		newSegDataPtr->numSegs - oldSegDataPtr->numSegs,
		(Address)newSegDataPtr->segTablePtr);
#endif /* sun4 */
a990 1
#ifdef sun4
a991 3
#else
    register	unsigned char 	*pmegPtr;
#endif
a996 1
#ifdef sun4
d998 1
a998 5
#else
    for (i = 0, pmegPtr = (unsigned int *) machPtr->segTablePtr;
#endif /* sun4 */
         i < machPtr->numSegs;
	 i++, pmegPtr++) {
a1167 4
#ifdef sun4
    ((List_Links *)pmegPtr)->prevPtr = (List_Links *) NIL;
    ((List_Links *)pmegPtr)->nextPtr = (List_Links *) NIL;
#endif sun4
a1168 6
#ifdef sun4
	if (((List_Links *)pmegPtr)->nextPtr != (List_Links *) NIL ||
		((List_Links *)pmegPtr)->prevPtr != (List_Links *) NIL) {
	    panic("PMEGGet: insertion of pmeg without NIL ptrs.\n");
	}
#endif sun4
d1210 1
d1217 1
a1217 1
#ifndef sun4
a1222 4
#ifdef sun4
	((List_Links *)pmegPtr)->prevPtr = (List_Links *) NIL;
	((List_Links *)pmegPtr)->nextPtr = (List_Links *) NIL;
#endif sun4
a1228 6
#ifdef sun4
    if (((List_Links *)pmegPtr)->nextPtr != (List_Links *)NIL ||
	    ((List_Links *)pmegPtr)->prevPtr != (List_Links *)NIL) {
	panic("PMEGGet: insertion of pmeg without NIL ptrs.\n");
    }
#endif sun4
a1389 1
#ifdef sun4
d1391 1
a1391 1
	    int	i;
d1393 1
a1398 5
#else
	ByteFill(VMMACH_INV_PMEG,
		  (int)((unsigned int)mach_KernStart >> VMMACH_SEG_SHIFT),
		  (Address)contextPtr->map);
#endif /* sun4 */
a1399 1
#ifdef sun4
d1403 1
a1403 5
#else
	bcopy((Address)segDataPtr->segTablePtr, 
	    (Address) (contextPtr->map + segDataPtr->offset),
	    segDataPtr->numSegs);
#endif /* sun4 */
a1404 1
#ifdef sun4
d1408 1
a1408 5
#else
	bcopy((Address)segDataPtr->segTablePtr, 
		(Address) (contextPtr->map + segDataPtr->offset),
		segDataPtr->numSegs);
#endif /* sun4 */
a1409 1
#ifdef sun4
d1413 1
a1413 5
#else
	bcopy((Address)segDataPtr->segTablePtr, 
		(Address) (contextPtr->map + segDataPtr->offset),
		segDataPtr->numSegs);
#endif /* sun4 */
a1658 1
#ifdef sun4
a1659 3
#else
    register unsigned char	*segTablePtr;
#endif /* sun4 */
d1726 1
a1726 5
#ifdef sun4				/* is this necessary for sun4? */
    netMemAddr += (numBytes + 7) & ~7;
#else
    netMemAddr += (numBytes + 3) & ~3;
#endif /* sun4 */
a2088 1
#ifdef sun4
a2089 3
#else
    register	unsigned char	*pmegNumPtr;
#endif
a2520 1
#ifdef sun4
a2521 3
#else
    register  unsigned  char	*segTablePtr;
#endif
a2544 1
#ifdef sun4
a2545 3
#else
    segTablePtr = (unsigned int *) GetHardSegPtr(segPtr->machPtr, hardSeg);
#endif /* sun4 */
a2569 4
#ifdef sun4
		((List_Links *)pmegPtr)->prevPtr = (List_Links *)NIL;
		((List_Links *)pmegPtr)->nextPtr = (List_Links *)NIL;
#endif sun4
a2720 6
#ifdef sun4
		if (((List_Links *)pmegPtr)->nextPtr != (List_Links *)NIL ||
			((List_Links *)pmegPtr)->prevPtr != (List_Links *)NIL) {
		    panic("PMEGGet: insertion of pmeg without NIL ptrs.\n");
		}
#endif sun4
@


1.4
log
@Sixteenth Kernel.  Kernel processes work and play tag over a monitor lock.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.3 89/03/03 15:40:55 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d346 4
d351 2
a352 1
						VMMACH_NUM_SEGS_PER_CONTEXT);
d669 6
a674 1
	VmMachSetSegMap((Address)(i << VMMACH_SEG_SHIFT), VMMACH_INV_PMEG);
d676 1
d716 2
a717 1
#ifdef sun4
d719 1
a719 1
#else
a720 1
#endif
d750 4
d755 1
d838 5
d845 2
d913 2
a914 1
	(VmMach_SegData *)malloc(sizeof(VmMach_SegData) + newSegTableSize);
d1257 4
d1262 6
d1309 7
a1315 1
#ifdef sun4
d1321 4
d1331 6
d1718 3
d1725 3
d1729 1
d1734 15
d1975 4
a1978 1
    VMMACH_ADDR_CHECK(virtAddr);
d2226 1
a2226 1
    register	unsigned int	*pmegNumPtr;
d2239 1
a2239 1
    pmegNumPtr = (unsigned int *) GetHardSegPtr(segPtr->machPtr, PageToSeg(firstPage)) - 1;
d2283 3
d2364 3
d2563 4
d2626 3
d2679 4
a2682 1
    VMMACH_ADDR_CHECK(addr);
d2718 4
d2835 4
a2838 1
    VMMACH_ADDR_CHECK(addr);
d2855 3
d2859 1
d2873 6
@


1.3
log
@Fifteenth Kernel.  Initializes all kernel procs and gets to idle loop.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.2 89/02/24 15:03:04 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
a39 6
#define	DEBUG_ADD(thing)	\
    if (debugCounter >= 500) {	\
	debugCounter = 0;	\
    }				\
    debugSpace[debugCounter++] = (int)(thing);

a1352 2
    DEBUG_ADD(0x999);
    DEBUG_ADD(procPtr);
a1355 2
	DEBUG_ADD(theAddrOfVmPtr);
	DEBUG_ADD(procPtr->vmPtr);
a1356 3
	DEBUG_ADD(theAddrOfMachPtr);
	DEBUG_ADD(procPtr->vmPtr->machPtr);
	DEBUG_ADD(contextPtr);
a1357 1
		DEBUG_ADD(0x10101);
a1358 1
		DEBUG_ADD(0x12121);
a1406 1
    DEBUG_ADD(0x4444);
a1408 1
    DEBUG_ADD(contextPtr);
a1410 1
	DEBUG_ADD(0x1);
a1416 1
	DEBUG_ADD(vmPtr->machPtr->contextPtr);
a1420 1
	DEBUG_ADD(0x555);
a1442 1
	DEBUG_ADD(contextPtr);
a1531 1
    DEBUG_ADD(0x555555);
a1533 1
    DEBUG_ADD(contextPtr);
a1567 1
    DEBUG_ADD(0x666666);
@


1.2
log
@Thirteenth Kernel.
sun4 finishes vm init now.
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun4.md/RCS/vmSun.c,v 1.1 89/02/23 12:58:43 mgbaker Exp Locker: mgbaker $ SPRITE (Berkeley)";
d32 14
a501 1
    Mach_MonPrintf("Finished MMUInit\n");
a1141 3
#ifdef sun4
    Mach_MonPrintf("PMEGGet with hardSeg = %x\n", hardSegNum);
#endif /* sun4 */
a1143 3
#ifdef sun4
	Mach_MonPrintf("pmegFreeList was empty\n");
#endif /* sun4 */
a1157 3
#ifdef sun4
    Mach_MonPrintf("pmegPtr = %x, pmegNum = %x\n", pmegPtr, pmegNum);
#endif /* sun4 */
a1159 3
#ifdef sun4
	Mach_MonPrintf("Have to steal pmeg\n");
#endif /* sun4 */
a1237 3
#ifdef sun4
    Mach_MonPrintf("Calling List_Remove of pmegPtr = %x\n", pmegPtr);
#endif /* sun4 */
a1239 3
#ifdef sun4
	Mach_MonPrintf("Calling List_Insert of pmeg to inuseList\n");
#endif /* sun4 */
a1241 3
#ifdef sun4
    Mach_MonPrintf("Setting flags to %x\n", flags);
#endif /* sun4 */
a1269 4
#ifdef sun4
    Mach_MonPrintf("PMEGFree with pmegNum = %x\n", pmegNum);
    Mach_MonPrintf("pmegPtr->flags = %x, pmegPtr->pageCount = %x\n", pmegPtr->flags, pmegPtr->pageCount);
#endif /* sun4 */
d1359 2
d1363 7
d1371 1
d1373 1
d1422 1
d1425 1
d1428 1
d1435 1
d1440 1
d1463 1
d1553 1
d1556 1
d1591 1
a1643 3
#ifdef sun4
    Mach_MonPrintf("MapIntelPage, virtAddr = %x, pmeg = %x\n", virtAddr, pmeg);
#endif /* sun4 */
a1649 3
#ifdef sun4
	Mach_MonPrintf("allocatedPMEG = %x\n", allocatedPMEG);
#endif /* sun4 */
a1653 3
#ifdef sun4
	Mach_MonPrintf("intelSavedPTE = %x\n", intelSavedPTE);
#endif /* sun4 */
a1662 1
    Mach_MonPrintf("intelPage = %x, pte = %x\n", intelPage, pte);
a1690 3
#ifdef sun4
    Mach_MonPrintf("UnmapIntelPage: virtAddr = %x, allocatedPMEG = %x\n", virtAddr, allocatedPMEG);
#endif /* sun4 */
a1700 3
#ifdef sun4
	Mach_MonPrintf("Freeing allocatedPMEG %x\n", allocatedPMEG);
#endif /* sun4 */
a1707 3
#ifdef sun4
	Mach_MonPrintf("Setting pagemap virtAddr %x, intelSavedPTE = %x\n", virtAddr, intelSavedPTE);
#endif /* sun4 */
a1709 3
#ifdef sun4
    Mach_MonPrintf("Page freeing intelSavedPTE = %x\n", intelSavedPTE);
#endif /* sun4 */
@


1.1
log
@Initial revision
@
text
@d13 1
a13 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/sun2.md/RCS/vmSun.c,v 8.8 89/01/11 14:12:56 nelson Exp Locker: jhh $ SPRITE (Berkeley)";
d205 3
d209 1
d442 3
d446 1
d460 1
d462 9
d474 1
a492 1
    Mach_MonPrintf("Finished bzeroing\n");
a518 1
    Mach_MonPrintf("Finished write-protecting\n");
a531 1
    Mach_MonPrintf("Finished invalidating\n");
a553 1
    Mach_MonPrintf("Finished copying to contexts\n");
d599 3
d603 1
a648 2
    Mach_MonPrintf("firstFreeSegment is %x\n", firstFreeSegment);
    Mach_MonPrintf("vmMachPMEGSegAddr is %x\n", vmMachPMEGSegAddr);
a672 1
	Mach_MonPrintf("pmeg %x for seg %x being reserved\n", pageCluster, i);
a682 1
    Mach_MonPrintf("i starts at %x\n", i);
a685 1
    Mach_MonPrintf("i ends at %x\n", i);
a698 1
    Mach_MonPrintf("i starts again at %d\n", i);
a715 1
		Mach_MonPrintf("virtAddr: %x, pte: %x\n", virtAddr, pte);
a729 1
			Mach_MonPrintf("pmeg for seg %x is reserved.\n", i);
a731 1
		    Mach_MonPrintf("page at addr %x gets 0 pte\n", virtAddr);
a748 1
		Mach_MonPrintf("Invalidating seg at virtAddr %x\n", virtAddr);
a754 1
    Mach_MonPrintf("i ends again at %d\n", i);
d756 1
a756 1
#if defined (sun3) || defined (sun4)		/* Will this apply to sun4??? */
a762 1
    Mach_MonPrintf("dontUse = %x\n", dontUse);
d774 1
a774 1
#if defined (sun3) || defined (sun4)
d819 1
d821 9
d832 1
d886 1
d888 4
d893 1
d929 1
d931 14
d949 1
d955 1
d959 14
d977 1
d1041 1
a1041 1
    register	unsigned int 	*pmegPtr;
d1050 3
d1054 1
d1129 3
d1133 4
d1151 3
d1156 3
d1237 3
d1242 3
d1247 3
d1278 4
d1293 3
d1297 1
d1464 10
d1477 1
d1479 1
d1482 4
d1487 1
d1489 1
d1492 4
d1497 1
d1499 1
d1502 4
d1507 1
d1636 3
d1645 3
d1652 3
d1664 1
d1693 3
d1706 3
d1716 3
d1721 3
d1753 3
d1757 1
d2612 1
a2612 1
    register  unsigned  int	*segTablePtr;
d2636 3
d2640 1
@
