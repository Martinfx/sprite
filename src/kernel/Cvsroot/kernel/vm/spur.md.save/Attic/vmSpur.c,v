head     8.9;
branch   ;
access   ;
symbols  sun4nw:8.9 symm:8.9 spur:8.9 newlib:8.0;
locks    ; strict;
comment  @ * @;


8.9
date     89.11.27.14.45.40;  author jhh;  state Exp;
branches ;
next     8.8;

8.8
date     89.09.18.17.38.59;  author shirriff;  state Exp;
branches ;
next     8.7;

8.7
date     89.09.06.15.19.07;  author shirriff;  state Exp;
branches ;
next     8.6;

8.6
date     89.07.23.18.34.15;  author jhh;  state Exp;
branches ;
next     8.5;

8.5
date     89.06.19.13.18.28;  author jhh;  state Exp;
branches ;
next     8.4;

8.4
date     89.03.15.17.25.24;  author david;  state Exp;
branches ;
next     8.3;

8.3
date     89.01.11.14.11.53;  author nelson;  state Exp;
branches ;
next     8.2;

8.2
date     88.12.28.19.58.16;  author nelson;  state Exp;
branches ;
next     8.1;

8.1
date     88.12.04.15.53.35;  author ouster;  state Exp;
branches ;
next     8.0;

8.0
date     88.11.11.18.45.17;  author douglis;  state Stable;
branches ;
next     1.22;

1.22
date     88.10.28.18.34.48;  author mlgray;  state Exp;
branches ;
next     1.21;

1.21
date     88.10.28.18.30.43;  author mendel;  state Exp;
branches ;
next     1.20;

1.20
date     88.10.24.09.22.16;  author mendel;  state Exp;
branches ;
next     1.19;

1.19
date     88.10.19.14.05.42;  author mendel;  state Exp;
branches ;
next     1.18;

1.18
date     88.09.27.18.06.04;  author nelson;  state Exp;
branches ;
next     1.17;

1.17
date     88.09.26.12.05.27;  author nelson;  state Exp;
branches ;
next     1.16;

1.16
date     88.09.12.10.15.32;  author mendel;  state Exp;
branches ;
next     1.15;

1.15
date     88.09.09.10.16.44;  author nelson;  state Exp;
branches ;
next     1.14;

1.14
date     88.09.07.14.04.59;  author mendel;  state Exp;
branches ;
next     1.13;

1.13
date     88.08.30.22.38.14;  author nelson;  state Exp;
branches ;
next     1.12;

1.12
date     88.08.30.20.10.14;  author nelson;  state Exp;
branches ;
next     1.11;

1.11
date     88.08.27.19.47.37;  author nelson;  state Exp;
branches ;
next     1.10;

1.10
date     88.08.27.15.17.34;  author nelson;  state Exp;
branches ;
next     1.9;

1.9
date     88.08.26.12.51.57;  author nelson;  state Exp;
branches ;
next     1.8;

1.8
date     88.08.25.22.37.16;  author nelson;  state Exp;
branches ;
next     1.7;

1.7
date     88.08.24.19.42.29;  author nelson;  state Exp;
branches ;
next     1.6;

1.6
date     88.08.23.11.14.26;  author nelson;  state Exp;
branches ;
next     1.5;

1.5
date     88.08.23.11.07.30;  author nelson;  state Exp;
branches ;
next     1.4;

1.4
date     88.08.23.10.14.29;  author nelson;  state Exp;
branches ;
next     1.3;

1.3
date     88.08.17.11.54.06;  author nelson;  state Exp;
branches ;
next     1.2;

1.2
date     88.07.31.22.51.50;  author mendel;  state Exp;
branches ;
next     1.1;

1.1
date     88.07.29.11.29.53;  author nelson;  state Exp;
branches ;
next     ;


desc
@@


8.9
log
@added VmMach_HandleSegMigration
@
text
@/* vmSpur.c --
 *
 *     	This file contains all hardware dependent routines for SPUR.
 *
 * Copyright (C) 1985 Regents of the University of California
 * All rights reserved.
 */

#ifndef lint
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.8 89/09/18 17:38:59 shirriff Exp Locker: jhh $ SPRITE (Berkeley)";
#endif not lint

#include "sprite.h"
#include "vmMach.h"
#include "vmMachInt.h"
#include "vm.h"
#include "vmInt.h"
#include "user/vm.h"
#include "user/spur.md/vmMach.h"
#include "machConfig.h"
#include "mach.h"
#include "list.h"
#include "stdlib.h"
#include "proc.h"
#include "sched.h"
#include "sync.h"
#include "sys.h"
#include "dbg.h"

/*
 * Number of page frames in system.
 */
static int	NumPageFrames;

/*
 * Structure for mapping virtual page frame numbers to physical page frame
 * numbers and back for each memory board.
 */
typedef struct {
    unsigned int	endVirPfNum;	/* Ending virtual page frame number
					 * on board. */
    unsigned int	startVirPfNum;  /* Starting virtual page frame number
					 * on board. */
    unsigned int	physStartAddr;	/* Physical address of page frame. */
    unsigned int	physEndAddr;	/* End Physical address of page frame.*/
} Memory_Board;

/*
 * Pointer to last configured Mboard structure.
 */
static Memory_Board   *LastMboard;
/*
 * Memory_Board structures for each board in system. This array is sorted
 * by endVirPfNum.
 */
static Memory_Board Mboards[16];
/*
 * Macros to translate from a virtual page to a physical page and back.
 */

#define	VirtToPhysPage(pfNum) ({\
	register Memory_Board 	*mb; \
	for (mb = Mboards; mb < LastMboard; mb++) { \
	    if (pfNum < mb->endVirPfNum) { \
		break; \
	    } \
	} \
	(mb->physStartAddr + pfNum - mb->startVirPfNum); })

#define	PhysToVirtPage(pfNum) ({\
	register Memory_Board 	*mb; \
	for (mb = Mboards; mb < LastMboard; mb++) { \
	    if (pfNum >= mb->physStartAddr && \
		pfNum < mb->physEndAddr) { \
		break; \
	    } \
	} \
	(pfNum - mb->physStartAddr + mb->startVirPfNum); })

/*
 * Macro to go from a virtual page number within a segment to the page
 * table entry for the page.
 */
#define	GetPageTablePtr(segDataPtr, virtPage) \
			    (segDataPtr->ptBasePtr + virtPage)

/*
 * Macro to go from a virtual page number within a segment page table to 
 * the page table entry which maps the page table.
 */
#define Get2ndPageTablePtr(segDataPtr, virtPage) \
			    (segDataPtr->pt2BasePtr + virtPage)

int	vmMachKernMemSize = 4096 * 1024;

VmMachPTE	*basePTPtr;	/* Pointer to base of the segment page
				 * tables. */
VmMachPTE	*kernPTPtr;	/* Pointer to kernel's page table. */
VmMachPTE	*kernPT2Ptr;	/* Pointer to page table that maps the kernel's
				 * page table. */
VmMachPTE	*kernPT3Ptr;	/* Pointer to page table that maps the page
				 * table that maps the kernel's page table. */
static int	rootPTPageNum;	/* The first physical page behind the root
				 * page table. */

/*
 * The kernel segments machine dependent data.
 */
VmMach_SegData	kernSegData;

/*
 * Machine dependent flags for the flags field in the Vm_VirtAddr struct.
 * We are only allowed to use the second byte of the flags.
 *
 *	USING_MAPPED_SEG		The parsed virtual address falls into
 *					the mapping segment.
 */
#define	USING_MAPPED_SEG	0x100

/*
 * Variable to tell whether have initialized VM or not.
 */
static Boolean initialized = FALSE;

/*
 * Variable to control whether we should use the simple or complex validation
 * of virtual addresses.  The simple one will only allow us to look at the
 * system segment.
 */
static Boolean useSimpleValidation = FALSE;

#ifdef lint
#ifndef volatile
#define volatile
#endif
#endif

/*
 * Variable to control reference bit usage.
 */
static Boolean flushOnRefBitClear = FALSE;
static Boolean useHardRefBit = TRUE;

/*
 * Variable to control coherency bit usage in page table entries.  If
 * TRUE all stack and heap segments have the VMMACH_COHERENCY_BIT bit set
 * and all reads become read for ownership.
 */
static Boolean ownStackAndHeap = TRUE;

/*
** Variable to control page flush policy.
** If TRUE then it flushes the page without doing a ReadAnyways.
** If FALSE then it first brings the page into the cache with a ReadAnyways
** then flushes it.
** THIS FLAG MUST NOT BE SET ON A MP SYSTEM!
*/
static Boolean uniprocessorFlushPage = FALSE;

/*
 * Variables to define kernel protection modes. If the ibuffer is on,
 * then everything has to be readable by the user. 
 */
static vmMachKroUnaProt = VMMACH_KRO_UNA_PROT;
static vmMachKrwUnaProt = VMMACH_KRW_UNA_PROT;


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_BootInit --
 *
 *      Do hardware dependent boot time initialization.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Pointers to kernel page tables set up.  Also the various size
 * 	fields are filled in.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_BootInit(pageSizePtr, pageShiftPtr, pageTableIncPtr, kernMemSizePtr,
		numKernPagesPtr, maxSegsPtr, maxProcessesPtr)
    int	*pageSizePtr;
    int	*pageShiftPtr;
    int	*pageTableIncPtr;
    int	*kernMemSizePtr;
    int	*numKernPagesPtr;
    int	*maxSegsPtr;
    int *maxProcessesPtr;
{
    ReturnStatus	status;
    int			boardNum;
    Mach_Board		board;
    int			nextVframeNum, numFrames;
    int			switches;
    /*
     * Initailize the memory boards.
     */
    LastMboard = Mboards;
    nextVframeNum = 0;
    for (boardNum = 0; boardNum < 16; boardNum++) {
	status = Mach_FindBoardDescription(MACH_CONFIG_MEMORY_MASK, boardNum,
					   TRUE, &board);
	if (status != SUCCESS) {
		break;
	} 
	numFrames = Mach_ConfigMemSize(board) / VMMACH_PAGE_SIZE;
	LastMboard->startVirPfNum = nextVframeNum;
	LastMboard->endVirPfNum = nextVframeNum + numFrames;
	nextVframeNum += numFrames;
	LastMboard->physStartAddr = (Mach_ConfigInitMem(board) >> 
						VMMACH_PAGE_SHIFT);
	LastMboard->physEndAddr = LastMboard->physStartAddr + numFrames;
	LastMboard++;
    }
    if (LastMboard == Mboards) {
	panic("No memory boards in system configuration.");
    }
    NumPageFrames = nextVframeNum;
    basePTPtr = (VmMachPTE *)VMMACH_KERN_PT_BASE;
    kernPTPtr = basePTPtr;
    kernPT2Ptr = (VmMachPTE *)VMMACH_KERN_PT2_BASE;
    kernPT3Ptr = (VmMachPTE *)((Address)kernPT2Ptr + VMMACH_SEG_PT2_SIZE / 4 * VMMACH_KERN_PT_QUAD);
    /*
     * Return lots of sizes to the machine independent module who called us.
     */
    *pageSizePtr = VMMACH_PAGE_SIZE;
    *pageShiftPtr = VMMACH_PAGE_SHIFT;
    *pageTableIncPtr = VMMACH_PAGE_TABLE_INCREMENT;
    *kernMemSizePtr = vmMachKernMemSize;
    *maxProcessesPtr = 128;
    *numKernPagesPtr = GetNumPages();
    /*
     * The number of useable segments by our caller is the maximum minus 1
     * because the last segment is intentionally left invalid to catch 
     * kernel errors.
     */
    *maxSegsPtr = VMMACH_NUM_SEGMENTS - 1;
    /*
     * If switch 2 is set then make everything user readable.
     */
    switches = read_physical_word(0x40000);
    if (switches & 0x2) {
	vmMachKroUnaProt = VMMACH_KRW_URO_PROT;
	vmMachKrwUnaProt = VMMACH_KRW_URO_PROT;
    }
}


/*
 * ----------------------------------------------------------------------------
 *
 * GetNumPages --
 *
 *     Determine how many pages of physical memory there are.
 *
 * Results:
 *     The number of physical pages.
 *
 * Side effects:
 *     None.
 *
 * ----------------------------------------------------------------------------
 */
int
GetNumPages()
{
    return(NumPageFrames);
}


/*
 * ---------------------------------------------------------------------------
 *
 * VmMach_AllocKernSpace --
 *
 *     Allocate memory for machine dependent stuff in the kernels VAS.
 *
 * Results:
 *     None.
 *
 * Side effects:
 *     None.
 *
 * ---------------------------------------------------------------------------
 */
Address
VmMach_AllocKernSpace(baseAddr)
    Address	baseAddr;
{
    return(baseAddr);
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_Init --
 *
 *     Initialize all virtual memory data structures.
 *
 * Results:
 *     None.
 *
 * Side effects:
 *     Kernel memory set up.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_Init(firstFreePage)
    int	firstFreePage;	/* Virtual page that is the first free for the 
			 * kernel. */
{
    VmMachPTE		*ptePtr;
    int 		i;
    int			pfNum;
    VmMach_SegData	*machPtr;
    extern	int	etext;
    int			firstCodePage;
    int			lastCodePage;

    /*
     * Unmap pages in low memory that aren't supposed to be accessible:
     *
     *	1) Page 0.
     *	2) The top of the main programs saved window stack.
     *	3) The top of the debuggers saved window stack.
     */
    ptePtr = basePTPtr;
    *ptePtr = 0;
    ptePtr = basePTPtr + ((unsigned)mach_StackBottom >> VMMACH_PAGE_SHIFT) + 1;
    *ptePtr = 0;
    ptePtr = basePTPtr + (MACH_DEBUG_STACK_BOTTOM >> VMMACH_PAGE_SHIFT) + 1;
    *ptePtr = 0;

    machPtr = &kernSegData;
    vm_SysSegPtr->machPtr = machPtr;
    machPtr->ptBasePtr = kernPTPtr;
    machPtr->pt2BasePtr = kernPT2Ptr;
    machPtr->firstPTPage = 0;
    machPtr->lastPTPage  = (((unsigned int)mach_KernEnd - 1) >> 
			   (VMMACH_PAGE_SHIFT + VMMACH_SEG_PT2_SHIFT));

    /*
     * Make sure that the page table that maps the page table is resident.
     */
    rootPTPageNum = -1;
    for (ptePtr = kernPT3Ptr, i = 0;
         i < VMMACH_NUM_PT3_PAGES;
	 ptePtr++, i++) {
	pfNum = GetPageFrame(*ptePtr);
	if (rootPTPageNum == -1) {
	    rootPTPageNum = pfNum;
	    machPtr->RPTPM = rootPTPageNum;
	}
	Vm_ReservePage((unsigned int)PhysToVirtPage(pfNum));
    }

    /*
     * Map enough of the kernel page tables to cover the page tables for
     * the maximum amount of the kernel VAS that can be in use.
     */
    for (ptePtr = kernPT2Ptr, i = 0;
	 i <= machPtr->lastPTPage;
	 ptePtr++, i++) {
	pfNum = GetPageFrame(*ptePtr);
	Vm_ReservePage((unsigned int)PhysToVirtPage(pfNum));
     }

    /*
     * Unmap the rest of the page table pages up to the root page table.
     */
    for (; ptePtr < kernPT3Ptr; ptePtr++, i++) {
	*ptePtr = (VmMachPTE)0;
    }
    ptePtr += VMMACH_NUM_PT3_PAGES;
    i += VMMACH_NUM_PT3_PAGES;
    /*
     * Unmap the rest of the page tables.
     */
    for (; i < VMMACH_NUM_PT_PAGES; ptePtr++, i++) {
	*ptePtr = (VmMachPTE)0;
    }
    bzero(((Address)kernPT2Ptr) + VMMACH_SEG_PT2_SIZE,
	    (VMMACH_NUM_SEGMENTS - 1) * VMMACH_SEG_PT2_SIZE);

    /*
     * Write protect the kernel code.
     */
    firstCodePage = MACH_CODE_START >> VMMACH_PAGE_SHIFT;
    lastCodePage = ((unsigned)&etext - VMMACH_PAGE_SIZE) >> VMMACH_PAGE_SHIFT;
    for (i = firstCodePage, ptePtr = kernPTPtr + firstCodePage;
	 i <= lastCodePage;
	 i++, ptePtr++) {
	VmMachPTE	pte;
	pte = *ptePtr; 
	pte &= ~VMMACH_PROTECTION_FIELD;
	pte |= vmMachKroUnaProt;
	*ptePtr = pte;
    }
         
    /*
     * Clear out the rest of kernel page tables starting with the first
     * free page.
     */
    for (ptePtr = kernPTPtr + firstFreePage, i = firstFreePage;
         i < (machPtr->lastPTPage + 1) * VMMACH_PTES_IN_PAGE; 
         ptePtr++, i++) {
	*ptePtr = (VmMachPTE) 0;
    }
    /*
     * Don't allow copy-on-write on SPUR.
     */
    vm_CanCOW = FALSE;
    /*
     * We have now initialized ourselves.
     */
    initialized = TRUE;
}

/*
 * ----------------------------------------------------------------------------
 *
 * The following routines deal with allocating, deleting and initializing
 * segments.  These are all synchronized by the caller in the machine
 * independent module.  It is up to it to ensure that only one operation
 * on a segment happens at a time.  We can't synchronize ourselves because
 * we have to call into the machine independent module to allocate and
 * free memory for the kernel page tables.
 *
 * ----------------------------------------------------------------------------
 */


/*
 * ----------------------------------------------------------------------------
 *
 * AllocPageTable --
 *
 *	Allocate kernel memory for a page table for the given segment.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The kernel's page table is modified to map the page table for the
 *	given segment.
 *
 * ----------------------------------------------------------------------------
 */
AllocPageTable(segPtr, offset, numPages)
    register	Vm_Segment	*segPtr;	/* Segment to alloc PT for. */
    unsigned int		offset;		/* Page offset within hardware
						 * segment. */
    int				numPages;	/* Number of pages in segment
						 * from offset. */
{
    register	VmMach_SegData	*segDataPtr;
    register	VmMachPTE	*ptePtr;
    register	int		firstPTPage;
    int				lastPTPage;
    Address			ptAddr;
    unsigned int		pfNum;


    segDataPtr = segPtr->machPtr;
    /*
     * Determine first and last page table pages to validate.
     */
    firstPTPage = offset >> VMMACH_SEG_PT2_SHIFT;
    lastPTPage = (offset + numPages - 1) >> VMMACH_SEG_PT2_SHIFT;
    if (firstPTPage < segDataPtr->firstPTPage) {
	segDataPtr->firstPTPage = firstPTPage;
    }
    if (lastPTPage > segDataPtr->lastPTPage) {
	segDataPtr->lastPTPage = lastPTPage;
    }
    /*
     * Validate the appropriate page table pages.
     */
    ptAddr = (Address)(segDataPtr->ptBasePtr + 
			    (firstPTPage << VMMACH_SEG_PT2_SHIFT));
    for (ptePtr = Get2ndPageTablePtr(segDataPtr, firstPTPage);
	 firstPTPage <= lastPTPage;
	 firstPTPage++, ptePtr++, ptAddr += VMMACH_PAGE_SIZE) {
	if (!(*ptePtr & VMMACH_RESIDENT_BIT)) {
	    pfNum = Vm_KernPageAllocate();
	    *ptePtr = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT | 
	              vmMachKrwUnaProt | VMMACH_REFERENCED_BIT |
		      VMMACH_MODIFIED_BIT | SetPageFrame(VirtToPhysPage(pfNum));
	    bzero(ptAddr, VMMACH_PAGE_SIZE);
	}
     }
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SegInit --
 *
 *      Initialize the hardware page table for the segment.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Machine dependent data struct is allocated and initialized.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_SegInit(segPtr)
    register	Vm_Segment	*segPtr;
{
    register	VmMach_SegData	*segDataPtr;
    int			        segOffset;
    int				numPages;

    segPtr->minAddr = (Address)(segPtr->type * VMMACH_SEG_SIZE);
    segPtr->maxAddr = (Address)(segPtr->minAddr + (VMMACH_SEG_SIZE - 1));
    if (segPtr->machPtr == (VmMach_SegData *) NIL) {
	segDataPtr = (VmMach_SegData *)malloc(sizeof(VmMach_SegData));
	segPtr->machPtr = segDataPtr;
	segDataPtr->ptBasePtr = 
		    basePTPtr + segPtr->segNum * (VMMACH_SEG_PT_SIZE / 4);
	segDataPtr->pt2BasePtr =
		    kernPT2Ptr + segPtr->segNum * (VMMACH_SEG_PT2_SIZE / 4); 
	segDataPtr->RPTPM = rootPTPageNum + segPtr->segNum / 4;
    } else {
	segDataPtr = segPtr->machPtr;
    }

    segDataPtr->firstPTPage = 0x7fffffff;
    segDataPtr->lastPTPage = -1;

    segOffset = segPtr->offset - segPtr->type * VMMACH_PAGES_PER_SEG;

    if (segOffset < 0) {
	/*
	 * HACK to allow us to work on SPUR when Proc_KernExec allocates
	 * dummy segments with ridiculous offsets.  The real patch
	 * is in Proc_KernExec not here.
	 */
	segOffset = 0;
    }
    if (segPtr->type == VM_STACK) {
	numPages = mach_LastUserStackPage - segPtr->offset + 1;
    } else {
	numPages = segPtr->numPages;
    }
    AllocPageTable(segPtr, (unsigned int)segOffset, numPages);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_SegExpand --
 *
 *	Allocate more space for the machine dependent structure.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_SegExpand(segPtr, firstPage, lastPage)
    register	Vm_Segment	*segPtr;	/* Segment to expand. */
    int				firstPage;	/* First page to add. */
    int				lastPage;	/* Last page to add. */
{
    AllocPageTable(segPtr, 
	   (unsigned int)(firstPage - segPtr->type * VMMACH_PAGES_PER_SEG),
		   lastPage - firstPage + 1);
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SegDelete --
 *
 *      Unmap the page table.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Page table and machine dependent struct for the segment is freed.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_SegDelete(segPtr)
    Vm_Segment	*segPtr;    /* Pointer to segment to free. */
{
    register	VmMachPTE	*ptePtr;
    register	VmMach_SegData	*segDataPtr;
    register	int		firstPage;
    Vm_VirtAddr			virtAddr;
    unsigned int		pfNum;

    segDataPtr = segPtr->machPtr;
    virtAddr.segPtr = vm_SysSegPtr;
    virtAddr.page = (unsigned int)(segDataPtr->ptBasePtr +
		       (segDataPtr->firstPTPage << VMMACH_SEG_PT2_SHIFT)) >>
			    VMMACH_PAGE_SHIFT;

    for (ptePtr = Get2ndPageTablePtr(segDataPtr, segDataPtr->firstPTPage),
	    firstPage = segDataPtr->firstPTPage;
	 firstPage <= segDataPtr->lastPTPage;
	 firstPage++, ptePtr++, virtAddr.page++) {
	if (*ptePtr & VMMACH_RESIDENT_BIT) {
	    /*
	     * The page table page was resident so free it and flush it out
	     * of the cache.
	     */
	    VmMach_FlushPage(&virtAddr, TRUE);
	    pfNum = GetPageFrame(*ptePtr);
	    Vm_KernPageFree((unsigned)PhysToVirtPage(pfNum));
	    *ptePtr = 0;
	}
     }
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_VirtAddrParse --
 *
 *	Determine which segment the virtual address falls into.
 *
 * Results:
 *	TRUE because can always parse the virtual address.
 *
 * Side effects:
 *	*transVirtAddrPtr is filled in.
 *
 *----------------------------------------------------------------------
 */
Boolean
VmMach_VirtAddrParse(procPtr, virtAddr, transVirtAddrPtr)
    Proc_ControlBlock		*procPtr;
    Address			virtAddr;
    register	Vm_VirtAddr	*transVirtAddrPtr;
{
    register	VmMach_ProcData	*procDataPtr;
    Address			origVirtAddr;

    procDataPtr = procPtr->vmPtr->machPtr;
    if (procDataPtr->segRegMask == (unsigned)-1) {
	return(FALSE);
    }
    if (((unsigned)virtAddr & VMMACH_SEG_REG_MASK) == 
					procDataPtr->mappedSegMask) {
	origVirtAddr = (Address) (((unsigned)virtAddr & ~VMMACH_SEG_REG_MASK) |
				  procDataPtr->segRegMask);
	transVirtAddrPtr->segPtr = procDataPtr->mapSegPtr;
	transVirtAddrPtr->page = (unsigned) (origVirtAddr) >> VMMACH_PAGE_SHIFT;
	transVirtAddrPtr->offset = (unsigned)virtAddr & VMMACH_OFFSET_MASK;
	transVirtAddrPtr->flags |= USING_MAPPED_SEG;
	return(TRUE);
    } else {
	return(FALSE);
    }
}

/*
 *----------------------------------------------------------------------
 *
 * The following routines work on a per-process state that only be modified
 * by the currently executing process.  Thus no synchronization is required.
 *
 *----------------------------------------------------------------------
 */


/*
 *----------------------------------------------------------------------
 *
 * VmMach_ProcInit --
 *
 *	Initalize the machine dependent part of the VM proc info.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Machine dependent proc info is initialized.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_ProcInit(vmPtr)
    register	Vm_ProcInfo	*vmPtr;
{
    register	VmMach_ProcData	*machPtr;

    if (vmPtr->machPtr == (VmMach_ProcData *)NIL) {
	vmPtr->machPtr = (VmMach_ProcData *)malloc(sizeof(VmMach_ProcData));
    }
    machPtr = vmPtr->machPtr;
    machPtr->segNums[VM_SYSTEM] = VM_SYSTEM_SEGMENT;
    machPtr->segNums[VM_CODE] = -1;
    machPtr->segNums[VM_HEAP] = -1;
    machPtr->segNums[VM_STACK] = -1;
    machPtr->RPTPMs[VM_SYSTEM] = rootPTPageNum;
    machPtr->segRegMask = -1;
}


/*
 * ----------------------------------------------------------------------------
 *
 * SetupContext --
 *
 *      Setup context for the given process that is executing on the 
 *	current processor.
 *	
 * Results:
 *      None.
 *
 * Side effects:
 *      Segment register values are modified.
 *
 * ----------------------------------------------------------------------------
 */
void
SetupContext(procPtr) 
    Proc_ControlBlock	*procPtr;
{
    register	Vm_Segment	**segPtrArray;
    register	VmMach_ProcData	*machPtr;
    register	int		*segNums;
    register	int		*RPTPMs;

    machPtr = procPtr->vmPtr->machPtr;
    segNums = machPtr->segNums;
    RPTPMs = machPtr->RPTPMs;
    if (segNums[VM_CODE] == -1) {
	segPtrArray = procPtr->vmPtr->segPtrArray;
	/*
	 * The segment number array has not been initialized yet.
	 */
	if (segPtrArray[VM_CODE] == (Vm_Segment *)NIL) {
	    int	invRPTPM;

	    invRPTPM = VMMACH_INVALID_SEGMENT / 4 + rootPTPageNum;
	    segNums[VM_CODE] = VMMACH_INVALID_SEGMENT;
	    RPTPMs[VM_CODE] = invRPTPM;
	    segNums[VM_HEAP] = VMMACH_INVALID_SEGMENT;
	    RPTPMs[VM_HEAP] =  invRPTPM;
	    segNums[VM_STACK] = VMMACH_INVALID_SEGMENT;
	    RPTPMs[VM_STACK] = invRPTPM;
	} else {
	    segNums[VM_CODE] = segPtrArray[VM_CODE]->segNum;
	    RPTPMs[VM_CODE] = segPtrArray[VM_CODE]->machPtr->RPTPM;
	    segNums[VM_HEAP] = segPtrArray[VM_HEAP]->segNum;
	    RPTPMs[VM_HEAP] = segPtrArray[VM_HEAP]->machPtr->RPTPM;
	    segNums[VM_STACK] = segPtrArray[VM_STACK]->segNum;
	    RPTPMs[VM_STACK] = segPtrArray[VM_STACK]->machPtr->RPTPM;
	}
    }
    VmMachSetSegRegisters(segNums, RPTPMs);
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SetupContext --
 *
 *      Setup context for the new process.
 *	
 * Results:
 *      None.
 *
 * Side effects:
 *      Context set up for the process.
 *
 * ----------------------------------------------------------------------------
 */
/* ARGSUSED */
void
VmMach_SetupContext(destProcPtr)
    Proc_ControlBlock	*destProcPtr;
{
    SetupContext(destProcPtr);
}


/*
 * ----------------------------------------------------------------------------
 *
 * Vm_FreeContext --
 *
 *      Release the context of the process.  This just entails clearing
 *	out the segment register value for the code segment.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Segment register value for the CODE segment in the machine dependent
 *	part of the process specific VM info struct is set to -1
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_FreeContext(procPtr)
    register	Proc_ControlBlock	*procPtr;
{
    procPtr->vmPtr->machPtr->segNums[VM_CODE] = -1;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_ReinitContext --
 *
 *      Free the current context and set up another one.  This is called
 *	by routines such as Proc_Exec that add things to the context and
 *	then have to abort or start a process running with a new image.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Context is set up for the currently executing process.
 *
 * ----------------------------------------------------------------------------
 */
void
VmMach_ReinitContext(procPtr)
    register	Proc_ControlBlock	*procPtr;
{
    VmMach_FreeContext(procPtr);
    SetupContext(procPtr);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_CopyInProc --
 *
 *	Copy from another processes address space into the current address
 *	space.   This is done by mapping the other processes segment into
 *	the current VAS and then doing the copy.  It assumed that this 
 *	routine is called with the source process locked such that its
 *	VM will not go away while we are doing this copy.
 *
 * Results:
 *	SUCCESS if the copy succeeded, SYS_ARG_NOACCESS if fromAddr is invalid.
 *
 * Side effects:
 *	What toAddr points to is modified.
 *
 *----------------------------------------------------------------------
 */
/* ARGSUSED */
ReturnStatus
VmMach_CopyInProc(numBytes, fromProcPtr, fromAddr, virtAddrPtr,
	          toAddr, toKernel)
    int 	numBytes;		/* The maximum number of bytes to 
					   copy in. */
    Proc_ControlBlock	*fromProcPtr;	/* Which process to copy from.*/
    Address		fromAddr;	/* The address to copy from */
    Vm_VirtAddr		*virtAddrPtr;
    Address		toAddr;		/* The address to copy to */
    Boolean		toKernel;	/* This copy is happening to the
					 * kernel's address space. */
{
    register VmMach_ProcData	*machPtr;
    register int		*segNums;
    register int		*RPTPMs;
    Proc_ControlBlock		*toProcPtr;
    ReturnStatus		status = SUCCESS;
    int				segToUse;
    int				savedSegReg;
    int				savedRPTPM;

    toProcPtr = Proc_GetCurrentProc();
    machPtr = toProcPtr->vmPtr->machPtr;
    machPtr->mapSegPtr = virtAddrPtr->segPtr;
    machPtr->segRegMask = (unsigned int) (fromAddr) & VMMACH_SEG_REG_MASK;
    /*
     * Determine where to map in the source segment.
     */
    fromAddr = (Address) ((unsigned int)fromAddr & ~VMMACH_SEG_REG_MASK);
    if (((unsigned)toAddr & VMMACH_SEG_REG_MASK) == 
	 (unsigned)VMMACH_STACK_SEG_START) {
	fromAddr = (Address) ((unsigned int)fromAddr | VMMACH_HEAP_SEG_START);
	machPtr->mappedSegMask = VMMACH_HEAP_SEG_START;
	segToUse = VM_HEAP;
    } else {
	fromAddr = (Address) ((unsigned int)fromAddr | VMMACH_STACK_SEG_START);
	segToUse = VM_STACK;
	machPtr->mappedSegMask = VMMACH_STACK_SEG_START;
    }

    /*
     * Map the source segment in.
     */
    segNums = machPtr->segNums;
    RPTPMs = machPtr->RPTPMs;
    savedSegReg = segNums[segToUse];
    savedRPTPM = RPTPMs[segToUse];
    segNums[segToUse] = virtAddrPtr->segPtr->segNum;
    RPTPMs[segToUse] = segNums[segToUse] / 4 + rootPTPageNum;
    VmMachSetSegRegisters(segNums, RPTPMs);
    /*
     * Copy the data in.
     */
    status = VmMachDoCopy(numBytes, fromAddr, toAddr);
    /*
     * Clean up.
     */
    segNums[segToUse] = savedSegReg;
    RPTPMs[segToUse] = savedRPTPM;

    VmMachSetSegRegisters(segNums, RPTPMs);

    machPtr->segRegMask = -1;

    return(status);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_CopyOutProc --
 *
 *	Copy from the current VAS to another processes VAS.  This is done by 
 *	mapping the other processes segment into the current VAS and then 
 *	doing the copy.  It assumed that this routine is called with the dest
 *	process locked such that its VM will not go away while we are doing
 *	the copy.
 *
 * Results:
 *	SUCCESS if the copy succeeded, SYS_ARG_NOACCESS if fromAddr is invalid.
 *
 * Side effects:
 *	What toAddr points to is modified.
 *
 *----------------------------------------------------------------------
 */
/* ARGSUSED */
ReturnStatus
VmMach_CopyOutProc(numBytes, fromAddr, fromKernel, toProcPtr, toAddr,
		   virtAddrPtr)
    int 		numBytes;	/* The maximum number of bytes to 
					   copy in. */
    Address		fromAddr;	/* The address to copy from */
    Boolean		fromKernel;	/* This copy is happening to the
					 * kernel's address space. */
    Proc_ControlBlock	*toProcPtr;	/* Which process to copy from.*/
    Address		toAddr;		/* The address to copy to */
    Vm_VirtAddr		*virtAddrPtr;
{
    register VmMach_ProcData	*machPtr;
    register int		*segNums;
    register int		*RPTPMs;
    Proc_ControlBlock		*fromProcPtr;
    ReturnStatus		status = SUCCESS;
    int				segToUse;
    int				savedSegReg;
    int				savedRPTPM;

    fromProcPtr = Proc_GetCurrentProc();
    machPtr = fromProcPtr->vmPtr->machPtr;
    machPtr->mapSegPtr = virtAddrPtr->segPtr;
    machPtr->segRegMask = (unsigned int) (toAddr) & VMMACH_SEG_REG_MASK;
    /*
     * Determine where to map in the source segment.
     */
    toAddr = (Address) ((unsigned)toAddr & ~VMMACH_SEG_REG_MASK);
    if (((unsigned)fromAddr & VMMACH_SEG_REG_MASK) == 
	(unsigned)VMMACH_STACK_SEG_START) {
        toAddr = (Address) ((unsigned)toAddr | VMMACH_HEAP_SEG_START);
	machPtr->mappedSegMask = VMMACH_HEAP_SEG_START;
	segToUse = VM_HEAP;
    } else {
        toAddr = (Address) ((unsigned)toAddr | VMMACH_STACK_SEG_START);
	machPtr->mappedSegMask = VMMACH_STACK_SEG_START;
	segToUse = VM_STACK;
    }

    /*
     * Map the source segment in.
     */
    segNums = machPtr->segNums;
    RPTPMs = machPtr->RPTPMs;
    savedSegReg = segNums[segToUse];
    savedRPTPM = RPTPMs[segToUse];
    segNums[segToUse] = virtAddrPtr->segPtr->segNum;
    RPTPMs[segToUse] = segNums[segToUse] / 4 + rootPTPageNum;
    VmMachSetSegRegisters(segNums, RPTPMs);
    /*
     * Copy the data in.
     */
    status = VmMachDoCopy(numBytes, fromAddr, toAddr);
    /*
     * Clean up.
     */
    segNums[segToUse] = savedSegReg;
    RPTPMs[segToUse] = savedRPTPM;
    VmMachSetSegRegisters(segNums, RPTPMs);

    machPtr->segRegMask = -1;

    return(status);
}

/*
 *----------------------------------------------------------------------
 *
 * The following routines read and write segments page tables. These
 * are underneath a monitor lock in order to synchronize access to these
 * structures.  We don't rely on the machine indepenent module to synchronize
 * for us because we may be called directly by the routines in the mach
 * module.
 *
 *----------------------------------------------------------------------
 */

Sync_Lock		ptLock = Sync_LockInitStatic("vm:ptLock");
#define	LOCKPTR		&ptLock


/*
 *----------------------------------------------------------------------
 *
 * VmMach_SetSegProt --
 *
 *	Change the protection in the page table for the given range of bytes
 *	for the given segment.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Page table may be modified for the segment.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_SetSegProt(segPtr, firstPage, lastPage, makeWriteable)
    register Vm_Segment		*segPtr;    /* Segment to change protection
					       for. */
    register int		firstPage;  /* First page to set protection
					     * for. */
    int				lastPage;   /* Last page to set protection
					     * for. */
    Boolean			makeWriteable;/* TRUE => make the pages 
					       *	 writable.
					       * FALSE => make readable only.*/
{
    register	VmMachPTE	*ptePtr;
    register	VmMach_SegData	*segDataPtr;
    VmMachPTE			machPTE;

    LOCK_MONITOR;

    Sync_LockRegister(&ptLock);

    segDataPtr = segPtr->machPtr;
    firstPage &= ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    lastPage &= ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, firstPage);
    for (; firstPage <= lastPage; firstPage++, ptePtr++) {
	if (*ptePtr & VMMACH_RESIDENT_BIT) {
	    machPTE = *ptePtr;
	    machPTE &= ~VMMACH_PROTECTION_FIELD;
	    machPTE |= 
		makeWriteable ? VMMACH_KRW_URW_PROT : VMMACH_KRW_URO_PROT;
	    *ptePtr = machPTE;
	    VmMachFlushPage(segPtr, firstPage, FALSE);
	}
    }

    UNLOCK_MONITOR;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_SetPageProt --
 *
 *	Set the protection in hardware and software for the given virtual
 *	page.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	Page table may be modified for the segment.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_SetPageProt(virtAddrPtr, softPTE)
    register	Vm_VirtAddr	*virtAddrPtr;	/* The virtual page to set the
						 * protection for.*/
    Vm_PTE			softPTE;	/* Software pte. */
{
    register	VmMach_SegData	*segDataPtr;
    register	VmMachPTE	*ptePtr;
    register	Vm_Segment	*segPtr;
    unsigned int		page;
    VmMachPTE			machPTE;

    LOCK_MONITOR;

    segPtr = virtAddrPtr->segPtr;
    segDataPtr = segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    machPTE = *ptePtr;
    machPTE &= ~VMMACH_PROTECTION_FIELD;
    machPTE |= (softPTE & (VM_COW_BIT | VM_READ_ONLY_PROT)) ? 
				VMMACH_KRW_URO_PROT : VMMACH_KRW_URW_PROT;
    *ptePtr = machPTE;
    VmMachFlushPage(segPtr, page, FALSE);

    UNLOCK_MONITOR;
}

/*
 *----------------------------------------------------------------------
 *
 * VmMach_SetProtForDbg --
 *
 *      Set the protection of the kernel pages for the debugger.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The protection is set for the given range of kernel addresses.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_SetProtForDbg(readWrite, numBytes, addr)
    Boolean     readWrite;      /* TRUE if should make pages writable, FALSE
                                 * if should make read-only. */
    int         numBytes;       /* Number of bytes to change protection for. */
    Address     addr;           /* Address to start changing protection at. */
{
    register	VmMachPTE	*ptePtr;
    register    VmMachPTE       pte;
    register    int             firstPage;
    register    int             lastPage;

    firstPage = (unsigned)addr >> VMMACH_PAGE_SHIFT;
    lastPage = ((unsigned)addr + numBytes - 1) >> VMMACH_PAGE_SHIFT;
    for (ptePtr = kernPTPtr + firstPage; 
	 firstPage <= lastPage;
	 firstPage++, ptePtr++) {
        pte = *ptePtr;
        pte &= ~VMMACH_PROTECTION_FIELD;
        pte |= readWrite ? vmMachKrwUnaProt : vmMachKroUnaProt;
        *ptePtr = pte;
    }
    /*
     * Flush the range from the cache so new protect will be read.  This
     * code should work before the Vm module is initialized.
     */
    while (numBytes > 0) { 
        VmMachFlushBlock(
		(Address)((unsigned)addr & ~(VMMACH_CACHE_LINE_SIZE-1)));
	addr += VMMACH_CACHE_LINE_SIZE;
	numBytes -= VMMACH_CACHE_LINE_SIZE;
    }
    /*
     * Flush the following cache block in case the range slops over into it.
     * For example addr = 0x1f numBytes = 2.
     */
    VmMachFlushBlock(
		(Address)((unsigned)addr & ~(VMMACH_CACHE_LINE_SIZE-1)));
   
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_AllocCheck --
 *
 *      Determine if this page can be reallocated.  A page can be reallocated
 *	if it has not been referenced or modified.
 *  
 * Results:
 *      None.
 *
 * Side effects:
 *      The given page will be invalidated in the hardware if it has not
 *	been referenced and *refPtr and *modPtr will have the hardware 
 *	reference and modify bits or'd in.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ENTRY void
VmMach_AllocCheck(virtAddrPtr, virtFrameNum, refPtr, modPtr)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned	int		virtFrameNum;
    register	Boolean		*refPtr;
    register	Boolean		*modPtr;
{
    register VmMach_SegData	*segDataPtr;
    int				page;
    VmMachPTE			*ptePtr;

        LOCK_MONITOR;

    segDataPtr = virtAddrPtr->segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    if (useHardRefBit) {
	*refPtr |= *ptePtr & VMMACH_REFERENCED_BIT;
    }
    *modPtr |= *ptePtr & VMMACH_MODIFIED_BIT;
    if (!*refPtr) {
	/*
	 * Invalidate the page so that it will force a fault if it is
	 * referenced.  Since our caller has blocked all faults on this
	 * page, by invalidating it we can guarantee that the reference and
	 * modify information that we are returning will be valid until
	 * our caller reenables faults on this page.
	 */
	VmMachFlushPage(virtAddrPtr->segPtr, page, TRUE);
	*ptePtr = 0;
	if (*modPtr && !(*ptePtr & VMMACH_MODIFIED_BIT)) {
	    /*
	     * This page had the modify bit set in software but not in
	     * hardware.
	     */
	    vmStat.notHardModPages++;
	}
    }

    UNLOCK_MONITOR;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_GetRefModBits --
 *
 *      Pull the reference and modified bits out of hardware.
 *  
 * Results:
 *      None.
 *
 * Side effects:
 *      
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ENTRY void
VmMach_GetRefModBits(virtAddrPtr, virtFrameNum, refPtr, modPtr)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned	int		virtFrameNum;
    register	Boolean		*refPtr;
    register	Boolean		*modPtr;
{
    register VmMach_SegData	*segDataPtr;
    int				page;
    VmMachPTE			*ptePtr;

    LOCK_MONITOR;

    segDataPtr = virtAddrPtr->segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    if (useHardRefBit) {
	*refPtr = *ptePtr & VMMACH_REFERENCED_BIT;
    } else {
	*refPtr = FALSE;
    }
    *modPtr = *ptePtr & VMMACH_MODIFIED_BIT;

    UNLOCK_MONITOR;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_ClearRefBit --
 *
 *      Clear the reference bit at the given virtual address.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Hardware reference bit cleared.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ENTRY void
VmMach_ClearRefBit(virtAddrPtr, virtFrameNum)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned 	int		virtFrameNum;
{
    register VmMach_SegData	*segDataPtr;
    VmMachPTE			*ptePtr;
    unsigned int		page;

    LOCK_MONITOR;

    if (useHardRefBit) {
	segDataPtr = virtAddrPtr->segPtr->machPtr;
	page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
	ptePtr = GetPageTablePtr(segDataPtr, page);
	*ptePtr &= ~VMMACH_REFERENCED_BIT;
	if (flushOnRefBitClear) {
	    VmMachFlushPage(virtAddrPtr->segPtr, page, FALSE);
	}	
    }

    UNLOCK_MONITOR;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SetRefBit --
 *
 *      Set the reference bit at the given address.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Hardware reference bit set.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_SetRefBit(addr)
    Address	addr;
{
    register VmMach_SegData	*segDataPtr;
    int				page;
    Vm_Segment			*segPtr;
    VmMachPTE			*ptePtr;
    Proc_ControlBlock		*procPtr;
    register	VmMach_ProcData	*procDataPtr;
    VmMachPTE	machPTE = 0;

    LOCK_MONITOR;

    vmStat.machDepStat.refBitFaults++;

    procPtr = Proc_GetCurrentProc();
    procDataPtr = procPtr->vmPtr->machPtr;
    if (procDataPtr->segRegMask != (unsigned)-1 &&
	((unsigned)addr & VMMACH_SEG_REG_MASK) == procDataPtr->mappedSegMask) {
	segPtr = procDataPtr->mapSegPtr;
    } else {
	segPtr = procPtr->vmPtr->segPtrArray[(unsigned int)addr >> 
						VMMACH_SEG_REG_SHIFT];
    }
    if (segPtr != (Vm_Segment *) NIL) {
        segDataPtr = segPtr->machPtr;
        page = ((unsigned int)(addr) & ~VMMACH_SEG_REG_MASK) >> 
							VMMACH_PAGE_SHIFT;
        ptePtr = GetPageTablePtr(segDataPtr, page);
	machPTE = *ptePtr;
	if (*ptePtr & VMMACH_RESIDENT_BIT) {
	    *ptePtr |= VMMACH_REFERENCED_BIT;
	    if ((vmWriteableRefPageout || vmWriteablePageout) && 
		segPtr->type != VM_CODE) {
		*ptePtr |= VMMACH_MODIFIED_BIT;
	    }
	}
    }

    UNLOCK_MONITOR;
}

/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_ClearModBit --
 *
 *      Clear the modified bit at the given virtual address.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      Hardware modified bit cleared.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ENTRY void
VmMach_ClearModBit(virtAddrPtr, virtFrameNum)
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned	int		virtFrameNum;
{
    register VmMach_SegData	*segDataPtr;
    unsigned int		page;
    VmMachPTE			*ptePtr;

    LOCK_MONITOR;

    segDataPtr = virtAddrPtr->segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    if ((!vmWriteableRefPageout && !vmWriteablePageout) || 
	!(*ptePtr & VMMACH_REFERENCED_BIT)) {
	*ptePtr &= ~VMMACH_MODIFIED_BIT;
	VmMachFlushPage(virtAddrPtr->segPtr, page, FALSE);
    }

    UNLOCK_MONITOR;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_SetModBit --
 *
 *      Set the modified bit at the given address.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *	Hardware modified bit set.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_SetModBit(addr)
    Address	addr;
{
    register VmMach_SegData	*segDataPtr;
    VmMachPTE			*ptePtr;
    int				page;
    Vm_Segment			*segPtr;
    Proc_ControlBlock		*procPtr;
    register	VmMach_ProcData	*procDataPtr;

    LOCK_MONITOR;

    vmStat.machDepStat.dirtyBitFaults++;
    procPtr = Proc_GetCurrentProc();
    procDataPtr = procPtr->vmPtr->machPtr;
    if (procDataPtr->segRegMask != (unsigned)-1 &&
	((unsigned)addr & VMMACH_SEG_REG_MASK) == procDataPtr->mappedSegMask) {
	segPtr = procDataPtr->mapSegPtr;
    } else {
	segPtr = procPtr->vmPtr->segPtrArray[(unsigned int)addr >> 
						VMMACH_SEG_REG_SHIFT];
    }
    segDataPtr = segPtr->machPtr;
    page = ((unsigned int)(addr) & ~VMMACH_SEG_REG_MASK) >> VMMACH_PAGE_SHIFT;
    ptePtr = GetPageTablePtr(segDataPtr, page);
    if (*ptePtr & VMMACH_RESIDENT_BIT) {
	*ptePtr |= VMMACH_MODIFIED_BIT;
	/*
	 * Flush the block from the cache so that David Wood can get better
	 * cache stats.
	 */
	VmMachFlushBlock(
		    (Address)((unsigned)addr & ~(VMMACH_CACHE_LINE_SIZE-1)));
    }

    UNLOCK_MONITOR;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_PageValidate --
 *
 *      Validate a page for the given virtual address.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The page table is modified to validate the page.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_PageValidate(virtAddrPtr, pte) 
    register	Vm_VirtAddr	*virtAddrPtr;
    Vm_PTE			pte;
{
    register  VmMach_SegData	*segDataPtr;
    int				page;
    VmMachPTE			*ptePtr;
    VmMachPTE			machPTE;

    LOCK_MONITOR;

    segDataPtr = virtAddrPtr->segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    if (*ptePtr & VMMACH_RESIDENT_BIT) {
	/*
  	 * If already resident do not validate again because we could lose
	 * the modify bit.
	 */
	if (VirtToPhysPage(Vm_GetPageFrame(pte)) == GetPageFrame(*ptePtr)) {
		UNLOCK_MONITOR;
		return;
	}
	if (virtAddrPtr->segPtr != vm_SysSegPtr)
		printf("Remapping page 0x%x of segment %d\n",page,
			virtAddrPtr->segPtr->segNum);
    }
    machPTE = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT | 
	      VMMACH_REFERENCED_BIT |
              SetPageFrame(VirtToPhysPage(Vm_GetPageFrame(pte)));
    if (virtAddrPtr->segPtr == vm_SysSegPtr) {
	machPTE |= VMMACH_KRW_URO_PROT | VMMACH_MODIFIED_BIT;
    } else {
	if (pte & (VM_COW_BIT | VM_READ_ONLY_PROT)) {
	    machPTE |= VMMACH_KRW_URO_PROT;
	} else {
	    machPTE |= VMMACH_KRW_URW_PROT;
	}
	if (virtAddrPtr->segPtr->type != VM_CODE) {
	    if (ownStackAndHeap) {
		machPTE |= VMMACH_COHERENCY_BIT;
	    }
	    if (vmWriteablePageout || vmWriteableRefPageout) {
		machPTE |= VMMACH_MODIFIED_BIT;
	    }
	}
    }
    *ptePtr = machPTE;

    UNLOCK_MONITOR;
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_PageInvalidate --
 *
 *      Invalidate a page for the given segment.  
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The page table is modified.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ENTRY void
VmMach_PageInvalidate(virtAddrPtr, virtPage, segDeletion) 
    register	Vm_VirtAddr	*virtAddrPtr;
    unsigned 	int		virtPage;
    Boolean			segDeletion;
{
    unsigned 	int	page;
    VmMach_SegData	*segDataPtr;
    VmMachPTE		*ptePtr;

    LOCK_MONITOR;

    segDataPtr = virtAddrPtr->segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    VmMachFlushPage(virtAddrPtr->segPtr, page, TRUE);
    *ptePtr = 0;

    UNLOCK_MONITOR;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_PinUserPages --
 *
 *	Force a user page to be resident in memory and have its reference
 *	and modify bits set.  Our caller  has already assured that
 *	the page is locked in memory such that its reference and modify
 *	bits won't get cleared once we set them.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_PinUserPages(mapType, virtAddrPtr, lastPage)
    int		mapType;	/* VM_READONLY_ACCESS | VM_READWRITE_ACCESS */
    Vm_VirtAddr	*virtAddrPtr;	/* Pointer to beginning virtual address to
				 * to pin. */
    int		lastPage;	/* Last page to unpin. */
{
    register	Address	addr;
    register	int	i;
    volatile	int	*intPtr;
    int			intVal;

    for (i = lastPage - virtAddrPtr->page + 1, 
		addr = (Address)(virtAddrPtr->page << VMMACH_PAGE_SHIFT);
	 i > 0;
	 i--, addr += VMMACH_PAGE_SIZE) {
	intPtr = (int *)addr;
	intVal = *intPtr;
	if (mapType == VM_READWRITE_ACCESS) {
	    *intPtr = intVal;
	}
    }
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_UnpinUserPages --
 *
 *	Allow a page that was pinned to be unpinned.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMach_UnpinUserPages(virtAddrPtr, lastPage)
    Vm_VirtAddr	*virtAddrPtr;
    int		lastPage;
{
}

static	int	nextDevPage = VMMACH_KERN_DEVICE_SPACE / VMMACH_PAGE_SIZE;


/*
 *----------------------------------------------------------------------
 *
 * VmMach_MapInDevice --
 *
 *	Map a device at some physical address into kernel virtual address.
 *	This is for use by the controller initialization routines.  Note
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
Address
VmMach_MapInDevice(devPhysAddr, numBytes)
    Address	devPhysAddr;	/* Physical address of the device to map in.*/
    int		numBytes;	/* Number of pages to validate. */
{
    int				numPages;
    int				firstPTPage;
    int				lastPTPage;
    register	VmMachPTE	*ptePtr;
    VmMachPTE			*ptAddr;
    int				i;
    VmMachPTE			pte;
    Address			retAddr;
    int				pfNum;

    numPages = (((unsigned)devPhysAddr + numBytes - 1) >> VMMACH_PAGE_SHIFT) -
	       ((unsigned)devPhysAddr >> VMMACH_PAGE_SHIFT) + 1;
    firstPTPage = nextDevPage >> VMMACH_SEG_PT2_SHIFT;
    lastPTPage = (nextDevPage + numPages - 1) >> VMMACH_SEG_PT2_SHIFT;

    ptePtr = Get2ndPageTablePtr(vm_SysSegPtr->machPtr, firstPTPage);
    ptAddr = (VmMachPTE *)(vm_SysSegPtr->machPtr->ptBasePtr) + 
			    (firstPTPage << VMMACH_SEG_PT2_SHIFT);
    /*
     * Allocate the kernel page table which will map the device pages.
     */
    for (ptePtr = Get2ndPageTablePtr(vm_SysSegPtr->machPtr, firstPTPage);
	 firstPTPage <= lastPTPage;
	 firstPTPage++, ptePtr++, ptAddr += VMMACH_PTES_IN_PAGE) {
	if (!(*ptePtr & VMMACH_RESIDENT_BIT)) {
	     pfNum = Vm_KernPageAllocate();
	    *ptePtr = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT | 
		      VMMACH_KRW_URO_PROT | VMMACH_REFERENCED_BIT |
		      VMMACH_MODIFIED_BIT | SetPageFrame(VirtToPhysPage(pfNum));
	    bzero((Address)ptAddr, VMMACH_PAGE_SIZE);
	}
    }
    /*
     * Now initialize the mappings in the page table.
     */
    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_URO_PROT | VMMACH_REFERENCED_BIT | 
		  VMMACH_MODIFIED_BIT | 
	          SetPageFrame((unsigned)devPhysAddr >> VMMACH_PAGE_SHIFT);
    for (i = 0, ptePtr = GetPageTablePtr(vm_SysSegPtr->machPtr, nextDevPage);
         i < numPages;
	 i++, pte = IncPageFrame(pte), ptePtr++) {
	*ptePtr = pte;
    }

    retAddr = (Address) ((nextDevPage << VMMACH_PAGE_SHIFT) + 
                         ((unsigned)devPhysAddr & VMMACH_OFFSET_MASK));
    nextDevPage += numPages;
    return(retAddr);
}


/*
 * ----------------------------------------------------------------------------
 *
 * Vm_MapKernelIntoUser --
 *
 *      Map a portion of kernel memory into the user's heap segment.  
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      None.
 *
 * ----------------------------------------------------------------------------
 */
/*ARGSUSED*/
ReturnStatus
VmMach_MapKernelIntoUser(kernelVirtAddr, numBytes, userVirtAddr,
			 realVirtAddrPtr) 
    int	kernelVirtAddr;		/* Kernel virtual address to map in. */
    int	numBytes;		/* Number of bytes to map. */
    int	userVirtAddr;		/* User virtual address to attempt to start 
				   mapping in at. */
    int	realVirtAddrPtr;	/* Where we were able to start mapping at. */
{
}

/* 
 * The following mask is used to detect proper alignment of addresses
 * for doing word operations instead of byte operations.  If none of the
 * following bits are set in an address, then word-based transfers may be
 * used.
 */
#define WORDMASK 0x3


/*
 *----------------------------------------------------------------------
 *
 * VmMachDoCopy --
 *
 *	Copy numBytes from *sourcePtr to *destPtr.  This routine is
 *	optimized to do transfers when sourcePtr and destPtr are both
 *	4-byte aligned and point to large areas.
 *
 * Results:
 *	Contents at sourcePtr copied to contents at destPtr
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
ReturnStatus
VmMachDoCopy(numBytes, sourcePtr, destPtr)
    register int numBytes;	/* The number of bytes to copy */
    Address	sourcePtr;	/* Where to copy from */
    Address	destPtr;	/* Where to copy to. */
{
    register int *sPtr = (int *) sourcePtr;
    register int *dPtr = (int *) destPtr;

    /*
     * If the destination is below the source then it is safe to copy
     * in the forward direction.  Otherwise, we must start at the top
     * and work down, again optimizing for large transfers.
     */

    if (dPtr < sPtr) {
	/*
	 * If both the source and the destination point to aligned
	 * addresses then copy as much as we can in large transfers.  Once
	 * we have less than a whole int to copy then it must be done by
	 * byte transfers.  Furthermore, use an expanded loop to avoid
	 * the overhead of continually testing loop variables.
	 */
	
	if (!((((int) sPtr) | (int) dPtr) & WORDMASK)) {
	    while (numBytes >= 16*sizeof(int)) {
		dPtr[0] = sPtr[0];
		dPtr[1] = sPtr[1];
		dPtr[2] = sPtr[2];
		dPtr[3] = sPtr[3];
		dPtr[4] = sPtr[4];
		dPtr[5] = sPtr[5];
		dPtr[6] = sPtr[6];
		dPtr[7] = sPtr[7];
		dPtr[8] = sPtr[8];
		dPtr[9] = sPtr[9];
		dPtr[10] = sPtr[10];
		dPtr[11] = sPtr[11];
		dPtr[12] = sPtr[12];
		dPtr[13] = sPtr[13];
		dPtr[14] = sPtr[14];
		dPtr[15] = sPtr[15];
		sPtr += 16;
		dPtr += 16;
		numBytes -= 16*sizeof(int);
	    }
	    while (numBytes >= sizeof(int)) {
		*dPtr++ = *sPtr++;
		numBytes -= sizeof(int);
	    }
	    if (numBytes == 0) {
		return(SUCCESS);
	    }
	}
	
	/*
	 * Copy the remaining bytes.
	 */
	
	sourcePtr = (char *) sPtr;
	destPtr = (char *) dPtr;
	while (numBytes > 0) {
	    *destPtr++ = *sourcePtr++;
	    numBytes--;
	}
    } else {
	/*
	 * Handle extra bytes at the top that are due to the transfer
	 * length rather than pointer misalignment.
	 */
	while (numBytes & WORDMASK) {
	    numBytes --;
	    destPtr[numBytes] = sourcePtr[numBytes];
	}
	sPtr = (int *) (sourcePtr + numBytes);
	dPtr = (int *) (destPtr + numBytes);

	if (!((((int) sPtr) | (int) dPtr) & WORDMASK)) {
	    while (numBytes >= 16*sizeof(int)) {
		sPtr -= 16;
		dPtr -= 16;
		dPtr[15] = sPtr[15];
		dPtr[14] = sPtr[14];
		dPtr[13] = sPtr[13];
		dPtr[12] = sPtr[12];
		dPtr[11] = sPtr[11];
		dPtr[10] = sPtr[10];
		dPtr[9] = sPtr[9];
		dPtr[8] = sPtr[8];
		dPtr[7] = sPtr[7];
		dPtr[6] = sPtr[6];
		dPtr[5] = sPtr[5];
		dPtr[4] = sPtr[4];
		dPtr[3] = sPtr[3];
		dPtr[2] = sPtr[2];
		dPtr[1] = sPtr[1];
		dPtr[0] = sPtr[0];
		numBytes -= 16*sizeof(int);
	    }
	    while (numBytes >= sizeof(int)) {
		*--dPtr = *--sPtr;
		numBytes -= sizeof(int);
	    }
	    if (numBytes == 0) {
		return(SUCCESS);
	    }
	}
	
	/*
	 * Copy the remaining bytes.
	 */
	
	destPtr = (char *) dPtr;
	sourcePtr = (char *) sPtr;
	while (numBytes > 0) {
	    *--destPtr = *--sourcePtr;
	    numBytes--;
	}
    }
    return(SUCCESS);
}


/*
 *----------------------------------------------------------------------
 *
 * Vm_CopyIn --
 *
 *	Copy numBytes from *sourcePtr to *destPtr. 
 *
 * Results:
 *	SUCCESS if successfully copied in the string.  SYS_ARG_NOACCESS
 *	otherwise.  SYS_ARG_NOACCESS is returned by the trap handlers if
 *	we hit an error.
 *
 * Side effects:
 *	Contents at sourcePtr copied to contents at destPtr
 *
 *----------------------------------------------------------------------
 */
ReturnStatus
Vm_CopyIn(numBytes, sourcePtr, destPtr)
    register int	numBytes;	/* The number of bytes to copy */
    Address		sourcePtr;	/* Where to copy from */
    Address		destPtr;	/* Where to copy to */
{
    if (numBytes == 0) {
	return(SUCCESS);
    } else {
	return(VmMachDoCopy(numBytes, sourcePtr, destPtr));
    }
}


/*
 *----------------------------------------------------------------------
 *
 * Vm_CopyOut --
 *
 *	Copy numBytes from *sourcePtr to *destPtr.
 *
 * Results:
 *	SUCCESS if successfully copied in the string.  SYS_ARG_NOACCESS
 *	otherwise.  SYS_ARG_NOACCESS is returned by the trap handlers if
 *	we hit an error.
 *
 * Side effects:
 *	Contents at sourcePtr copied to contents at destPtr.
 *
 *----------------------------------------------------------------------
 */
ReturnStatus
Vm_CopyOut(numBytes, sourcePtr, destPtr)
    register int	numBytes;	/* The number of bytes to copy */
    Address		sourcePtr;	/* Where to copy from */
    Address		destPtr;	/* Where to copy to */
{
    if (numBytes == 0) {
	return(SUCCESS);
    } else if (destPtr < mach_FirstUserAddr || destPtr > mach_LastUserAddr) {
	return(SYS_ARG_NOACCESS);
    } else {
	return(VmMachDoCopy(numBytes, sourcePtr, destPtr));
    }
}


/*
 *----------------------------------------------------------------------
 *
 * Vm_StringNCopy --
 *
 *	Copies exactly n characters from src to dst.  If src doesn't
 *	contain exactly n characters, then the last characters are
 *	ignored (if src is too long) or filled with zeros (if src
 *	is too short).  In the case of truncation, dst may not be
 *	null-terminated.
 *
 * Results:
 *	SUCCESS if successfully copied in the string.  SYS_ARG_NOACCESS
 *	otherwise.  SYS_ARG_NOACCESS is returned by the trap handlers if
 *	we hit an error.
 *
 * Side effects:
 *	Memory at *dst is modified.  The caller must ensure that the
 *	destination is large enough to hold n characters.  Alternatively,
 *	dst may be dynamically allocated.
 *
 *----------------------------------------------------------------------
 */
ReturnStatus
Vm_StringNCopy(n, src, dst, bytesCopiedPtr)
    register int	n;			/* How many characters to 
						 * place at dst. */
    register char	*src;			/* Source string. */
    char		*dst;			/* Destination area. */
    int			*bytesCopiedPtr;	/* OUT: The number of bytes 
						 * actually copied. */
{
    register char	*copy = dst;
    char		*origSrc = src;

    if (n == 0) {
	*bytesCopiedPtr = 0;
	return(SUCCESS);
    }
#ifndef lint
    do {
	if ((*copy++ = *src) != 0) {
	    src += 1;
	} else {
	    break;
	}
    } while (--n > 0);
#else
    do {
	*copy = *src;
	copy++;
	if (*src != 0) {
	    src += 1;
	} else {
	    break;
	}
	n--;
    } while (n > 0);
#endif
    *bytesCopiedPtr = src - origSrc;

    return(SUCCESS);
}



/*
 *----------------------------------------------------------------------
 *
 * Vm_TouchPages --
 *
 *	Touch each of the given pages.
 *
 * Results:
 *	SUCCESS if successfully touched all pages.  SYS_ARG_NOACCESS
 *	otherwise.  SYS_ARG_NOACCESS is returned by the trap handlers if
 *	we hit an error.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
ReturnStatus
Vm_TouchPages(firstPage, numPages)
    register	int	firstPage;
    register	int	numPages;
{
    register Address	addr;
    volatile int	*intPtr;
    int			intVal;

    for (addr = (Address)(firstPage << VMMACH_PAGE_SHIFT);
	 numPages > 0;
	 numPages--, addr += VMMACH_PAGE_SIZE) {
	intPtr = (int *)addr;
	intVal = *intPtr;
    }
    return(SUCCESS);
}

/*
 * Routine to mark the end of the copy routines so that the trap handlers
 * will know whether a kernel error happened on a cross-address space
 * copy.
 */
void
VmMachCopyEnd()
{
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_FlushPage --
 *
 *	Flush the page at the given virtual address from all caches.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The given page is flushed from the caches.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMach_FlushPage(virtAddrPtr, invalidate)
    register	Vm_VirtAddr	*virtAddrPtr;	/* Virtual address of page. */
    Boolean			invalidate;	/* Should invalidate the pte
						 * after flushing. */
{
    LOCK_MONITOR;

    VmMachFlushPage(virtAddrPtr->segPtr, 
		    (unsigned int)(virtAddrPtr->page &
			~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT)),
		    invalidate);

    UNLOCK_MONITOR;
}


/*
 *----------------------------------------------------------------------
 *
 * VmMachFlushPage --
 *
 *	Flush the given page from all caches.  Note the page is relative
 *	to the beginning of the segment.
 *
 *	NOTE: This code assumes that it will not get involuntarily 
 *	      context switch within it.  If we can get context switched
 *	      then it will have to modify the segment register value
 *	      stored in the machine dependent proc table struct.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The given page is flushed from the caches.
 *
 *----------------------------------------------------------------------
 */
INTERNAL void
VmMachFlushPage(segPtr, pageNum, invalidate)
    register	Vm_Segment	*segPtr; /* Segment to flush. */
    unsigned	int		pageNum; /* Page within segment to flush. */
    Boolean			invalidate;	/* Should invalidate the pte
						 * after flushing. */
{
    register	Address		pageAddr;
    register	Address		addr;
    register	VmMachPTE	*ptePtr;
    VmMach_SegData		*segDataPtr;
    int				origSegNum;
    int				refBit;
    int				resBit;

    segDataPtr = segPtr->machPtr;
    ptePtr = GetPageTablePtr(segDataPtr, pageNum);
    if (!(*ptePtr & VMMACH_RESIDENT_BIT) || !(*ptePtr & VMMACH_CACHEABLE_BIT)) {
	return;
    }

    if (segPtr->segNum > 0) {
	origSegNum = VmMachSetSegReg1(segPtr->segNum, 
				      segPtr->segNum / 4 + rootPTPageNum);
	pageAddr = (Address) (VMMACH_CODE_SEG_START |
			      (pageNum << VMMACH_PAGE_SHIFT));
    } else {
	pageAddr = (Address) (pageNum << VMMACH_PAGE_SHIFT);
    }
    /*
     * Reset the page resident bit saving the value to restore.
     * If the called wants the page invalidated restore a value of 
     * zero.
     */
    resBit = !invalidate ? (*ptePtr & VMMACH_RESIDENT_BIT) : 0;
    *ptePtr &= ~VMMACH_RESIDENT_BIT;
    /*
     * Set the reference bit because the ReadAnyways will cause a
     * reference bit fault.
     */
    refBit = !invalidate ? (*ptePtr & VMMACH_REFERENCED_BIT) : 0;
    *ptePtr |= VMMACH_REFERENCED_BIT;
    if (uniprocessorFlushPage) {
	for (addr = pageAddr; 
         	addr < pageAddr + VMMACH_PAGE_SIZE;
		addr = addr + VMMACH_CACHE_LINE_SIZE) 
	{
	    VmMachFlushBlock(addr);
    	}
    }
    else {
    	for (addr = pageAddr; 
         	addr < pageAddr + VMMACH_PAGE_SIZE;
		addr = addr + VMMACH_CACHE_LINE_SIZE) 
	{
	    VmMachReadAnyways(addr);
	    VmMachFlushBlock(addr);
    	}
    }
    if (!refBit) {
	*ptePtr &= ~VMMACH_REFERENCED_BIT;
    }
    if (resBit) {
	*ptePtr |= VMMACH_RESIDENT_BIT;
    }
    if (segPtr->segNum > 0) {
	(void) VmMachSetSegReg1(origSegNum, origSegNum / 4 + rootPTPageNum);
    }
}



/*
 *----------------------------------------------------------------------
 *
 * VmMachFlushBytes --
 *
 *	Flush the given range of bytes from all caches.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The given range of bytes is flushed from the caches.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
VmMachFlushBytes(segPtr, startAddr, numBytes)
    Vm_Segment	*segPtr;	/* Segment to flush bytes in. */
    Address	startAddr;	/* Address to start flushing at. */
    int		numBytes;	/* Number of bytes to flush. */
{
    unsigned	int	firstPage;
    unsigned	int	lastPage;

    LOCK_MONITOR;

    firstPage = (unsigned int)startAddr >> VMMACH_PAGE_SHIFT;
    firstPage &= ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    lastPage = (unsigned int)(startAddr + numBytes - 1) >> VMMACH_PAGE_SHIFT;
    lastPage &= ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    for (; firstPage <= lastPage; firstPage++) {
	VmMachFlushPage(segPtr, firstPage, FALSE);
    }

    UNLOCK_MONITOR;
}


/*
 *----------------------------------------------------------------------
 *
 * SimpleValidate --
 *
 *	Make sure that the range of bytes are valid in the kernel's address
 *	space.  This works even before virtual memory has been initialized.
 *
 * Results:
 *	TRUE if the range of addresses are valid and FALSE otherwise.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
Boolean
SimpleValidate(addr, numBytes)
    Address	addr;
    int		numBytes;
{
    register VmMachPTE	*ptePtr;
    unsigned int	firstPage;
    unsigned int	lastPage;
    unsigned int	firstPTPage;
    unsigned int	lastPTPage;

    if (((unsigned int)addr & VMMACH_SEG_REG_MASK) != 0 ||
	((unsigned int)(addr + numBytes - 1) & VMMACH_SEG_REG_MASK) != 0) {
	return(FALSE);
    }

    firstPage = (unsigned int)addr >> VMMACH_PAGE_SHIFT;
    lastPage = (unsigned int)(addr + numBytes - 1) >> VMMACH_PAGE_SHIFT;
    firstPTPage = firstPage >> VMMACH_SEG_PT2_SHIFT;
    lastPTPage = lastPage >> VMMACH_SEG_PT2_SHIFT;

    /*
     * Assume that the root page tables are mapped and check the second
     * level page tables first.  If the root page tables aren't mapped then
     * we won't get this far in the first place.
     */
    for (ptePtr = (VmMachPTE *)(VMMACH_KERN_PT2_BASE) + firstPTPage;
	 firstPTPage <= lastPTPage;
	 firstPTPage++, ptePtr++) {
	if (!(*ptePtr & VMMACH_RESIDENT_BIT) ||
	    (*ptePtr & VMMACH_PROTECTION_FIELD) == 0) {
	    return(FALSE);
	}
    }
    /*
     * Now check the first level page table entries.
     */
    for (ptePtr = (VmMachPTE *)(VMMACH_KERN_PT_BASE) + firstPage;
	 firstPage <= lastPage;
	 firstPage++, ptePtr++) {
	if (!(*ptePtr & VMMACH_RESIDENT_BIT)) {
	    return(FALSE);
	}
    }
    return(TRUE);
}


/*
 *----------------------------------------------------------------------
 *
 * Vm_ValidateRange --
 *
 *	Make sure that the range of bytes are valid in the kernel's address
 *	space.
 *
 * Results:
 *	TRUE if the range of addresses are valid and FALSE otherwise.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
Boolean
Vm_ValidateRange(addr, numBytes)
    Address	addr;
    int		numBytes;
{
    register VmMachPTE	*ptePtr;
    unsigned int	firstPage;
    unsigned int	lastPage;
    unsigned int	firstPTPage;
    unsigned int	lastPTPage;
    VmMach_SegData	*segDataPtr;
    int			hardSegNum;
    Proc_ControlBlock	*procPtr;
    Vm_Segment		*segPtr;
    int			origSegNum;

    if (!initialized || useSimpleValidation) {
	return(SimpleValidate(addr, numBytes));
    }

    hardSegNum = (unsigned int)addr >> VMMACH_SEG_REG_SHIFT;
    if (hardSegNum == 0) {
	segPtr = vm_SysSegPtr;
    } else {
	procPtr = Proc_GetCurrentProc();
	segPtr = procPtr->vmPtr->segPtrArray[hardSegNum];
	if (segPtr == (Vm_Segment *)NIL) {
	    return(FALSE);
	}
	switch (hardSegNum) {
	    case 1:
		origSegNum = VmMachSetSegReg1(segPtr->segNum, 
				      segPtr->segNum / 4 + rootPTPageNum);
		if (origSegNum != segPtr->segNum) {
		    (void)VmMachSetSegReg1(origSegNum,
				      origSegNum / 4 + rootPTPageNum);
		    return(FALSE);
		}
		break;
	    case 2:
		origSegNum = VmMachSetSegReg2(segPtr->segNum, 
				      segPtr->segNum / 4 + rootPTPageNum);
		if (origSegNum != segPtr->segNum) {
		    (void)VmMachSetSegReg2(origSegNum,
				      origSegNum / 4 + rootPTPageNum);
		    return(FALSE);
		}
		break;
	    case 3:
		origSegNum = VmMachSetSegReg3(segPtr->segNum, 
				      segPtr->segNum / 4 + rootPTPageNum);
		if (origSegNum != segPtr->segNum) {
		    (void)VmMachSetSegReg3(origSegNum,
				      origSegNum / 4 + rootPTPageNum);
		    return(FALSE);
		}
		break;
	}
    }
    segDataPtr = segPtr->machPtr;

    addr = (Address) ((unsigned int)addr & ~VMMACH_SEG_REG_MASK);
    firstPage = (unsigned int)addr >> VMMACH_PAGE_SHIFT;
    lastPage = (unsigned int)(addr + numBytes - 1) >> VMMACH_PAGE_SHIFT;
    firstPTPage = firstPage >> VMMACH_SEG_PT2_SHIFT;
    lastPTPage = lastPage >> VMMACH_SEG_PT2_SHIFT;

    /*
     * Assume that the root page tables are mapped and check the second
     * level page tables first.  If the root page tables aren't mapped then
     * we won't get this far in the first place.
     */
    for (ptePtr = Get2ndPageTablePtr(segDataPtr, firstPTPage);
	 firstPTPage <= lastPTPage;
	 firstPTPage++, ptePtr++) {
	if (!(*ptePtr & VMMACH_RESIDENT_BIT) ||
	    (*ptePtr & VMMACH_PROTECTION_FIELD) == 0) {
	    return(FALSE);
	}
    }
    /*
     * Now check the first level page table entries.
     */
    for (ptePtr = GetPageTablePtr(segDataPtr, firstPage);
	 firstPage <= lastPage;
	 firstPage++, ptePtr++) {
	if (!(*ptePtr & VMMACH_RESIDENT_BIT)) {
	    return(FALSE);
	}
    }
    return(TRUE);
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_Trace --
 *
 *	SPUR VM tracing.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
void
VmMach_Trace()
{
}


/*
 *----------------------------------------------------------------------
 *
 * VmMach_Cmd --
 *
 *	Machine dependent vm command handler.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */
ReturnStatus
VmMach_Cmd(command, arg)
    int	command;
    int	arg;
{
    switch (command) {
	case VM_SET_FLUSH_ON_REF_BIT_CLEAR:
	    printf("flushOnRefBitClear val was %d, is %d\n",
		        flushOnRefBitClear, arg);
	    flushOnRefBitClear = arg;
	    return(SUCCESS);
	case VM_SET_USE_HARD_REF_BIT:
	    printf("useHardRefBit val was %d, is %d\n",
		        useHardRefBit, arg);
	    useHardRefBit = arg;
	    return(SUCCESS);
	case VM_SET_COHERENCY_BIT:
	    printf("ownStackAndHeap val was %d, is %d\n",
		        ownStackAndHeap, arg);
	    ownStackAndHeap = (Boolean) arg;
	    return(SUCCESS);
	case VM_SET_UP_FLUSH_PAGE:
	    printf("uniprocessorFlushPage val was %d, is %d\n",
			uniprocessorFlushPage, arg);
	    uniprocessorFlushPage = (Boolean) arg;
	    return(SUCCESS);
	default:
	    printf("Warning: VmMach_Cmd: Unknown command %d\n", command);
	    return(GEN_INVALID_ARG);
    }
}


/*
 * ----------------------------------------------------------------------------
 *
 * VmMach_MakeNonCachable --
 *
 *      Make a page non cachable.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      The page table is modified.
 *
 * ----------------------------------------------------------------------------
 */
ENTRY void
VmMach_MakeNonCachable(virtAddrPtr, pte) 
    register	Vm_VirtAddr	*virtAddrPtr;
    Vm_PTE			pte;
{
    register  VmMach_SegData	*segDataPtr;
    int				page;
    VmMachPTE			*ptePtr;
    VmMachPTE			machPTE;

    LOCK_MONITOR;

    segDataPtr = virtAddrPtr->segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    *ptePtr &= ~VMMACH_CACHEABLE_BIT;

    UNLOCK_MONITOR;
}


/*
 * Dummy function which will turn out to be the function that the debugger
 * prints out on a backtrace after a trap.  The debugger gets confused
 * because trap stacks originate from assembly language stacks.  I decided
 * to make a dummy procedure because it was to confusing seeing the
 * previous procedure (VmMach_MapKernelIntoUser) on every backtrace.
 */
VmMachTrap()
{
}

/*
 *----------------------------------------------------------------------
 *
 * VmMach_FlushCode --
 *
 *      Machine dependent vm commands.
 *
 * Results:
 *      None.
 *
 * Side effects:
 *      None.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMach_FlushCode(procPtr, virtAddrPtr, virtPage, numBytes)
    Proc_ControlBlock   *procPtr;
    Vm_VirtAddr         *virtAddrPtr;
    unsigned            virtPage;
    int                 numBytes;
{
}

VmMach_SharedStart() {}

ReturnStatus VmMach_SharedStartAddr() {}
void VmMach_SharedSegFinish() {}
void VmMach_SharedProcStart() {}
void VmMach_SharedProcFinish() {}

void
/*ARGSUSED*/
VmMach_HandleSegMigration(segPtr)
    Vm_Segment	*segPtr;
{}
@


8.8
log
@Added shared memory stubs.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.7 89/09/06 15:19:07 shirriff Exp Locker: shirriff $ SPRITE (Berkeley)";
d2583 6
@


8.7
log
@Added VmMach_SharedStart stub.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.6 89/07/23 18:34:15 jhh Exp Locker: shirriff $ SPRITE (Berkeley)";
d2578 5
@


8.6
log
@added VmMach_FlushCode and  changed to VMMACH_CACHE_LINE_SIZE
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.5 89/06/19 13:18:28 jhh Exp Locker: jhh $ SPRITE (Berkeley)";
d2577 1
a2577 1

@


8.5
log
@Cleaning up locked files
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.4 89/03/15 17:25:24 david Exp $ SPRITE (Berkeley)";
d1186 3
a1188 3
		(Address)((unsigned)addr & ~(VMMACH_CACHE_BLOCK_SIZE-1)));
	addr += VMMACH_CACHE_BLOCK_SIZE;
	numBytes -= VMMACH_CACHE_BLOCK_SIZE;
d1195 1
a1195 1
		(Address)((unsigned)addr & ~(VMMACH_CACHE_BLOCK_SIZE-1)));
d1492 1
a1492 1
		    (Address)((unsigned)addr & ~(VMMACH_CACHE_BLOCK_SIZE-1)));
d2197 1
a2197 1
	 	addr = addr + VMMACH_CACHE_BLOCK_SIZE) 
d2205 1
a2205 1
	 	addr = addr + VMMACH_CACHE_BLOCK_SIZE) 
d2551 27
@


8.4
log
@1) Added uniprocessorFlushPage flush option.
2) VirtToPhysPage() evaluates its argument twice.  Don't do a 
   VirtToPhysPage(Vm_KernPageAllocate()).
3) Used new lock init routine.
4) Added routine so mapped pages can be made noncacheable.
5) Don't clear the modify bit when validating a page.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.3 89/01/11 14:11:53 nelson Exp Locker: david $ SPRITE (Berkeley)";
d160 7
d199 1
d243 8
d403 1
a403 1
	pte |= VMMACH_KRO_UNA_PROT;
d494 1
a494 1
	              VMMACH_KRW_UNA_PROT | VMMACH_REFERENCED_BIT |
d1177 1
a1177 1
        pte |= readWrite ? VMMACH_KRW_UNA_PROT : VMMACH_KRO_UNA_PROT;
@


8.3
log
@Fixed a couple of race conditions on multi-processors.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.2 88/12/28 19:58:16 nelson Exp Locker: nelson $ SPRITE (Berkeley)";
d29 1
d151 9
d452 2
d476 1
d479 1
a479 2
		      VMMACH_MODIFIED_BIT |
		      SetPageFrame(VirtToPhysPage(Vm_KernPageAllocate()));
d595 1
d613 2
a614 1
	    Vm_KernPageFree((unsigned)PhysToVirtPage(GetPageFrame(*ptePtr)));
d1024 1
a1024 1
Sync_Lock		ptLock = SYNC_LOCK_INIT_STATIC();
d1062 2
d1214 1
a1214 1
    LOCK_MONITOR;
d1355 1
d1375 1
d1513 13
d1688 1
d1705 1
d1708 1
a1708 2
		      VMMACH_MODIFIED_BIT |
		      SetPageFrame(VirtToPhysPage(Vm_KernPageAllocate()));
d2149 1
d2153 1
a2153 1
    if (!(*ptePtr & VMMACH_RESIDENT_BIT)) {
d2165 6
a2170 1

d2176 1
a2176 1
    refBit = *ptePtr & VMMACH_REFERENCED_BIT;
d2178 22
a2199 13
    for (addr = pageAddr; 
         addr < pageAddr + VMMACH_PAGE_SIZE;
	 addr = addr + VMMACH_CACHE_BLOCK_SIZE) {
	VmMachReadAnyways(addr);
	VmMachFlushBlock(addr);
    }
    *ptePtr &= ~VMMACH_REFERENCED_BIT;
    if (!invalidate) {
	/*
	 * We are not invalidating this page so restore the reference and
	 * modify bits.
	 */
	*ptePtr |= (VMMACH_RESIDENT_BIT | refBit);
d2477 5
d2487 37
@


8.2
log
@Mendel checking in for Mike.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 8.1 88/12/04 15:53:35 ouster Exp $ SPRITE (Berkeley)";
d41 2
d67 1
a67 1
	(mb->physStartAddr + pfNum); })
d77 1
a77 1
	(pfNum - mb->physStartAddr); })
a118 4
static void FlushSegment();
static void SetCreateTime();
static void FlushAllCaches();

a119 6
 * Variable to control cache flushing on segment creation.
 */
static unsigned createTime = 0;
static unsigned flushTime = 0;

/*
d194 1
a506 1
	segDataPtr->createTime = 0;
a510 2
    SetCreateTime(segDataPtr);

d596 2
a597 3
	     * The page table page was resident so free it.  No need to
	     * flush the page here because we will zero it out before we
	     * use it next time anyway.
d599 1
a599 1
	    VmMach_FlushPage(&virtAddr);
d1010 1
a1010 1
Sync_Lock		ptLock;
d1044 1
d1054 3
a1056 2
	    *ptePtr &= ~VMMACH_PROTECTION_FIELD;
	    *ptePtr |= 
d1058 2
a1061 1
    FlushSegment(segPtr);
d1093 1
d1101 3
a1103 2
    *ptePtr &= ~VMMACH_PROTECTION_FIELD;
    *ptePtr |= (softPTE & (VM_COW_BIT | VM_READ_ONLY_PROT)) ? 
d1105 2
a1106 1
    VmMachFlushPage(segPtr, page);
d1171 62
d1306 1
a1306 1
	    VmMachFlushPage(virtAddrPtr->segPtr, page);
d1358 6
a1363 4
        *ptePtr |= VMMACH_REFERENCED_BIT;
	if ((vmWriteableRefPageout || vmWriteablePageout) && 
	    segPtr->type != VM_CODE) {
	    *ptePtr |= VMMACH_MODIFIED_BIT;
d1403 1
a1403 1
	VmMachFlushPage(virtAddrPtr->segPtr, page);
d1451 9
a1459 7
    *ptePtr |= VMMACH_MODIFIED_BIT;
    /*
     * Flush the block from the cache so that David Wood can get better
     * cache stats.
     */
    VmMachFlushBlock(
		(Address)((unsigned)addr & ~(VMMACH_CACHE_BLOCK_SIZE-1)));
d1488 1
d1495 2
a1496 2
    *ptePtr = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT | 
	     VMMACH_REFERENCED_BIT |
d1499 1
a1499 1
	*ptePtr |= VMMACH_KRW_URO_PROT | VMMACH_MODIFIED_BIT;
d1502 1
a1502 1
	    *ptePtr |= VMMACH_KRW_URO_PROT;
d1504 1
a1504 1
	    *ptePtr |= VMMACH_KRW_URW_PROT;
d1508 1
a1508 1
		*ptePtr |= VMMACH_COHERENCY_BIT;
d1511 1
a1511 1
		*ptePtr |= VMMACH_MODIFIED_BIT;
d1515 1
d1552 1
a1552 1
    VmMachFlushPage(virtAddrPtr->segPtr, page);
d2067 4
a2070 2
VmMach_FlushPage(virtAddrPtr)
    register	Vm_VirtAddr	*virtAddrPtr;
d2076 2
a2077 1
			~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT)));
d2105 1
a2105 1
VmMachFlushPage(segPtr, pageNum)
d2108 2
d2147 7
a2153 1
    *ptePtr |= (VMMACH_RESIDENT_BIT | refBit);
d2192 1
a2192 1
	VmMachFlushPage(segPtr, firstPage);
a2193 124

    UNLOCK_MONITOR;
}

/*
 *----------------------------------------------------------------------
 *
 * VmMachFlushSegment --
 *
 *	Flush the given page from all caches.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The given page is flushed from the caches.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
void
VmMachFlushSegment(segPtr)
    Vm_Segment	*segPtr;
{
    LOCK_MONITOR;

    FlushSegment(segPtr);

    UNLOCK_MONITOR;
}


/*
 *----------------------------------------------------------------------
 *
 * FlushSegment --
 *
 *	Flush the given page from all caches.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The given page is flushed from the caches.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
INTERNAL static void
FlushSegment(segPtr)
    Vm_Segment	*segPtr;
{
    int		i;

    /*
     * Uni-processor solution.  Flush the entire cache on this machine.
     */
    for (i = 0; i < VMMACH_CACHE_SIZE; i += VMMACH_CACHE_BLOCK_SIZE) {
	VmMachFlushBlock((Address)i);
    }
}


/*
 *----------------------------------------------------------------------
 *
 * FlushAllCaches --
 *
 *	Flush the caches on all processors.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	The caches on all processors are flushed.
 *
 *----------------------------------------------------------------------
 */
/*ARGSUSED*/
INTERNAL static void
FlushAllCaches()
{
    int		i;

    /*
     * Uni-processor solution.  Flush the entire cache on this machine.
     */
    for (i = 0; i < VMMACH_CACHE_SIZE; i += VMMACH_CACHE_BLOCK_SIZE) {
	VmMachFlushBlock((Address)i);
    }
}


/*
 *----------------------------------------------------------------------
 *
 * SetCreateTime --
 *
 *	Set the creation time of the segment and flush all of the caches
 *	if necessary.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	createTime incremented and the createTime field is set in the
 *	segment data struct.  Also the cache may be flushed and flushTime
 *	incremented.
 *
 *----------------------------------------------------------------------
 */
ENTRY static void
SetCreateTime(segDataPtr)
    register	VmMach_SegData	*segDataPtr;
{
    LOCK_MONITOR;

    if (segDataPtr->createTime > flushTime) {
	createTime++;
	flushTime = createTime;
	FlushAllCaches();
    }
    createTime++;
    segDataPtr->createTime = createTime;
@


8.1
log
@Stop using obsolete header files.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 1.22 88/10/28 18:34:48 mlgray Exp $ SPRITE (Berkeley)";
d177 1
a177 1
		numKernPagesPtr, maxSegsPtr)
d184 1
d224 1
d1434 1
a1434 1
	      VMMACH_REFERENCED_BIT |
@


8.0
log
@Changing version numbers.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 1.22 88/10/28 18:34:48 mlgray Exp Locker: douglis $ SPRITE (Berkeley)";
d23 1
a23 1
#include "mem.h"
a27 1
#include "byte.h"
@


1.22
log
@Converted to new C library.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 1.21 88/10/28 18:30:43 mendel Exp Locker: mlgray $ SPRITE (Berkeley)";
@


1.21
log
@Mary checking in for Mendel.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 1.20 88/10/24 09:22:16 mendel Exp $ SPRITE (Berkeley)";
d369 2
a370 2
    Byte_Zero((VMMACH_NUM_SEGMENTS - 1) * VMMACH_SEG_PT2_SIZE,
	      ((Address)kernPT2Ptr) + VMMACH_SEG_PT2_SIZE);
d474 1
a474 1
	    Byte_Zero(VMMACH_PAGE_SIZE, ptAddr);
d506 1
a506 1
	segDataPtr = (VmMach_SegData *)Mem_Alloc(sizeof(VmMach_SegData));
d691 1
a691 1
	vmPtr->machPtr = (VmMach_ProcData *)Mem_Alloc(sizeof(VmMach_ProcData));
d1613 1
a1613 1
	    Byte_Zero(VMMACH_PAGE_SIZE, (Address)ptAddr);
d2465 1
a2465 1
	    Sys_Printf("flushOnRefBitClear val was %d, is %d\n",
d2470 1
a2470 1
	    Sys_Printf("useHardRefBit val was %d, is %d\n",
d2475 1
a2475 1
	    Sys_Printf("ownStackAndHeap val was %d, is %d\n",
d2480 1
a2480 1
	    Sys_Panic(SYS_WARNING, "VmMach_Cmd: Unknown command %d\n", command);
@


1.20
log
@Added various vmcmd for David Wood's VM tests.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: /sprite/src/kernel/vm/spur.md/RCS/vmSpur.c,v 1.19 88/10/19 14:05:42 mendel Exp Locker: mendel $ SPRITE (Berkeley)";
d20 1
d30 4
d36 20
a57 2
#define	VirtToPhysPage(pfNum) (MACH_PAGE_SLOT_MASK | ((pfNum) + MACH_FIRST_PHYS_PAGE))
#define	PhysToVirtPage(pfNum) (((pfNum)-MACH_FIRST_PHYS_PAGE) & ~MACH_PAGE_SLOT_MASK)
d59 19
d186 27
d252 1
a252 1
    return(MACH_NUM_PHYS_PAGES);
@


1.19
log
@Handle the case when a bogus address is passed to SetRefBit.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.18 88/09/27 18:06:04 nelson Exp $ SPRITE (Berkeley)";
d110 7
d1232 4
d1271 5
a1275 2
    *ptePtr &= ~VMMACH_MODIFIED_BIT;
    VmMachFlushPage(virtAddrPtr->segPtr, page);
d1323 6
d1375 6
a1380 1
	    *ptePtr |= VMMACH_COHERENCY_BIT;
d2404 5
@


1.18
log
@Sets coherency bit when validating a heap or stack page.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.17 88/09/26 12:05:27 nelson Exp $ SPRITE (Berkeley)";
d1219 7
a1225 4
    segDataPtr = segPtr->machPtr;
    page = ((unsigned int)(addr) & ~VMMACH_SEG_REG_MASK) >> VMMACH_PAGE_SHIFT;
    ptePtr = GetPageTablePtr(segDataPtr, page);
    *ptePtr |= VMMACH_REFERENCED_BIT;
@


1.17
log
@Added stub for machine dependent commands.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.16 88/09/12 10:15:32 mendel Exp $ SPRITE (Berkeley)";
d1350 3
@


1.16
log
@Modified VmMach_SetProtForDbg to flush old protection from cache.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.14 88/09/07 14:04:59 mendel Exp $ SPRITE (Berkeley)";
d18 2
d104 6
d1128 5
a1132 1
    *refPtr = *ptePtr & VMMACH_REFERENCED_BIT;
d1166 9
a1174 4
    segDataPtr = virtAddrPtr->segPtr->machPtr;
    page = virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT);
    ptePtr = GetPageTablePtr(segDataPtr, page);
    *ptePtr &= ~VMMACH_REFERENCED_BIT;
d1208 2
d1292 1
d2340 38
@


1.15
log
@Added SimpleValidate routine for validating debugger generated address
before the vm module is initialized. 
@
text
@d1068 17
@


1.14
log
@Fixed cache flushing bugs and write-protected the code.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.13 88/08/30 22:38:14 nelson Exp $ SPRITE (Berkeley)";
d84 12
d316 4
d2114 64
d2207 4
a2282 1

@


1.13
log
@Fixed lint errors.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.12 88/08/30 20:10:14 nelson Exp $ SPRITE (Berkeley)";
d207 3
d277 15
d295 1
a295 1
    for (ptePtr = basePTPtr + firstFreePage, i = firstFreePage;
a489 1
    register	unsigned int	ptPage;
d491 1
d494 2
a495 1
    ptPage = (unsigned int)(segDataPtr->ptBasePtr +
d498 1
d502 1
a502 1
	 firstPage++, ptePtr++, ptPage++) {
d504 8
a511 3
	    Vm_KernPageFree((unsigned int)PhysToVirtPage(GetPageFrame(*ptePtr)));
	    *ptePtr &= ~VMMACH_RESIDENT_BIT;
	    VmMachFlushPage(vm_SysSegPtr, ptPage);
d1015 39
a1124 1
    VmMachFlushPage(virtAddrPtr->segPtr, page);
d1335 1
a1335 3
    if (!segDeletion) {
	VmMachFlushPage(virtAddrPtr->segPtr, page);
    }
d1894 1
d1912 6
d1924 2
a1925 1
    *ptePtr |= VMMACH_RESIDENT_BIT;
d2125 1
d2136 29
d2193 1
a2193 2
	if (!(*ptePtr & VMMACH_RESIDENT_BIT) ||
	    (*ptePtr & VMMACH_PROTECTION_FIELD) == 0) {
@


1.12
log
@Added Vm_TouchPages to allow pages to be quickly faulted in.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.11 88/08/27 19:47:37 nelson Exp $ SPRITE (Berkeley)";
d84 6
a1749 1
    int			lastPage;
@


1.11
log
@Fixed lint errors.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.10 88/08/27 15:17:34 nelson Exp $ SPRITE (Berkeley)";
d1278 1
a1278 1
 * VmMach_PinUserPage --
d1294 5
a1298 2
VmMach_PinUserPage(virtAddrPtr)
    Vm_VirtAddr	*virtAddrPtr;
d1300 4
a1303 2
    int	*intPtr;
    int	i;
d1305 10
a1314 7
    /*
     * Read out the value and write it back to ensure that the reference
     * and modify bits are set and we don't accidently change the value.
     */
    intPtr = (int *) (virtAddrPtr->page << VMMACH_PAGE_SHIFT);
    i = *intPtr;
    *intPtr = i;
d1321 1
a1321 1
 * VmMach_UnpinUserPage --
d1333 1
a1333 1
/* ARGSUSED */
d1335 1
a1335 1
VmMach_UnpinUserPage(virtAddrPtr)
d1337 1
d1614 5
a1618 1
    return(VmMachDoCopy(numBytes, sourcePtr, destPtr));
d1645 3
a1647 1
    if (destPtr < mach_FirstUserAddr || destPtr > mach_LastUserAddr) {
d1719 2
d1722 36
d2047 4
d2052 9
a2060 3
    if (((unsigned int)addr & VMMACH_SEG_REG_MASK) != 0 ||
	((unsigned int)(addr + numBytes - 1) & VMMACH_SEG_REG_MASK) != 0) {
	return(FALSE);
d2062 1
d2064 1
d2075 1
a2075 1
    for (ptePtr = (VmMachPTE *)(VMMACH_KERN_PT2_BASE) + firstPTPage;
d2086 1
a2086 1
    for (ptePtr = (VmMachPTE *)(VMMACH_KERN_PT_BASE) + firstPage;
@


1.10
log
@Fixed lint complaints.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.9 88/08/26 12:51:57 nelson Exp $ SPRITE (Berkeley)";
d236 1
a236 1
	Vm_ReservePage(PhysToVirtPage(pfNum));
d247 1
a247 1
	Vm_ReservePage(PhysToVirtPage(pfNum));
d414 1
a414 1
    AllocPageTable(segPtr, segOffset, numPages);
d439 2
a440 1
    AllocPageTable(segPtr, firstPage - segPtr->type * VMMACH_PAGES_PER_SEG,
d466 1
a466 1
    register	int		ptPage;
d478 1
a478 1
	    Vm_KernPageFree(PhysToVirtPage(GetPageFrame(*ptePtr)));
d968 1
a968 1
    int				page;
d1047 1
a1047 1
    int				page;
d1128 1
a1128 1
    int				page;
d1510 1
a1510 1
		return;
d1563 1
a1563 1
		return;
d1578 1
d1737 2
a1738 1
	virtAddrPtr->page & ~(VMMACH_SEG_REG_MASK >> VMMACH_PAGE_SHIFT));
d1768 1
a1768 1
    int				pageNum; /* Page within segment to flush. */
d1827 2
a1828 2
    int		firstPage;
    int		lastPage;
d1897 1
a1897 1
	VmMachFlushBlock(i);
d1927 1
a1927 1
	VmMachFlushBlock(i);
@


1.9
log
@Flushes the cache only when a segment is reused since the last time
that the cache was flushed instead of on each segment deletion.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.7 88/08/24 19:42:29 nelson Exp $ SPRITE (Berkeley)";
d510 1
a510 1
    if (procDataPtr->segRegMask == -1) {
d751 2
a752 1
    if (((unsigned)toAddr & VMMACH_SEG_REG_MASK) == VMMACH_STACK_SEG_START) {
d839 2
a840 1
    if (((unsigned)fromAddr & VMMACH_SEG_REG_MASK) == VMMACH_STACK_SEG_START) {
d1090 1
a1090 1
    if (procDataPtr->segRegMask != -1 &&
d1172 1
a1172 1
    if (procDataPtr->segRegMask != -1 &&
d1947 1
a1947 1
ENTRY void
@


1.8
log
@Fixed several bugs.
@
text
@d389 1
a391 1
	segDataPtr->createTime = 0;
a420 36
 * SetCreateTime --
 *
 *	Set the creation time of the segment and flush all of the caches
 *	if necessary.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	createTime incremented and the createTime field is set in the
 *	segment data struct.  Also the cache may be flushed and flushTime
 *	incremented.
 *
 *----------------------------------------------------------------------
 */
ENTRY void
SetCreateTime(segDataPtr)
    register	VmMach_SegData	*segDataPtr;
{
    LOCK_MONITOR;

    if (segDataPtr->createTime > flushTime) {
	createTime++;
	flushTime = createTime;
	FlushAllCaches();
    }
    createTime++;
    segDataPtr->createTime = createTime;

    UNLOCK_MONITOR;
}


/*
 *----------------------------------------------------------------------
 *
d880 1
a880 1
 * for us because we made be called directly by the routines in the mach
d1924 36
@


1.7
log
@Added limiting the maximum number of segments.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.6 88/08/23 11:14:26 nelson Exp $ SPRITE (Berkeley)";
d74 10
d391 1
d393 3
d421 36
a517 2
    VmMachFlushSegment(segPtr);
    segPtr->machPtr = (VmMach_SegData *)NIL;
d546 5
a550 1
    if (((unsigned)virtAddr & VMMACH_SEG_REG_MASK) == procDataPtr->segRegMask) {
d786 4
a789 1
    if (((unsigned)toAddr >> VMMACH_SEG_REG_SHIFT) == VM_STACK) {
d792 1
d794 1
d804 1
a804 1
    segNums[segToUse] = (unsigned)fromAddr >> VMMACH_SEG_REG_SHIFT;
a806 2
    fromAddr = (Address) ((unsigned int)fromAddr & ~VMMACH_SEG_REG_MASK);
    fromAddr = (Address) ((unsigned int)fromAddr | (segToUse << VMMACH_SEG_REG_SHIFT));
d873 4
a876 1
    if (((unsigned)fromAddr >> VMMACH_SEG_REG_SHIFT) == VM_STACK) {
d879 2
d891 1
a891 1
    segNums[segToUse] = (unsigned)toAddr >> VMMACH_SEG_REG_SHIFT;
a893 2
    toAddr = (Address) ((unsigned)toAddr & ~VMMACH_SEG_REG_MASK);
    toAddr = (Address) ((unsigned)toAddr | (segToUse << VMMACH_SEG_REG_SHIFT));
d970 1
a970 1
    VmMachFlushSegment(segPtr);
d1118 1
d1123 8
a1130 1
    segPtr = procPtr->vmPtr->segPtrArray[(unsigned int)addr >> VMMACH_SEG_REG_SHIFT];
d1200 1
d1205 8
a1212 1
    segPtr = procPtr->vmPtr->segPtrArray[(unsigned int)addr >> VMMACH_SEG_REG_SHIFT];
d1251 2
a1252 1
    *ptePtr = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT |
d1255 1
a1255 2
	*ptePtr |= VMMACH_KRW_URO_PROT | VMMACH_REFERENCED_BIT | 
		   VMMACH_MODIFIED_BIT;
d1498 3
a1500 4
    register int *sPtr;
    register int *dPtr;
    int	    	  extra;
    
d1502 3
a1504 4
     * If the destination is below the source or doesn't overlap
     * the source, then it is safe to copy in the forward direction.
     * Otherwise, we must start at the top and work down, again optimizing
     * for large transfers.
d1507 1
a1507 1
    if (destPtr < sourcePtr || (destPtr > sourcePtr + numBytes)) {
d1509 1
a1509 1
	 * If both the sourcePtr and the destPtr point to aligned
d1511 1
a1511 1
	 * we have less than 4 bytes to copy then it must be done by
d1516 21
a1536 13
	/*
	 * If the pointers aren't aligned, but they're both misaligned by the
	 * same amount, we can copy a few bytes to align them.
	 */
	extra = (int) sourcePtr & WORDMASK;
	if (extra && extra == ((int)destPtr&WORDMASK)) {
	    extra = 4 - extra;
	    if (extra < numBytes) {
		while (extra > 0) {
		    *destPtr++ = *sourcePtr++;
		    extra--;
		    numBytes--;
		}
d1538 1
a1538 7
	}

	sPtr = (int *)sourcePtr;
	dPtr = (int *)destPtr;
	    
	if (!((int) sPtr & WORDMASK) && !((int) dPtr & WORDMASK)) {
	    while (numBytes >= 32) {
d1540 1
a1540 8
		*dPtr++ = *sPtr++;
		*dPtr++ = *sPtr++;
		*dPtr++ = *sPtr++;
		*dPtr++ = *sPtr++;
		*dPtr++ = *sPtr++;
		*dPtr++ = *sPtr++;
		*dPtr++ = *sPtr++;
		numBytes -= 32;
d1542 2
a1543 3
	    while (numBytes >= 4) {
		*dPtr++ = *sPtr++;
		numBytes -= 4;
a1544 2
	    sourcePtr = (char *) sPtr;
	    destPtr = (char *) dPtr;
d1551 2
d1558 7
a1564 10
	destPtr += numBytes;
	sourcePtr += numBytes;
	extra = (int)sourcePtr & WORDMASK;

	if (extra && extra == ((int)destPtr & WORDMASK) && (extra < numBytes)){
	    while (extra > 0) {
		*--destPtr = *--sourcePtr;
		extra--;
		numBytes--;
	    }
d1566 2
d1569 21
a1589 14
	sPtr = (int *)sourcePtr;
	dPtr = (int *)destPtr;
	
	if (!((int) sPtr & WORDMASK) && !((int) dPtr & WORDMASK)) {
	    while (numBytes >= 32) {
		*--dPtr = *--sPtr;
		*--dPtr = *--sPtr;
		*--dPtr = *--sPtr;
		*--dPtr = *--sPtr;
		*--dPtr = *--sPtr;
		*--dPtr = *--sPtr;
		*--dPtr = *--sPtr;
		*--dPtr = *--sPtr;
		numBytes -= 32;
d1591 1
a1591 1
	    while (numBytes >= 4) {
d1593 1
a1593 1
		numBytes -= 4;
d1595 3
a1597 2
	    sourcePtr = (char *) sPtr;
	    destPtr = (char *) dPtr;
d1604 2
a1610 2

    return(SUCCESS);
d1816 2
a1817 1
	pageAddr = (Address) (0x40000000 | (pageNum << VMMACH_PAGE_SHIFT));
d1894 28
d1924 7
a1930 1
    LOCK_MONITOR;
d1932 22
a1959 2

    UNLOCK_MONITOR;
@


1.6
log
@Return 0 bytes when no bytes need to be copied by stringNcopy
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.5 88/08/23 11:07:30 nelson Exp $ SPRITE (Berkeley)";
d93 1
a93 1
		numKernPagesPtr)
d99 1
d113 6
@


1.5
log
@Fixed lint errors and Vm_StringNCopy (again).
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.4 88/08/23 10:14:29 nelson Exp $ SPRITE (Berkeley)";
d1628 1
@


1.4
log
@Works well enough to allow commands to run on SPUR.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.3 88/08/17 11:54:06 nelson Exp $ SPRITE (Berkeley)";
a180 1
    VmMachPTE		pte;
a181 2
    int 		firstFreeSegment;
    Address		virtAddr;
a799 2
    int				segOffset;
    int				bytesToCopy;
d968 1
d1007 1
d1081 1
d1202 1
d1373 1
d1616 7
a1622 4
Vm_StringNCopy(n, src, dst)
    register int n;		/* How many characters to place at dst. */
    register char *src;		/* Source string. */
    char *dst;			/* Destination area. */
d1624 2
a1625 1
    register char *copy = dst;
d1630 1
d1634 2
d1638 14
d1807 1
@


1.3
log
@*** empty log message ***
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.2 88/07/31 22:51:50 mendel Exp $ SPRITE (Berkeley)";
d262 4
a464 3
/*
    VmMachFlushBytes(vm_SysSegPtr, segDataPtr->pt2BasePtr, VMMACH_SEG_PT2_SIZE);
*/
d1055 1
d1059 2
a1060 1
    segPtr = VmGetSegPtr((unsigned int)addr >> VMMACH_SEG_REG_SHIFT);
d1128 1
d1132 2
a1133 1
    segPtr = VmGetSegPtr((unsigned int)addr >> VMMACH_SEG_REG_SHIFT);
d1175 1
a1175 1
	*ptePtr |= VMMACH_KRW_UNA_PROT | VMMACH_REFERENCED_BIT | 
d1648 28
d1681 5
d1696 2
a1697 2
    Vm_Segment	*segPtr;	/* Segment to flush. */
    int		pageNum;	/* Page within segment to flush. */
d1699 5
a1703 5
    register	Address	pageAddr;
    register	Address	addr;
    VmMachPTE		*ptePtr;
    Boolean		setResBit = FALSE;
    VmMach_SegData	*segDataPtr;
a1705 1

d1707 2
a1708 3
    if (*ptePtr & VMMACH_RESIDENT_BIT) {
	*ptePtr &= ~VMMACH_RESIDENT_BIT;
	setResBit = TRUE;
d1710 10
a1719 2
    pageAddr = (Address) ((segPtr->segNum << VMMACH_SEG_REG_SHIFT) + 
			  (pageNum << VMMACH_PAGE_SHIFT));
d1726 3
a1728 2
    if (setResBit) {
	*ptePtr |= VMMACH_RESIDENT_BIT;
@


1.2
log
@Patched bug in VmMach_MapInDevice and made device pages referenced and
modified.  

Made VmMachValidate set MODIFIED and REFERENCED.
THIS SHOULD BE CHANGED.
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSpur.c,v 1.1 88/07/29 11:29:53 nelson Exp $ SPRITE (Berkeley)";
d61 5
d188 15
a202 1
    machPtr = (VmMach_SegData *)Mem_Alloc(sizeof(VmMach_SegData));
a221 5
#ifdef remap_pte
	pte = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT | 
	      VMMACH_KRW_URO_PROT | SetPageFrame(pfNum);
	*ptePtr = pte;
#endif
a232 5
#ifdef remap_pte
	pte = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT | 
	      VMMACH_KRW_URO_PROT | SetPageFrame(pfNum);
	*ptePtr = pte;
#endif
d254 2
a255 2
     * Map enough memory to cover the current amount of kernel memory that
     * is in use.
d257 2
a258 21
    for (ptePtr = basePTPtr, i = 0; i < firstFreePage; ptePtr++, i++) {
	if (i == (((unsigned int)mach_StackBottom + VMMACH_PAGE_SIZE) >> 
							VMMACH_PAGE_SHIFT)) {
	    *ptePtr = 0;
	} else if (i == (((unsigned int)MACH_DEBUG_STACK_BOTTOM + 
			VMMACH_PAGE_SIZE) >> VMMACH_PAGE_SHIFT)) {
	    *ptePtr = 0;
	} else {
#ifdef remap_pte
	    pfNum = GetPageFrame(*ptePtr);
	    pte = VMMACH_RESIDENT_BIT | VMMACH_CACHEABLE_BIT | 
		  VMMACH_KRW_URO_PROT | SetPageFrame(pfNum);
	    *ptePtr = pte;
#endif
	}
    }

    /*
     * Clear out the rest of kernel page tables.
     */
    for (; i < (machPtr->lastPTPage + 1) * VMMACH_PTES_IN_PAGE; 
d295 5
a299 3
    register	Vm_Segment	*segPtr;
    unsigned int		offset;
    int				numPages;
d305 1
a305 1
    Address	ptAddr;
d313 1
a313 1
    if (offset < segDataPtr->firstPTPage) {
d329 2
a330 1
	              VMMACH_KRW_UNA_PROT | 
d358 1
a358 1
    unsigned	int		segOffset;
d378 9
d461 3
a463 1
    VmMachFlushBytes(segPtr, segDataPtr->pt2BasePtr, VMMACH_SEG_PT2_SIZE);
d584 3
d588 1
a588 1
	    RPTPMs[VM_CODE] = VMMACH_INVALID_SEGMENT / 4 + rootPTPageNum;
d590 1
a590 1
	    RPTPMs[VM_HEAP] =  VMMACH_INVALID_SEGMENT / 4 + rootPTPageNum;
d592 1
a592 1
	    RPTPMs[VM_STACK] = VMMACH_INVALID_SEGMENT / 4 + rootPTPageNum;
d595 1
a595 1
	    RPTPMs[VM_CODE] = segNums[VM_CODE] / 4 + rootPTPageNum;
d597 1
a597 1
	    RPTPMs[VM_HEAP] = segNums[VM_HEAP] / 4 + rootPTPageNum;
d599 1
a599 1
	    RPTPMs[VM_STACK] = segNums[VM_STACK] / 4 + rootPTPageNum;
a1167 1
	     VMMACH_REFERENCED_BIT | VMMACH_MODIFIED_BIT | 
d1170 2
a1171 1
	*ptePtr |= VMMACH_KRW_UNA_PROT;
a1213 1
    *ptePtr = 0;
d1217 1
d1222 58
d1329 2
a1330 1
		      VMMACH_KRW_URO_PROT | 
@


1.1
log
@Initial revision
@
text
@d10 1
a10 1
static char rcsid[] = "$Header: vmSun.c,v 1.5 88/03/18 14:54:58 nelson Exp $ SPRITE (Berkeley)";
d1161 1
a1255 1

d1261 1
a1261 1
	 firstPTPage++, ptePtr++, ptAddr += VMMACH_PAGE_SIZE) {
d1272 3
a1274 2
    pte = VMMACH_RESIDENT_BIT | VMMACH_KRW_URO_PROT |
	  SetPageFrame((unsigned)devPhysAddr >> VMMACH_PAGE_SHIFT);
@
